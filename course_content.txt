================================================================================
COURSE 1: RN 2025
================================================================================

Slide 1:
Course 1
Neural Networks

Slide 2:
Course evaluation

Machine learning concepts
Basic Linear Algebra
Numpy
Datatype
Arrays
Shapes
Indexing & slicing
Operations
Serialization
Overview

Slide 3:
Course Evaluation

Slide 4:

60% of final grade
Points can be received by completing 5 assignments




Laboratory Requirements
40% of final grade
Points will be received from a final test that will take place during the examination session

   Minimum 35 points out of 100
Course Requirements
Evaluation
Minimum 35 points out of 100

Slide 5:
More about the Laboratory:
	(subject to change)
100 points divided in 5 assignments:
 First Half of the semester:
Assignment 1: 10 points (Deadline: 3rd  week)
Assignment 2: 15 points (Deadline: 4th  week)
Assignment 3: 30 points (Deadline: 7th  week)

 Second Half of the semester
Assignment 4: 15 points	 (Deadline: 10th week)
Assignment 5: 30 points (Deadline: 13th  week)




Evaluation

Slide 6:
More about the Laboratory:
Assignments must be solved using python, numpy and pytorch
During the first half of the semester:
no machine learning framework must be used
During the second half of the semester:
using the pytorch machine learning framework

Evaluation

Slide 7:
Overview of neural networks
Basic Neural Network Elements

Slide 8:
What is AI ?
A.I.(Artificial Intelligence) refers to the simulation of human intelligence in machines.

In practice – we can consider this form of simulation an approximation on what we (human) consider intelligence.

Slide 9:
What is AI ?
Let’s see a simple example: 



How can we translate this behavior/functionality to a computer ?

Slide 10:
What is AI ?
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.

Slide 11:
What is AI ?
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.

Slide 12:
What is AI ?
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.

Slide 13:
What is AI ?
f(…)
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.

Slide 14:
What is AI ?
f(…)
Horse
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.

Slide 15:
What is AI ?
f(…)
Horse
Let’s see a simple example:


First, we need the ability “to look”  this simply translate into taking a picture
That picture is transferred to a computer where it is converted into a matrix of (w x h), where “w” is the width of the picture and “h” is its height. Each cell of that matrix is a color (a numerical value) that reflects the color of the pixels from the original picture.
We convert the matrix into a vector of numbers by concatenating each row one after another
Now we need a function that takes “w” x “h” parameters (let’s call it function f), and returns a number with a very special meaning: if the result is 1, it corresponds to the word “horse”, if the result is 2, it corresponds to the word “cat’ and so on.
Finally, we use a map with associations (1 horse, 2 cat, …) and we print the result.
This is where the magic happens.
The question is how do we find this function or if it exists ?

Slide 16:
What is AI ?

Slide 17:
So … the question here is  Is there a function f that can satisfy our requirements ?

The answer is that we should not search for an exact function, but one that approximates the results as much as possible !

The function that we find (meaning its form and its parameters) is called a model !
What is AI ?

Slide 18:
What is AI ?

Slide 19:
So … the new question is how can we approximate a function/model that has to obtain a specific value ?

Or even more, assuming we have such a function/model, how can we measure its effectiveness ?
What is AI ?

Slide 20:
To answer both these questions – we will need a dataset.
A data set (for classification) is a set of entries where we know their output. For example, assuming we are trying to find the function “add ” for two numbers, the data set will look like this:
What is AI ?
Input for the function

Slide 21:
Having access to a database allows us to:
Use several techniques to approximate the output as best as we can
Validate that the output is the one that we expect.
What is AI ?

Slide 22:
Having access to a database allows us to:
Use several techniques to approximate the output as best as we can
Validate that the output is the one that we expect.
What is AI ?
One simple example is the Lagrange Interpolation technique (that even if initially design to work with 2-dimensional points) is a good start in understanding how A.I. works.

Slide 23:
Lagrange interpolation – given a set of points, identify a function “f ” that approximates “Y” given an “X”  f(x) = y
What is AI ?

Slide 24:
Having access to a database allows us to:
Use several techniques to approximate the output as best as we can
Validate that the output is the one that we expect.
What is AI ?
The main question here is how can we measure this ?

Slide 25:
How to validate A.I. ?
There are several options that can be used to test this:
The database has to be split into several parts (some to be used to identify the function “f “, others for validation of that function
We will need some metrics (to validate how good the resulted function is).
What is AI ?

Slide 26:
How to validate A.I. ?
First – lets see how we can split a database:
What is AI ?
Training data set (used to obtain/train the f function)
Validation data set (used to fine tune the result)
Test data set (used to validate the result)

Slide 27:
How to validate A.I. ?
First – lets see how we can split a database:
What is AI ?
Training data set
Validation data set
Test data set
It’s important to select a proper balance between these data sets. For example, if the training set is too small while the test set is too large, the results will be inconclusive as the algorithm will not have enough data to learn from. Similarly, if no test/validation data set is available (or too small), the results may not be validated properly, or an overfitting state can be achieved.

Slide 28:
Overfitting
Overfitting refers to a modeling error that occurs when a machine learning algorithm captures noise in the training data rather than the underlying data distribution. This results in a model that performs well on the training data but poorly on new data, because it has essentially "memorized" the training data rather than generalizing from it. 
What is AI ?

Slide 29:
Overfitting
Let’s see an example (we will consider a data set of 4 entries where the first 3 entries are used for training, and the last one for validation.
What is AI ?
Generic solution
f(4) = 8
Overfitting
f(4) = 0

Slide 30:
How to validate A.I. ?
First – lets see how we can split a database:
What is AI ?
Training data set
Validation data set
Test data set
The validation data set is OPTIONAL Some percentages commonly used:
60/20/20 (60% training, 20% validation, 20% testing)
70/15/15 (70% training, 15% validation, 15% testing)
80/10/10 (80% training, 10% validation, 10% testing)

Slide 31:
How to validate A.I. ?
First – lets see how we can split a database:
What is AI ?
Training data set
Validation data set
Test data set
The validation and testing data set don’t have to be equals.
If you perform hyper-parameter tuning  it is recommended to use a larger validation data set
If you want the result to be more generic, and have a better check and a more reliable result  it is recommended to use a larger test data set.

Slide 32:
How to validate A.I. ?
Another solution on how to split a database is to use a technique called k-fold cross-validation. The idea is to split the entire database into “k” partition, and then use (k-1) partitions for training and the partition that remains for testing. The validation is performed for “k” times (each time using a different partition for testing). 
What is AI ?
For example, in this case we use partition 1,2,3,5,6,7,8 to train and partition 4 for testing (in a 8-fold cross validation)

Slide 33:
How to validate A.I. ?
Before we discuss metrics to evaluate, lets first discuss about type of artificial intelligence algorithms:
Supervised learning - the model is trained on a labeled dataset, meaning the algorithm is provided with input-output pairs.
Unsupervised learning - works with unlabeled data. The goal is to find patterns or structures in the data. One such example are clustering algorithms
Other variations:
Reinforcement learning
Semi-supervised learning
…
What is AI ?

Slide 34:
How to validate A.I. ?
When talking about supervised learning a machine learning algorithm can be classified (taking into consideration the output) in: 
Single class (binary models) - Classify instances into one of two classes. For example , in our example that tries to identify a picture and find a horse, the output of a function can be 1 (TRUE) or 0 (FALSE).
Multi class - Classify instances into one of more than two classes. For the same example, the output of the function could be 1 (for a Horse), 2 for a Monkey, 3 for a Cat, ….
What is AI ?

Slide 35:
How to validate A.I. ?
To better explain how several metrics work, lets consider a supervised single class machine learning that attempts to identify pictures with cats from a data base. 
What is AI ?
TRUE POSITIVE
TRUE NEGATIVE
FALSE POSITIVE
FALSE NEGATIVE

Slide 36:
How to validate A.I. ?
Let’s assume that we have the following data base:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)

Slide 37:
How to validate A.I. ?
Let’s assume that we have the following data base:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)

Slide 38:
How to validate A.I. ?
Let’s assume that we have the following data base:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)

Slide 39:
How to validate A.I. ?
Let’s assume that we have the following data base:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)

Slide 40:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)
Accuracy (Acc)
accuracy is a measure of the correctness of a model's predictions , in other words how many samples were correctly classified from the test set. It is good for a balanced data set.

Slide 41:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positive Rate (FPR)
proportion of negative instances that are incorrectly classified as positive
False Positives (FP) = 2
(entries #3, #7)

Slide 42:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Negative Rate (FNR)
proportion of positive instances that are incorrectly classified as negative
False Positives (FP) = 2
(entries #3, #7)

Slide 43:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)
Precision
Evaluate the correctness of positive predictions (it is in particular valuable for models where TP is relevant  for example a classifier that tries to determine if a disease is present or not).

Slide 44:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)
Recall or Sensitivity (Se) 
Shows how many positive instances the model might be missing. Relevant for scenarios where failing to identify a positive instance has significant consequences. Also known as True Positive Rate.

Slide 45:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)
Specificity (Sp) 
Evaluates how well the model identifies negative instances and avoids false alarms. Also known as True Negative Rate.

Slide 46:
How to validate A.I. ?
With this values computed we can evaluate several metrics:
What is AI ?
True Positives (TP) = 4
(entries #1, #4, #8, #9)
True Negatives (TN) = 2
(entries #5, #6)
False Negatives (FN) = 1
(entry #2)
False Positives (FP) = 2
(entries #3, #7)
F1 Score
Useful when both false positives and false negatives are crucial and for cases where data set is unbalanced (the number of entries of one class is far bigger than the entries of another class).

Slide 47:
How to validate A.I. ?
Now, lets see how a model can be validated using a 3-fold cross validation and the FPR metric. To simplify the problem lets consider that the database is formed out of 3 partition A, B and C (all equal in size).
What is AI ?
A
B
C
This means that the average value for FPR is (3%+4%+5%)/3 = 4%. This value represents how well the model behaves on this data set.

Slide 48:
Basic linear algebra notions

Slide 49:
Basic notions
Before we start, lets discuss some basic notions used in linear algebra that we will further use in our course:
Scalar
Vector
Matrix
And some operations:
vector
matrixes


Slide 50:
Basic notions

Slide 51:
Basic notions

Slide 52:
Basic notions

Slide 53:
Basic operations (vectors)

Slide 54:
Basic operations (vectors)

Slide 55:
Basic operations (vectors)

Slide 56:
Basic operations (vectors)

Slide 57:
Basic operations (vectors)

Slide 58:
Basic operations (vectors)
Distance 
Let’s see an example (we will consider a 2-dimensional space).

We can compute the distance based on a simple distance formula !

Slide 59:
Basic operations (vectors)
Distance 
Let’s see an example (we will consider a 2-dimensional space).

Or we can consider P1 and P2 two bidimensional vectors and use the norm.

Slide 60:
Basic operations (matrices)

Slide 61:
Basic operations (matrices)

Slide 62:
Basic operations (matrices)

Slide 63:
Basic operations (matrices)

Slide 64:
Basic operations (matrices)
Example (2 x 3 matrix with a 3 x 2 matrix):
This concept will further be relevant to the study of DENSE neural networks.

Slide 65:
What is numpy

Slide 66:
What is numpy
One of the most important Python’s mathematical modules, delivering high-speed numerical computing with a robust C/C++ backend for unparalleled efficiency.

Current version: 2.3 (28.09.2025)
Site: https://numpy.org/ 
Documentation: https://numpy.org/doc/stable/ 


Slide 67:
What is numpy
To install numpy:
Link: https://numpy.org/ 
PIP: pip install numpy
CONDA: conda install numpy

Compatibility (for latest version): Python 3.12
For best usage, we should use a version that is compatible with Python 3.11 (that will match latest version of PyTorch)

Slide 68:
What is numpy
Support for:
Arrays (ndarray)
Mathematical functions (including basic arithmetic operations, statistical functions, algebraic operations, etc)
Broadcasting (A powerful feature that allows NumPy to work with arrays of different shapes when performing arithmetic operations)
Linear Algebra
Indexing and Slicing
FFT and Polynomials (Built-in support for Fast Fourier Transforms and polynomial operations).
Support to save/load data into different formats

Slide 69:
Data Types

Slide 70:
Data types
NumPy supports a variety of numeric data types, which are essential for handling diverse computational requirements.
Integers:
int8 : 8-bit signed integer (-128 to 127)
int16 : 16-bit signed integer (-32768 to 32767)
int32 : 32-bit signed integer (-231 to 231 -1)
int64 : 64-bit signed integer (-263 to 263 -1)
Unsigned Integers:
uint8 : 8-bit unsigned integer (0 to 255)
uint16 : 16-bit unsigned integer (0 to 65535)
uint32 : 32-bit unsigned integer (0 to 232 -1)
uint64 : 64-bit unsigned integer (0 to 264 -1)

Slide 71:
Data types
NumPy supports a variety of numeric data types, which are essential for handling diverse computational requirements.
Floating Point:
float16: Half precision float
float32: Single precision float
float64: Double precision float (default for float)
float128: Extended precision float (not available on all platforms)
Complex Numbers:
complex64: Complex number with two 32-bit floats (real and imaginary components)
complex128: Complex number with two 64-bit floats 
complex256: Complex number with two 128-bit floats (not available on all platforms)

Slide 72:
Data types
NumPy supports a variety of numeric data types, which are essential for handling diverse computational requirements.
Boolean:
bool_: Boolean type storing True and False values
Strings:
string_: Fixed-size string type
unicode_: Fixed-size Unicode type
Datetime:
datetime64: Date and time representation
timedelta64: Represents the difference between two dates or times

Slide 73:
Data types
Numerical data types are directly mapped to corresponding C (and occasionally C++) data types, ensuring both speed and compact memory usage. 

The primary reason NumPy is so efficient compared to native Python lists or arrays is that it leverages these low-level languages (C/C++) for the storage and operations, avoiding the overhead of Python's dynamic typing and object-oriented features.

Slide 74:
Data types
C/C++ type backend type mapping:


Slide 75:
Arrays (constructors)

Slide 76:
Arrays
An array can be created using the method .array(…) defined as follows:
.array( <data>, [type] ), where
<data> is the raw data (one usual solution is to provide a list of values)
[type] is an optional parameter that describes a possible scalar type for the data. If not provided, it will be inferred from the data type.
Example:
import numpy as np
a = np.array([1,2,3],dtype=np.int32)
print(a)
print(a.dtype)

Slide 77:
Arrays
You can also build an array with multiple dimensions (by providing  lists within lists as the first parameter.

Example:
import numpy as np
a = np.array([[1,2,3],[4,5,6]],dtype=np.int32)
print(a)
print(a.dtype)

Slide 78:
Arrays
Or a more complex matrix (in terms of shape and definition):
Example:
import numpy as np
a = np.array(
[
    [
        [1,2,3],[4,5,6],[7,8,9]
    ],
    [
        [10,20,30],[40,50,60],[70,80,90]
    ],
])
print(a)
print(a.dtype)

Slide 79:
Arrays
However, keep in mind that when building n-dimensional arrays you have to respect the shape (sizes) for each list within the declaration.
Example:
import numpy as np
a = np.array([[1,2,3],[4,5]],dtype=np.int32)
print(a)
print(a.dtype)

Slide 80:
Arrays
However, keep in mind that when building n-dimensional arrays you have to respect the shape (sizes) for each list within the declaration.
Example:
import numpy as np
a = np.array([[1,2,3],[4,5]],dtype=np.int32)
print(a)
print(a.dtype)
In our case, [1,2,3] has the size of 3 and [4,5] has the size 2 (and it is impossible to construct a matrix this way)

Slide 81:
Arrays
When converting the value from an array into internal data, a cast is being performed (that might lose precision).
Examples:
import numpy as np
a = np.array([1,"2",4],dtype=np.int32)
print(a)
print(a.dtype)
import numpy as np
a = np.array([1,2.7,4],dtype=np.int32)
print(a)
print(a.dtype)

Slide 82:
Arrays
However, keep in mind that the cast has to be possible (this usually translates into having an operator such as int() defined for that specific type).
Example:
import numpy as np
a = np.array([1,"some text",4],dtype=np.int32)
print(a)
print(a.dtype)

Slide 83:
Arrays
If the second parameter is missing, the type is inferred. If the input data contains values of different types, the larger one will be selected.
Example:
Since 2.5 (a float64) is the biggest value, the entire array will be considered of this type.
import numpy as np
a = np.array([1,2.5,4])
print(a)
print(a.dtype)

Slide 84:
Arrays
We should also mention that using strings (or large strings) might have a different behavior.
Example:
This has the following translation:
“<“ 	 little endian
“U” 	 Unicode string
“33” 	 33 characters
import numpy as np
a = np.array(["1",2.5,4,"hello world from python and numpy"])
print(a)
print(a.dtype)

Slide 85:
Arrays
Another way of create an array is with .zeros(<shape>,[type]) method.
Examples:
import numpy as np
a = np.zeros((3,4))
print(a)
print(a.dtype)
import numpy as np
a = np.zeros((2,4),np.uint16)
print(a)
print(a.dtype)

Slide 86:
Arrays
Similarly, .ones(<shape>,[type]) method performs the same task but uses 1 instead of 0 to fill a matrix:
Examples:
import numpy as np
a = np.ones((2,3))
print(a)
print(a.dtype)
import numpy as np
a = np.ones((2,4),np.uint16)
print(a)
print(a.dtype)

Slide 87:
Arrays
Another method is .arange(…) that creates a vector with consecutive values.  Since every array/vector can be reshaped into another array, this method can be used to create any kind of array
Format:
.arange(stop,[type])
.arange(start, stop, [type])
.arange(start, stop, step, [type]

Slide 88:
Arrays
Example (with stop parameter):



Example (with start and stop parameter):

import numpy as np
a = np.arange(10)
print(a)
print(a.dtype)
import numpy as np
a = np.arange(3,10)
print(a)
print(a.dtype)

Slide 89:
Arrays
Example (with start, stop and step parameter):



Example (with start, stop , step and type parameters):

import numpy as np
a = np.arange(3,10,2)
print(a)
print(a.dtype)
import numpy as np
a = np.arange(3,10,2,dtype=np.float16)
print(a)
print(a.dtype)

Slide 90:
Arrays
For large arrays, you can use .empty(shape,[type]) to create an empty array and then fill it with values. If the type is not provided, float64 is implied.
Example:



Notice that the values are unknown (what lies in memory at the moment of allocation).
import numpy as np
a = np.empty((3,2),dtype=np.int32)
print(a)
print(a.dtype)

Slide 91:
Arrays
Another method is .full(shape, fill_value, [type]) that can be used to create a matrix filled with a specific value

Example:



import numpy as np
a = np.full((3,2),3)
print(a)
print(a.dtype)

Slide 92:
Arrays
Overview




Slide 93:
Shapes

Slide 94:
Shapes
There is a difference between how we (people) understand a vector / and array / etc and how a computer stores the data.
Data in memory
Vector
Shape: (8)
Bi-dimentional matrix
Shape: (2 , 4)
Bi-dimentional matrix
Shape: (4 , 2)
tri-dimentional matrix
Shape: (2 , 2, 2)

Slide 95:
Shapes
For any numpy array you can use .shape and .size to get the shape of the array and its size (total number of elements in the array)
Example:



import numpy as np
a = np.array([1,2,3])
print(a)
print(a.shape)
print(a.size)
import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print(a)
print(a.shape)
print(a.size)

Slide 96:
Shapes
We can also reshape any array (as long as the number of elements of the resulting shape is the same as the one from the current array). For this we can use the method reshape(…).
Example:



import numpy as np
a = np.array([1,2,3,4,5,6,7,8])
print(a)
print(a.shape)
print(a.size)
a = a.reshape((2,4))
print(a)
print(a.shape)

Slide 97:
Shapes
Additionally, we can also use:
.size  to get the total number of elements in the array
.ndim  to get the number of dimensions
.itemsize  to get the size of an item/element (in bytes)
Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("Array    = ",a)
print("Shape    = ",a.shape)
print("Dim      = ",a.ndim)
print("Size     = ",a.size)
print("Type     = ",a.dtype)
print("ItemSize = ",a.itemsize)

Slide 98:
Indexes & Slicing

Slide 99:
Indexes & Slicing
Indexes and slices work in a similar manner as with Python list (in particular if we are referring to vectors).

Example:



import numpy as np
a = np.array([1,2,3,4,5,6])
print("a[0]   = ",a[0])
print("a[-1]  = ",a[-1])
print("a[1:3] = ",a[1:3])
print("a[:2]  = ",a[:2])

Slide 100:
Indexes & Slicing
In case of matrixes, the logic is similar (but the results are somehow different as they will not be a scalar but rather another matrix or vectors).
Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("a[0]   = ",a[0])
print("a[-1]  = ",a[-1])
print("a[1:4] = ",a[1:4])
print("a[:2]  = ",a[:2])

Slide 101:
Indexes & Slicing
In case of matrixes, the logic is similar (but the results are somehow different as they will not be a scalar but rather another matrix or vectors).
Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("a[0]   = ",a[0])
print("a[-1]  = ",a[-1])
print("a[1:4] = ",a[1:4])
print("a[:2]  = ",a[:2])
Notice that even if the upper index (3) is not an index that exists in the matrix (the shape of the matrix is (2x3) – so only 2 rows (not 3), the slicing operation is limited to the maximum number of rows

Slide 102:
Indexes & Slicing
In case of matrixes, you can access an element or a smaller matrix by using the “,” character and specify multiple dimensions and operations such as slicing to be performed for each dimension.
Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("a[0,0]   = ",a[0,0])
print("a[-1,-1] = ",a[-1,-1])
print("a[1:2,2] = ",a[1:2,2])
print("a[1,2] = ",a[1,2])
print("a[:2,:2] = ",a[:2,:2])

Slide 103:
Indexes & Slicing
In case of matrixes, you can access an element or a smaller matrix by using the “,” character and specify multiple dimensions and operations such as slicing to be performed for each dimension.
Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("a[0,0]   = ",a[0,0])
print("a[-1,-1] = ",a[-1,-1])
print("a[1:2,2] = ",a[1:2,2])
print("a[:2,:2] = ",a[:2,:2])
In this case we request a slicing from the original matrix (of shape 2x3) to another matrix of shape 2x2

Slide 104:
Indexes & Slicing
Observation: When you use indexing to access a specific index, you lose one dimension per index.
You can avoid this by using “:” operator and accessing just one element.
import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print(f"a.ndim = {a.ndim}, a.shape={a.shape}")
print(f"a[0,0] = {a[0,0]}, dim={a[0,0]. ndim}")
print(f"a[1,:] = {a[1,:]}, dim={a[1,:].ndim}")
print("solution for keeping the same nr of dimensions”)print(f"a[:,1] = {a[:,1]}, dim={a[:,1].ndim}")
print(f"a[0,0:1] = {a[0,0:1]}, dim={a[0,0:1].ndim}")
print(f"a[0:1,0:1] = {a[0:1,0:1]}, dim={a[0:1,0:1].ndim}")
print(f"a[1:2,:] = {a[1:2,:]}, dim={a[1:2,:].ndim}")
print(f"a[:,1:2]] = {a[:,1:2]}, dim={a[:,1:2].ndim}"

Slide 105:
Indexes & Slicing
You can also use another special character (…) that specifies to fill all of the rest of the dimensions (similar to using ‘:’ in Python list for each dimension).


import numpy as np
a = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
print("a[0,0]      = ", a[0, 0])
print("a[-1,-1,-1] = ", a[-1, -1,-1])
print("a[1,...]    = ", a[1,...])
print("a[1,:,:]    = ", a[1,:,:])
print("a[...,1]    = ", a[...,1])
print("a[...,1:2]    = ", a[...,1:2])

Notice that a[1,…] and a[1,:,:] are equivalent !

Slide 106:
Indexes & Slicing
Additionally, the index operation can be used with a function parameter (something that evaluates to true or false). That function works as a filtering method, keeping only the elements that yield true when that function is called upon them. When using this method of filtering use the name of the array as a reference to each element:
Example:



import numpy as np
a = np.array([1,2,3,4,5,6])
print("a[a>5]    = ", a[a>5])
print("a[a%2==0] = ", a[a%2==0])

Slide 107:
Indexes & Slicing
Additionally, the index operation can be used with a function parameter (something that evaluates to true or false). That function works as a filtering method, keeping only the elements that yield true when that function is called upon them. When using this method of filtering use the name of the array as a reference to each element:
Example:



import numpy as np
a = np.array([1,2,3,4,5,6])
print("a[a>5]    = ", a[a>5])
print("a[a%2==0] = ", a[a%2==0])
In this case, the “a” from “a>5” expression represents each element from “a” matrix.

Slide 108:
Indexes & Slicing
When this method is being used with matrixes, it works in a similar manner, but the result is not a matrix , but a vector.
Example:



You can use this method to convert a matrix into a vector:
import numpy as np
a = np.array([[1,2,3],
              [4,5,6]])
print("a[a>5]    = ", a[a>5])
print("a[a%2==0] = ", a[a%2==0])
import numpy as np
a = np.array([[1,2,3],[4,5,6]])
print("a[a>0] = ",a[a>0])

Slide 109:
Indexes & Slicing
You can also use for statement to iterate through a matrix or vector.

Example:


import numpy as np
a = np.array([1,2,3,4,5,6])
for i in a:
    print(i)

Slide 110:
Indexes & Slicing
In case of matrixes, the output of one iteration will be a lower dimension matrix or a vector for a bidimensional matrix.

Example:



import numpy as np
a = np.array([[1,2,3],[4,5,6]])
for i in a:
    print(i)

Slide 111:
Indexes & Slicing
To iterate through all elements in a matrix use multiple imbricated for statements:

Example:

import numpy as np
a = np.array([[1,2,3],
              [4,5,6]])
for row in a:
    for element in row:
        print(row,":",element)

Slide 112:
Operations

Slide 113:
Indexes & Slicing
The following operations are supported by numpy:
Arithmetic operations (addition, subtraction, multiplication, division) between arrays of the same shape. The operation are performed element wise and the operation results in a new array of the same shape.

Scalar operations (also called broadcasting) – addition, subtraction, multiplication and division between an array and a scalar. 

DOT product

Matrix multiplication

Transpose matrix

Statistical operations (sum, min, max, mean, …)





Slide 114:
Array operations
Example (vector addition):




Example (vector addition – different shapes):



import numpy as np
v = np.array([1,2,3])
u = np.array([10,20,30])
print("sum = ",v+u)
print("dif = ",u-v)
print("mul = ",v*u)
print("div = ",u/v)
import numpy as np
v = np.array([1,2,3])
u = np.array([10,20,30,40])
print("sum = ",v+u)

Slide 115:
Array operations
Example (matrix addition):




Example (matrix addition – different shapes):



import numpy as np
v = np.array([[1,2,3],[4,5,6]])
u = np.array([[10,20,30],[40,50,60]])
print("sum = ",v+u)
print("dif = ",u-v)
print("mul = ",v*u)
print("div = ",u/v)
import numpy as np
v = np.array([[1,2,3],[4,5,6]])
u = np.array([[10,20],[40,50]])
print("sum = ",v+u)

Slide 116:
Array operations
Arithmetic operations create a new array. This can impact the overall performance and memory. If a new array is not needed, but instead it is possible to perform the operation directly over an existing array, you can use operators such as ‘+=‘ , ‘-=‘ , ‘*=‘, ‘/=‘

Example (matrix addition – different shapes):



import numpy as np
v = np.array([[1,2,3],[4,5,6]])
u = np.array([[10,20,30],[40,50,60]])
V += u
print("sum = ",v)

Slide 117:
Array operations
Operations between a matrix and a scalar are performed elementwise (for each element in the matrix, the scalar operation is being performed). The result is a new matrix with the same shape as the original one. It is also possible to compute this changes in place via +=,-=,*=,/= operators.
Example:



import numpy as np
m = np.array([[1,2,3],[4,5,6]])
print("add = ",m+2)
print("sub = ",m-1)
print("mul = ",m*2)
print("div = ",m/2)

Slide 118:
Array operations
Dot product can be computed using .dot() method
Example:








import numpy as np
v = np.array([1,2,3])
u = np.array([4,5,6])
print("dot = ",v.dot(u))

Slide 119:
Array operations
You can also use .dot() method to compute matrix multiplication. The same result is obtained via ‘@’ operator.
Example:







import numpy as np
v = np.array([[1,2,3],
              [4,5,6]])
u = np.array([[10,20,30,40],
              [40,50,60,70],
              [80,90,100,110]])
print("result     = ",v.dot(u))
print("matrix_mul = ",v @ u)

Slide 120:
Array operations
You can also use .dot() method to compute matrix multiplication. The same result is obtained via ‘@’ operator.
Example:







import numpy as np
v = np.array([[1,2,3],
              [4,5,6]])
u = np.array([[10,20,30,40],
              [40,50,60,70],
              [80,90,100,110]])
print("result     = ",v.dot(u))
print("matrix_mul = ",v @ u)
390 = dot product (row 1 from matrix “v” and column 2 from matrix “u”) = 
1 x 20 + 2 x 50 + 3 x 90 = 20 + 100+ 270 = 390

Slide 121:
Array operations
To transpose a matrix, use the .transpose() method

Example:







import numpy as np
m = np.array([[1,2,3],
              [4,5,6]])
print("matrix = ",m)
print("transpose = ",m.transpose())

Slide 122:
Array operations
Several statistical operations are available. When used against a matrix, without any parameters, all elements from the matrix are being used for analysis.

Example:







import numpy as np
m = np.array([[1,2,3],[4,5,6]])
print("sum     = ",m.sum())
print("min     = ",m.min())
print("max     = ",m.max())
print("mean    = ",m.mean())

Slide 123:
Array operations
However, all of the statistical methods have a parameter called axes that can allow specifying an axes/shape that can be used to limit the computations for a specific dimension
Example:







import numpy as np
m = np.array([[1,2,3],
              [4,5,6]])
print("sum     = ",m.sum(axis=0))
print("min     = ",m.min(axis=0))
print("max     = ",m.max(axis=0))
print("mean    = ",m.mean(axis=0))

Slide 124:
Array operations
However, all of the statistical methods have a parameter called axis that can allow specifying an axes/shape that can be used to limit the computations for a specific dimension
Example:







import numpy as np
m = np.array([[1,2,3],
              [4,5,6]])
print("sum     = ",m.sum(axis=0))
print("min     = ",m.min(axis=0))
print("max     = ",m.max(axis=0))
print("mean    = ",m.mean(axis=0))
axis = 0 (means “X” axis). This translates that all operations will be perform over “X” axis. For sum, the values [5 7 9] are obtained in the following way: 5=1+4, 7=2+5, 9=3+6

Slide 125:
Array operations
Similarly, using axis=1 refers to the “Y” axis (the second dimension) and all computation will be made around that axis.

Example:







import numpy as np
m = np.array([[1,2,3],
              [4,5,6]])
print("sum     = ",m.sum(axis=1))
print("min     = ",m.min(axis=1))
print("max     = ",m.max(axis=1))
print("mean    = ",m.mean(axis=1))

Slide 126:
Array operations
Similarly, using axis=1 refers to the “Y” axis (the second dimension) and all computation will be made around that axis.

Example:







import numpy as np
m = np.array([[1,2,3],
              [4,5,6]])
print("sum     = ",m.sum(axis=1))
print("min     = ",m.min(axis=1))
print("max     = ",m.max(axis=1))
print("mean    = ",m.mean(axis=1))
In this case, the values for sum method are computed in the following way: 
6=1+2+3, 15=4+5+6

Slide 127:
Array operations
More on axis: Using axis, you specify that you want to perform operations on that axis. This will make the result one dimension lower.  

Example:







import numpy as np
a = np.arange(1,1+2*3*4).reshape(2,3,4)
print("a=",a)
print("a.sum(axis=0)=", a.sum(axis=0))
print("a.sum(axis=1)=", a.sum(axis=1))
print("a.sum(axis=2)=", a.sum(axis=2))

Slide 128:
Array operations
axis=0
axis=1
axis=2
a.sum(axis=0) 
a.sum(axis=2) 
+
=
shape=(2,3,4)
(3,4)
(2,3)
+
+
(2,4)
+
+
+
=
=
=
=
a.sum(axis=1) 

Slide 129:
Array operations
Other statistical operations (such as percentile, mean, median) are used as functions exported directly from the numpy module.
Example:







import numpy as np
v = np.array([1,1,2,2,3,4,5,6])
print("mean          = ",np.mean(v))
print("median        = ",np.median(v))
print("average       = ",np.average(v))
print("percentile 5  = ",np.percentile(v,5))
print("percentile 50 = ",np.percentile(v,50))
print("percentile 95 = ",np.percentile(v,95))

Slide 130:
Array operations
You can also use method such as .square(), .sqrt() or .cbrt() [Cubic root] to performed more advanced mathematical computation (for both arrays and scalars)
Example:







import numpy as np
m = np.array([[1,2,3],[4,5,6]])
print("Square           = ",np.square(m))
print("Sqrt             = ",np.sqrt(m))
print("Cubic root       = ",np.cbrt(m))
print("Scalar square(2) = ",np.square(2))
print("Scalar sqrt(100) = ",np.sqrt(100))
print("Scalar cbrt(27)  = ",np.cbrt(27))

Slide 131:
Array operations
Besides these functions, there a lot of other methods available in the numpy library (including methods for sorting elements, resize-ing arrays, etc).
Those methods are improved for optimal performance.








Slide 132:
Array operations

Slide 133:
Array operations
Let’s see how we can write RMSE in numpy
Example :







import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)

Slide 134:
Array operations
Let’s see how we can write RMSE in numpy
Example :







import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)

Slide 135:
import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)
Array operations
Let’s see how we can write RMSE in numpy
Example :








Slide 136:
Let’s see how we can write RMSE in numpy
Example :







import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)
Array operations

Slide 137:
Let’s see how we can write RMSE in numpy
Example :







import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)
Array operations

Slide 138:
Let’s see how we can write RMSE in numpy
Example :







import numpy as np
v = np.array([1,2,3,4])
u = np.array([5,6,7,8])
# it is assumed the v and u have the same size
rmse = np.sqrt(np.sum(np.square(v-u))/v.size)
print("RMSE = ",rmse)
Array operations

Slide 139:
Serialization/Deserialization

Slide 140:
Serialization/Deserialization
Numpy arrays can be saved and loaded to disk. This allows to create checkpoints or to save models. A special extension (.npy) will be added when saving to a file.

Example:
import numpy as np
v = np.array([1,3,5,7,9])
np.save("my_array", v)
u = np.load("my_array.npy")
print(u)
my_array.npy file content

Slide 141:
Serialization/Deserialization
While the .save() method saves a file in a binary format, there is another method ( .savetxt(…) ) that can be used to save a file as a text (more suitable for debug purposes).

Example:
import numpy as np
v = np.array([1,3,5,7,9])
np.savetxt("my_array.txt", v)
u = np.loadtxt("my_array.txt")
print(u)
my_array.txt file content

Slide 142:
Serialization/Deserialization
In practice, complex models are formed out of multiple arrays (and it is easier to save all of them into an archive than in individual files). For this purpose, there are two methods: .savez(…) and .savez_compressed(…)  (“z” stands for zip). The default extension will be npz
Example:
import numpy as np
v = np.array([1,3,5,7,9])
u = np.array([[1,3],[5,7]])
t = np.array([1,2,3])
np.savez("my_arrays", vector1=v, matrix1=u, vector2=t)

Slide 143:
Serialization/Deserialization
In practice, complex models are form out of multiple arrays (and it is easier to save all of them into an archive than in individual files). For this purpose, there are two methods: .savez(…) and .savez_compressed(…)  (“z” stands for zip). The default extension will be npz
Example:
import numpy as np
v = np.array([1,3,5,7,9])
u = np.array([[1,3],[5,7]])
t = np.array([1,2,3])
np.savez("my_arrays", vector1=v, matrix1=u, vector2=t)
Array “u” will be stored in this file

Slide 144:
Serialization/Deserialization
To load a zip archive, create by the previous code, we can use the np.load(…) method and the property `files` (to access all arrays).

Example:
import numpy as np
data = np.load("my_arrays.npz")
print(data.files)
for fname in data.files:
    print(fname+" = ",data[fname])

Slide 145:
Serialization/Deserialization
Another interesting method is .genfromtxt(…) with the following definition:
numpy.genfromtxt( fname,                   dtype=<class 'float’>,                                    comments='#’,                   delimiter=None,                   skip_header=0,                   skip_footer=0,                   missing_values=None,                   filling_values=None,                   usecols=None,                   replace_space='_’,                   case_sensitive=True,                  …)

Slide 146:
Serialization/Deserialization
This method ( .genfromtxt(…) ) can be used to load data in a list format files (such as a .csv or .tsv file) into an array or matrix.

Some examples:
import numpy as np
# Load data from a regular cvs file with no headers
array = np.genfromtxt('csv_file.csv', delimiter=',')
# Load data from a cvs file with headers
array = np.genfromtxt('csv_file.csv', delimiter=',', skip_header=1)

Slide 147:
Questions?



================================================================================
COURSE 2: PERCEPTRON
================================================================================

Slide 1:
Course 3: The Perceptron Algorithm
Neural Networks

Slide 2:
Properties
Online Training
Batch Training
Adaline Perceptron
Conclusions

Overview

Slide 3:
Perceptron Properties

Slide 4:
Perceptron history
Proposed by Frank Rosenblatt in 1957
Based on a model proposed by Warren McCulloch and Walter Pits in 1943
It follows the way a simple neuron works

Picture from: https://cdn1.byjus.com/wp-content/uploads/2020/02/STRUCTURE-OF-NEURON.png 

Slide 5:
Perceptron Properties
The idea is simple: a weighted sum over the inputs is compared to a threshold. If it greater than the threshold, than the neuron fires.






Input1
Input2
Input3
Inputn
weight1
weight2
weight3
weightn

Slide 6:
Perceptron Properties
Makes decisions by weighting up evidence






Learning is performed by varying the weights and the threshold



Slide 7:
Algorithm
A more mathematical way to see this, is with some vectors and a dot product.



The previous equation can be written in a different way (to avoid using the threshold as a separate value).


Slide 8:
Algorithm
This form of the perceptron algorithm (that implies that a dot product must be bigger than 0) addresses the scenario with a binary label (a label can have two possible values – for example a label can indicate the presence or absence of a class).
Let’s write the perceptron equations for inputs with 2 elements:

Slide 9:
Algorithm
This form of the perceptron algorithm (that implies that a dot product must be bigger than 0) addresses the scenario with a binary label (a label can have two possible values – for example a label can indicate the presence or absence of a class).
Let’s write the perceptron equations for inputs with 2 elements:

Slide 10:
Algorithm

Slide 11:
Algorithm

Slide 12:
Algorithm
P(-3,0)
Since “P” is located on the line, then
1∙(-3) + (-4)∙0 + 3 = 0

Slide 13:
Algorithm
P(-7,-1)
Since “P” is located on the line, then
1∙(-7) + (-4)∙(-1) + 3 = 0

Slide 14:
Algorithm
P(1,1)
Since “P” is located on the line, then
1∙1 + (-4)∙1 + 3 = 0

Slide 15:
Algorithm
In fact, for every point that resides on this line, the line equation (1x-4y+3) will always result in 0

Slide 16:
Algorithm
P(-6,3)
But what if point “P” is not on the line. In this case:
1∙(-6) + (-4)∙3 + 3 = -15

Slide 17:
Algorithm
P(-11,0)
But what if point “P” is not on the line. In this case:
1∙(-11) + (-4)∙0 + 3 = -8

Slide 18:
Algorithm
P(7,3)
But what if point “P” is not on the line. In this case:
1∙(7) + (-4)∙(-3) + 3 = -2

Slide 19:
Algorithm
We can say that for all points on this side of the line the equation (1x-4y+3) will always result in negative number

Slide 20:
Algorithm
P(-9,-3)
Now let's see some point on the other side of a line
1∙(-9) + (-4)∙(-3) + 3 = 6

Slide 21:
Algorithm
P(0,0)
Now let's see some point on the other side of a line
1∙0 + (-4)∙0 + 3 = 3

Slide 22:
Algorithm
Similarly, every point from the other side of the line , when testing them against the equation (1x-4y+3) will always result in a positive number

Slide 23:
Algorithm
This means that the hyper-plane equation can used to separate points (some on one side that result in a positive value, and the other one on the other side that result in a negative value).

Slide 24:
Algorithm
It is important to understand that the assignment of those two zones (red and green) is dictated by how we want to interpret the label with value 1. We can consider that:
the class we are trying to classify corresponds to label 1, and everything else has the label -1
the class we are trying to classify corresponds to label -1, and everything else has the label 1


Slide 25:
Algorithm
So, the perceptron algorithm can be described as follows: given dataset, single class, labeled, can we find a hyperplane that separates all example of one class from the other ones ? (in the case below – can we separate the magenta points from the blue ones ?)

Slide 26:
Algorithm
In practice, data sets are not always linear-separable. In this case, the purpose is to find a hyper-plane that minimizes or maximizes one of the metrics (FPR, ACC, etc).

Slide 27:
Algorithm
The perceptron is also called a linear classifier since it is defined by a linear equation.
In a 2 dimensional space, the perceptron is described as a line.  In a space with 3 or more dimensions it is described as a hyperplane








Slide 28:
Algorithm
The coefficients (weights) define a normal vector, that we can consider a unit vector (length 1)
The bias defines the distance from the origin to the plane
The dot product          will be equal to |b|, for every point located on the plane
Will be less than |b| for every point located under the plane
Will be grater than |b| for every point located above b






b
w

Slide 29:
Training

Slide 30:
Training
At this point, we know that the perceptron algorithm is meant to search for a hyper-plane that can separate two data sets (as best of possible).
Let’s assume that we have the following dataset (for training):

Slide 31:
Training
At this point, we know that the perceptron algorithm is meant to search for a hyper-plane that can separate two data sets (as best of possible).
Let’s assume that we have the following dataset (for training):
These are scalar values that represent the data associated with a particular entry. We can say that these could be considered coordinates in a “n” dimensional space
Coordinate on dimension 1
Coordinate on dimension 2
Coordinate on dimension n

Slide 32:
Training
At this point, we know that the perceptron algorithm is meant to search for a hyper-plane that can separate two data sets (as best of possible).
Let’s assume that we have the following dataset (for training):
The label could be anything:
Yes/No or True/False (to reflect that the sample belongs or not to the class)
A string that reflects the name of the class and another one that reflects the rest (e.g. CAT and OTHERS) or (CLASS-A and CLASS-B)
Some numerical values (1/0) 

Slide 33:
Training
At this point, we know that the perceptron algorithm is meant to search for a hyper-plane that can separate two data sets (as best of possible).
Let’s assume that we have the following dataset (for training):
The target is to find a hyper-plane equation / or a vector “w=[w1,w2…wn]” and a threshold that will classify most (if not all) of the samples from the dataset that are labeled with YES on one side of the hyperplane and most (if not all) of the samples from the dataset that are labeled with NO to the other side. 

Slide 34:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)

Slide 35:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)

Slide 36:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)

Slide 37:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)
To simplify this equation, lets consider that all samples that are labeled YES are the ones that should be on the side of the hyper-plane where f(x) computes a positive value, and the ones labeled NO should be on the side of the hyperplane where f(x) computes a negative value.
We can change the labels as follows:
YES will be converted into 1, and NO will be converted into -1

Slide 38:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)
We can change the labels as follows:
YES will be converted into 1, and NO will be converted into -1

Slide 39:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)

Slide 40:
Training
Assuming we have the following dataset (for training):
No need for threshold (column Inputn+1)

Slide 41:
Training

Slide 42:
Training

Slide 43:
Training

Slide 44:
Training

Slide 45:
Training
Then, we can adjust “w” vector in the following way:
First let’s consider “w(iteration t)” the weight vector at a specific iteration

We can also consider “ft(k)” the output of the perceptron equation for sample “k” at iteration “t” 


We can adjust the weight vector in the following way:



Slide 46:
Training
As a general observation, the formula for adjusting the vector “w” can be written in different ways (but the logic is similar) – for the cases where these values need to be adjusted (the tested sample is not classified correctly) :


The bias (threshold) is included in the weights vector
(for cases 1 and 2)

Slide 47:
Training
Let’s put all of these together and build a training algorithm:



Slide 48:
Training
Let’s put all of these together and build a training algorithm:


classified is a Boolean value that will be true if the result is (>0) and false otherwise.

Slide 49:
Training
Let’s put all of these together and build a training algorithm:


Exit condition can be several things:
A number of iterations (epochs) is achieved
Every sample is correctly classified
A specific metric is achieved (e.g. FPR < 3%, ACC > 99%, etc)

Slide 50:
Training
Let’s put all of these together and build a training algorithm:


If threshold (β) is being used, then this equation below changes into:
w  w + sample.input x α x sample.label 
β  β + α x sample.label                

Slide 51:
Example

Slide 52:
Demo
1. Let’s consider the following training dataset that consists in points in a two dimensional plane:

Slide 53:
Demo
2. Now let’s create a weight vector initialized with random (small values) and a threshold (β)
This blue line is the graphical representation of the model (the weight vector and threshold).
0.1x + 0.1y + 0.05 = 0

Slide 54:
Demo
3. Let’s also consider the learning rate (α) as 0.02

Slide 55:
Demo
4. For the training part, we will check each entry and see if it is correctly classified. If not, we will adjust the weight vector.
Notice that the samples are sorted (first the ones with label 1 and then the ones with label -1). In practice it is best to shuffle them.

Slide 56:
Demo
5. Testing each entry (from 1 to 6) against the current model.
This point is correctly classified (is on the right side of the hyper-plane). As such, we don’t have to do anything,

Slide 57:
Demo
5. Testing each entry (from 1 to 6) against the current model.
Similar situation as with entry #1. This point is also correctly classified (its label is green, and it resides on the green part of the hyperplane).
As such, we don’t need to do anything  to change the hyper-plane.

Slide 58:
Demo
5. Testing each entry (from 1 to 6) against the current model.
This point however is not correctly classified (it does not reside on right part of the hyper-plan) and as such we will have to adjust the hyper plane

Slide 59:
Demo
5. Testing each entry (from 1 to 6) against the current model.

Slide 60:
Demo
5. Testing each entry (from 1 to 6) against the current model.
This point is correctly classified (it has the color red and it is on the red side of the hyper-plane)

Slide 61:
Demo
5. Testing each entry (from 1 to 6) against the current model.
This point is correctly classified (it has the color red and it is on the red side of the hyper-plane)

Slide 62:
Demo
5. Testing each entry (from 1 to 6) against the current model.
This point however is not correctly classified (it does not reside on right part of the hyper-plan) and as such we will have to adjust the hyper plane

Slide 63:
Demo
5. Testing each entry (from 1 to 6) against the current model.

Slide 64:
Demo
5. Testing each entry (from 1 to 6) against the current model.
Notice that at this point we have found a hyper-plane that separates all green dots from the red ones. At this point we can stop the algorithm.

Slide 65:
Batch training

Slide 66:
Batch training
Let’s consider the previous example, but with different arrangements of the order of the elements in the dataset.
All of these databases contain the same entries just organized in different order. As such, the training will be different for each case. All of these cases will eventually reach a point where the hyperplane separates all green and red dots. For each case we will record how many iterations (epochs) it takes to find the best hyperplane, and how many changes to the original weight vector it takes.

Slide 67:
Batch training
Let’s consider the previous example, but with different arrangements of the order of the elements in the dataset.
Epochs=2,Changes=4
Epochs=5,Changes=15
Epochs=9,Changes=22
Epochs=10,Changes=28
As seen, different shuffles of the same database produce different results.

Slide 68:
Batch training
Considering that:
At sample “t” from the training we have a “wt” and that we need to change it because it is not correctly classified

Then at sample “t+1” we will have:



This means that the way current algorithm is, we can not know in advance what is the value of wt+1 unless we have already computed wt. Furthermore, because of this, we can not parallelize the algorithm. 


Slide 69:
Batch training
Let’s write the original algorithm (this time using threshold):



Slide 70:
Batch training
Now let’s see how we can modify it to work with a batch:



Slide 71:
Batch training
Now let’s see how we can modify it to work with a batch:


Notice that we have introduced 2 new variables (Δ and Β) both of them initialized with 0. They store the relative change that needs to be applied to the hyperplane.

Slide 72:
Batch training
Now let’s see how we can modify it to work with a batch:


For every sample, we check it against the current hyper-plane (w and β). If the sample is not correctly classified, we modify the new parameters instead of the hyperplane. This makes the process be invariant to sample order. 

Slide 73:
Batch training
Now let’s see how we can modify it to work with a batch:


Finally, after every sample from the training set has been validated against the current hyperplane and we have computed the differences from all samples, we adjust the hyperplane and resume the process.

Slide 74:
Batch training
Now let’s see how we can modify it to work with a batch:


To simplify the process, let’s consider the following code as a separate function defined as follows:
train(trainingSet, weights, beta)  (Δ, Β)

Slide 75:
Batch training
This is how the train function looks like:



Slide 76:
Batch training
With this in mind we can modify the original algorithm as follows:






Now that we have changed the code in this way we can do some other things as well:
Mini batch
Parallel training




Slide 77:
Batch training
Let’s consider the following functions:
split(sampleset, number)  this method splits the sample set into multiple (disjunctive) sample sets provided by the parameter number. For example, assuming:
Sample set has 100 entries
We want to split in 4 batches
We will call split(sampleset,4). This will result in 4 sample sets: one that contains the first 25 entries, the second one that contains the entries from index 26 to index 50, and so on.
thread::run(command)  this will execute a specific command but on a different thread 




Slide 78:
Batch training
A parallel processing will look like this:










Slide 79:
Batch training
A mini-batch consists in the following:










Slide 80:
Overview training
A couple of additional observations:
Online training :
The order of elements is relevant
The update on the weights is performed after the processing of each element in the dataset
Parallelization is not possible. 
It is very sensible to outliers.
Batch training
The order of elements is not relevant
The update on the weights is performed after the processing of all the elements in the dataset
It can be parallelized by processing each batch in parallel
Less sensible to outliers since large gradients get diluted. 





Slide 81:
Overview training
A couple of additional observations:
Mini Bath training :
The order of elements is relevant, but can be made less relevant by increasing the size of the minibatch
The update on the weights is performed after each minibatch. This increases the number of updates by size(dataset)/size(minibatch). 
Parallelization is possible inside minibatch. -> suitable for GPU. 
Usual minibatch sizes are 64,128 depending on the GPU. This makes them less sensible to outliers.

         Currently, the most used technique when training neural networks


Slide 82:
Adaline perceptron

Slide 83:
Adaline perceptron
The training algorithm as discussed up to this moment has some limitations:
If the data set is not linear separable, the algorithm will never be able to achieve a stable state (the hyperplane will jump from one position to another)



Slide 84:
Adaline perceptron
The training algorithm as discussed up to this moment has some limitations:
If the data set is not linear separable, the algorithm will never be able to achieve a stable state (the hyperplane will jump from one position to another)


So, in fact, the hyper plane keeps moving between a couple of points, but it never achieves a stable state.

Slide 85:
Adaline perceptron
The training algorithm as discussed up to this moment has some limitations:
If the data is linear separable, the hyperplane obtained is not the best one (the one that best generalize the data) because we don’t change any weight if a sample is already classified.



Slide 86:
Adaline perceptron
The training algorithm as discuss up to this moment has some limitations:
If the data is linear separable, the hyperplane obtained is not the best one (the one the best generalize the data) because we don’t change any weight if a sample is already classified.


This is the best hyperplane. It is equally distance from the red and the green dots.
Same distance
Same distance

Slide 87:
Adaline perceptron
The training algorithm as discuss up to this moment has some limitations:
If the data is linear separable, the hyperplane obtained is not the best one (the one the best generalize the data) because we don’t change any weight if a sample is already classified.


This is the possible hyperplane that the algorithm might obtain. Notice that the distances are quite different.

Slide 88:
Adaline perceptron
The training algorithm as discuss up to this moment has some limitations:
If the data is linear separable, the hyperplane obtained is not the best one (the one the best generalize the data) because we don’t change any weight if a sample is already classified.


Furthermore, a point (from the testing set) that is just outside this hyperplane (even if very distant from the red dots) will not be classified correctly

Slide 89:
Adaline perceptron
The solution is to take into consideration how big the error is (even for the cases where a sample is correctly classified).



Slide 90:
Adaline perceptron
Adjusting weights even for correctly classified samples makes the Adaline Perceptron adapt past the point where every data point is correctly classified.
The stopping condition of the algorithm can not be one that check that all samples are correctly classified. 
It can be a number of iterations , a small overall error or both. 
The Adaline perceptron will adapt the hyperplane to consider the distribution of the elements from each class. This is because at each iteration the hyperplane gets modified for every point in the dataset in proportion to how far each data point is from the ideal label.
Same distance
Same distance

Slide 91:
Questions & Discussion

Slide 92:

http://neuralnetworksanddeeplearning.com/ by Michael Nielsen
Perceptrons: an introduction to computational geometry, by Marvin Minsky and Seymour Papert, 1969
http://blog.sairahul.com/2014/01/linear-separability.html, by Sai Raul, 17 January 2014
http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html, by Sebastian Raschka, 24 March 2015
https://sebastianraschka.com/faq/docs/diff-perceptron-adaline-neuralnet.html


Bibliography



================================================================================
COURSE 3: GRADIENT DESCENT
================================================================================

Slide 1:
Course 3: Gradient Descent
Neural Networks

Slide 2:
Feed Forward Network Architecture
Cost Function
Gradient Descent
Overview

Slide 3:
Neuron unit

Slide 4:
Neuron unit
We know that a perceptron can be described in the following way:


Slide 5:
Neuron unit
But we can view a perceptron as a unit formed out of the following components:
Weights (a vector of weights)

Sums up function (a function that computes the dot product between the vector of weights and its input)

Activation function (a function that takes the result of the previous function and convert that scalar value into another value).


Slide 6:
Neuron unit
This means that we can organize a perceptron in the following layers:



Slide 7:
Neuron unit
This means that we can organize a perceptron in the following layers:



Slide 8:
Neuron unit: notations
Scalar
Value

Slide 9:
Neuron unit: notations
Scalar
Value
x
w
z
y

Slide 10:
Neuron Unit: Activation functions
Activation functions were named like this because they replicated the activation process of a biological neuron. 
Activate (output 1) when weighted input is greater than a threshold.

In practice, some of the activation functions don’t do this, they just transform the input into another value (for example there are activation functions that perform normalization tasks).


Slide 11:
Neuron Unit:
Linear activation:
This is not really a function that is being used, more like a method to explain that no activation is needed.

y
z

Slide 12:
Neuron Unit:
Binary Threshold
If the input is above a threshold then output 1, else output 0 (or -1)

y=1
y

Slide 13:
Neuron Unit:
Binary Threshold
If the input is above a threshold then output 1, else output 0 (or -1)

y=1
y

Slide 14:
Neuron Unit:
Binary Threshold
If the input is above 0, output 1, else output 0 (or -1)

y=1
y
Threshold is usually learned as any other weight, considering input has another dimension with value 1

Slide 15:
Feed Forward Network Architecture
Why not use the binary threshold activation?
Not derivable
Also, no change in output when changing weights by a small value
y=1

Slide 16:
Feed Forward Network Architecture

Slide 17:
Activation functions
Logistic (sigmoid) function:

The function is always positive

This function normalizes values in 0..1 interval. The larger a value is, the close this functions tends to 1. The lower a number is (into the negative space) the closest the result of this function will be towards 0.

The slope of the function varies. It has a consistent slope on input between [-4,4] and the slope tends to get to 0 outside this interval. 

Sigmoid functions are commonly used in the output layer of binary classification neural networks, where they model the probability that an input belongs to one of the two classes.


Slide 18:
Neuronal Network Architecture

Slide 19:
A neuronal network is a network formed out of multiple neurons organized in layers as follows:
An input layer (a set of neurons – at least one -  that receive the input). 

Hidden layers (one or more layers of neurons that process the output from the input layer)

Output layer (a final layer that takes values from the previous layer and convert it into the final value).



Neuronal Network Architecture

Slide 20:
Observations:
The input layer must have at least one neuron 
Hidden layers are optional
The number of neurons in the hidden layers can vary from one hidden layer to another
Output layer consists in one neuron for regression or binary classification, but for multi-class classification it can contain multiple neurons.
A neuronal network with no hidden layer, one neuron in the output layer and with a binary threshold activation is a perceptron



Neuronal Network Architecture

Slide 21:
Let’s see a graphical representation of a fully connected neural network:This type of network is also called Multi Layer Perceptron (MLP), but this is confusing, since the perceptron is rarely used in this kind of architecture.
Neuronal Network Architecture

Slide 22:
The same network can be abstracted in the following way:
Neuronal Network Architecture

Slide 23:
A more detailed view of this architecture looks like this:
Neuronal Network Architecture

Slide 24:
A more detailed view of this architecture looks like this:
Neuronal Network Architecture
Let’s analyze this part of the architecture from a mathematical point of view. 

Slide 25:
A more detailed view of this architecture looks like this:

Neuronal Network Architecture
This looks like a matrix multiplication operation !
While all of these “k” dot products produces individual scalar values, we can also consider that each one of this operations in fact produce a part of a vector.  

Slide 26:
A more detailed view of this architecture looks like this:
Neuronal Network Architecture

Slide 27:
A more detailed view of this architecture looks like this:
Neuronal Network Architecture
Where W(k,1)…W(k,n) are the weights associates with NeuronUnitk 

Slide 28:
So, in fact, we can see a part of the original architecture presented as follows:
Neuronal Network Architecture

Slide 29:
So, in fact, we can see a part of the original architecture presented as follows:
Neuronal Network Architecture
Is it OK to apply the activation function over a vector and not a scalar as it was initially designed ?

Slide 30:
Neuronal Network Architecture
We can convert it to serve the same purpose as follows:
The main advantage when using this type of form is that you can also apply the activation function to the entire vector and not just for individual scalars.

Slide 31:
Or a more simplified version of a fully connected layer:
Neuronal Network Architecture

Slide 32:
As a result, we can describe a fully connected neuronal network as follows:
Neuronal Network Architecture
Input Layer
Hidden Layers
Output Layer

Slide 33:
Let’s see a more practical example with a fully connected neuronal network with 3 inputs, 2 hidden layers of 4 and 2 neurons and one output.
Neuronal Network Architecture
Input Layer
Hidden Layers
Output Layer

Slide 34:
Feed Forward Network Architecture
The neurons from the hidden layer should have a non-linear, derivable activation function. 
Example:  Let’s say we have a neural network with 1 hidden layer, 1 output layer and linear activation function. (so no activation)







Input Layer
Hidden Layer
Output Layer

Slide 35:
Feed Forward Network Architecture
Example:  Let’s say we have a neural network with 1 hidden layer, 1 output layer and linear activation function. (so no activation)


We’ve obtained a linear activation of the input. Just like we had only one layer

Slide 36:
It is also important to notice that a neuronal network can be fully connected (or dense) or not. 
Neuronal Network Architecture

Slide 37:
Cost function

Slide 38:
Cost function
Let’s consider this generic representation of a neuronal network.
To simplify this representation, let’s consider that this neuronal network is in fact a function – and lets call that function NNw

Slide 39:
Cost function
This means that we can represent a neuronal network in a mathematical way as follows:


Now, lets consider that our training set looks like this:

Slide 40:
Cost function
This means that we can represent a neuronal network in a mathematical way as follows:



Now, lets consider that our training set looks like this:

Slide 41:
Cost function
For the entry #2 from the training set, we will obtain the following equation:



This is also called predicted value

Slide 42:
Cost function
For the entry #2 from the training set, we will obtain the following equation:



Ideally, we would like the result of NN function to be:




Slide 43:
Cost function
For the entry #2 from the training set, we will obtain the following equation:



Ideally, we would like the result of NN function to be:



This actually translates that we need to find a way for the output vector (the prediction) to be closer to what we expect.

Slide 44:
Cost function
We will use:
X for input
Y for prediction. Y =NNw(x)
t for target (expected value)

Slide 45:
What is a cost (loss) function?:
Cost Functions
It quantifies how well or poorly a neural network is performing in terms of making predictions.

The primary purpose of the loss function is to measure the discrepancy between the network's predicted outputs and the actual target values (ground truth). 

A change of weights affects the cost function even if the number of (in)correctly classified elements doesn’t change.











Slide 46:
Cost function
One well known cost function is Mean Squared Error function:



Observations:
The cost is always positive
The closest the prediction vector is to the expected one, the closest the MSE will be to 0
Predictions that are far from target are penalized more


Slide 47:
Cost function
Mean Squared Error has some advantages:
Continuous and differentiable

Resembles the variance formula from regression (we can look at this formula as a way to evaluate how far the input vector is from a hyperplane defined by the network).

In practice, the MSE formula uses 1/2k instead of 1/k to allow easy derivation:



Slide 48:
Mean Square Error:
Cost Functions
Advantages:

Primarily used for regression tasks

Penalizes Larger Errors: MSE squares the error, so larger errors have a disproportionately larger effect on the loss. 

MSE is a convex function, which means that any local minimum is also a global minimum.

Disadvantages:

Very sensitive to outliers: A single large error can dominate the loss and lead to a model that performs poorly on the majority of the data 










Slide 49:
Gradient Descent

Slide 50:
Gradient Descent
The cost function tells us how bad we’re classifying an element. We can use this metric to tells us how bad we’re classifying a minibatch, or the entire dataset.


Where: * m is the size of the minibatch* yi,j, is the j-th element from the output vector        of the i-th input* ti,j is the j-th element from the target vector        for the i-th element

Slide 51:
Gradient Descent
The cost function tells us how bad we’re classifying an element. We can use this metric to tells us how bad we’re classifying a minibatch, or the entire dataset.


A shorter notation

Slide 52:
Mean Square Error:
Gradient Descent
So having a cost function we know how bad we’re classifying the entire dataset.






The training is no longer driven by directly comparing prediction and output, as in perceptron, but by the Cost Function.

This is an optimization problem. There are multiple ways to minimize/maximize a function. Neural networks use gradient descent.  
Minimize the cost function 
Increase Model Accuracy on training set
=

Slide 53:
Examples of graphs of loss functions with only 2 weights
Loss Functions
Source: https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

Slide 54:
Gradient Descent
OBS: We have two functions:
1. Function that maps the input to an output:

This is what we aim to obtain: Is what we’ve previously defined by NNw.
An example: a function that assigns a color to a point with 2 coordinates. 

This is a function of the input (x) and we’re interested in the output (y)

We do not know how this function should look like, so we change the weights to find the best function.  Each change of weights, generates a new function of the input.

A move along the input of the graph of the function is equivalent to predicting another input



Slide 55:
Gradient Descent
2. A Cost function

This is a function of the function we’re trying to find (NNw). 

The output of this function is a value that shows the discrepancy between the output of the first function (neural network) and the target. 

The cost function doesn’t change during training. 

A move along this graph is a change of weights in the neural network  this changes the first function (NNw)
Source: https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/

Slide 56:
Gradient Descent
https://www.linkedin.com/pulse/deep-learning-optimization-algorithms-gradient-descent-shashi-singh/
Example: How reduction in cost function translates to an improvement of the target function

Slide 57:
Gradient Descent
What is a gradient?
A gradient is just a fancy word for derivate 

The gradient (first derivative) determines the slope of the tangent of the graph of the function. (points in the direction of the greatest rate of increase)








The intuition behind this method is to take a step in the opposite direction









Slide 58:
Gradient Descent:

Slide 59:
Basic calculus notions
It’s important to highlight that there are several notation for derivative functions:



Slide 60:
Basic calculus notions

Slide 61:
Basic calculus notions

Slide 62:
Basic calculus notions

Slide 63:
Basic calculus notions
In this case “y” is a constant (as we compute the derivate of f with respect to x. As such, the derivation of a constant is 0

Slide 64:
Basic calculus notions

Slide 65:
Basic calculus notions

Slide 66:
Basic calculus notions
In this case “x” is a constant (as we compute the derivate of f with respect to y. As such, the derivation of a constant is 0

Slide 67:
Basic calculus notions

Slide 68:
Basic calculus notions

Slide 69:
Basic calculus notions

Slide 70:
Gradient Descent

Slide 71:
Gradient Descent

Slide 72:
Gradient Descent

Slide 73:
Gradient Descent

Slide 74:
Minimizing the Cost function:

The problem is that we do not change y directly. Y is the output of the neural network






So, to change y, we need to change w. So we’re interested in changing w to minimize the Cost function. 

Gradient Descent

Slide 75:
Gradient Descent

Slide 76:
Minimizing the Cost function:

As we’ve previously said, NNw is just a function.
 
So we need to compute the partial derivative of a compound function
MSE(NNw(w, x)) with respect to w
Gradient Descent

Slide 77:
Basic calculus notions

Slide 78:
Basic calculus notions

Slide 79:
Gradient Descent

Slide 80:
Gradient Descent

Slide 81:
Gradient Descent

Slide 82:
Gradient Descent

Slide 83:
Gradient Descent

Slide 84:
Gradient Descent
This is the same update rule we used for the Adaline in the previous course. The formula looks different since now we are averaging over all samples

Slide 85:
Adaline perceptron has no input activation. How would the gradient descent algorithm work for a neuron with logistic activation







Gradient Descent

Slide 86:
Gradient Descent

Slide 87:
Gradient Descent

Slide 88:
Gradient Descent

Slide 89:
Gradient Descent

Slide 90:
Using gradient descent we can optimize a network with just one output layer
However, can we train a network with multiple layers?

Backpropagation

Slide 91:
However, can we train a network with multiple layers?
Yes, but not yet ! 


Backpropagation
We know the error at the last layers, so we can update the immediate weights that affect that error 
The last layer is a function of the previous. So we need to apply the chain rule again. 


We need to backpropagate the error

Slide 92:
Questions & Discussion

Slide 93:
Bibliography
http://neuralnetworksanddeeplearning.com/
Chris Bishop, “Neural Network for Pattern Recognition”
http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html
http://deeplearning.net/







================================================================================
COURSE 4: BACKPROPAGATION
================================================================================

Slide 1:
Course 4: CrossEntropy & Backpropagation
Neural Networks

Slide 2:
Cross Entropy
Softmax
Backpropagation		
How to adjust hyper-parameters.

Overview

Slide 3:
The Problem With MSE

Slide 4:
The Problem With Quadratic Cost

Slide 5:
The Problem With Quadratic Cost
We have seen that a linear combination of linear classifiers is still a linear classifier. To achieve better classification accuracy, we need non-linearity

We have also introduced the sigmoid activation function which adds the necessary nonlinearity 







Slide 6:
The Problem With Quadratic Cost

Slide 7:
The Problem With Quadratic Cost

An important feature that we want from a neuron (and from a neural network) is to learn fast. For this to work, the weights must be lowered in direct proportion to how big the error is.

If the error is big, then big adjustment must be made in order to drive the cost down

If the error is small, then we want to make small adjustments to not overshoot our target






Slide 8:
The Problem With Quadratic Cost

Slide 9:
The Problem With Quadratic Cost

Slide 10:
The Problem With Quadratic Cost

Slide 11:
Cross Entropy

Slide 12:
Cross Entropy

Slide 13:
Cross Entropy

Slide 14:
Cross Entropy

Slide 15:
Cross Entropy
Just for one sample

Slide 16:
Cross Entropy

We will try to repeat the previous experiment, but this time with the cross entropy function.

A thing that we must also change is the learning rate. Learning rate is dependent on the cost function. Changing the learning rate is not cheating since we are interested in how the learning speed changes but rather the slope of the learning curve. 

	

Slide 17:
(more on)Cross Entropy

Where did the function come from?! (it looks very complicated at first sight)

Cross Entropy is specifically designed for classification. The target must be either one-hot, or must be a probability distribution


The term "cross-entropy" comes from information theory. It measures the difference between two probability distributions and quantifies how well one distribution predicts or approximates another. 

One hot vector for classifying digit 8
Soft target vector for classifying digit 8

Slide 18:
(more on)Cross Entropy

Slide 19:
(more on)Cross Entropy

Slide 20:
(more on)Cross Entropy

Slide 21:
(more on)Cross Entropy

Slide 22:
(more on)Cross Entropy

We can obtain an estimation by averaging this metric across the entire dataset.


Where i is the index of a sample, j is the index class, n is the number of samples and m is the number of classes

If the number of classes is just 2, the above formula becomes:

 
				

Slide 23:
(more on)Cross Entropy

Slide 24:
Softmax

Slide 25:
Softmax

Slide 26:
Softmax
When we’ve talked about Cross Entropy we have assumed that the output of the model is a distribution


However, when performing multi class classification and using linear or sigmoid activation, we do not have this guarantee.

For Cross Entropy to work efficiently, we need to convert the output layer to a distribution


Slide 27:
Softmax

Slide 28:
Softmax

Slide 29:
Activation functions

Slide 30:
Activation functions
Example:
So , for z = [1,3,4,2], softmax(v) = [0.03, 0.24, 0.64, 0.09]
We can also see softmax(z) as [3%, 24%, 64%, 9%].

This allows us to see what is most important element in the vector (the 3rd).

Observe that the difference between 3 and 4 is amplified from 4/3 (from 33% ) to 0.64/0.24 (166%)



Slide 31:
Activation functions

Slide 32:
How does the gradient look like when using Categorical CrossEntropy and  Softmax?

Softmax

Slide 33:
Softmax

Slide 34:
Softmax

Slide 35:
Softmax

Slide 36:
How does the gradient look like when using Categorical CrossEntropy and  Softmax?

Softmax

Slide 37:
How does the gradient look like when using Categorical CrossEntropy and  Softmax?

Softmax
Just for one sample
Obseration: the update rule is the same for CrossEntropy + sigmoid and CrossEntropy+Softmax

Slide 38:
Backpropagation

Slide 39:
Backpropagation
Layer 1
Layer 2
Layer 3
Layer 4

Slide 40:
Backpropagation
There are 2 main stages in backpropagation:
Forward Step 
Backward Step



Layer 1

Layer 2
Layer 3
Layer 4
Forward Step 
Backward Step 

Slide 41:
Backpropagation
Layer 1

Layer 2
Layer 3
Layer 4
Forward Step 

Slide 42:
Backpropagation
Layer 1

Layer 2
Layer 3
Layer 4
Backward Step 

Slide 43:
Backpropagation
Layer 1
Layer 2
Layer 3
Layer 4

Slide 44:
Backpropagation

Slide 45:
Compute the error for the final layer:




Backpropagation
Layer L-1
Last layer

Slide 46:
Backpropagate the error (write the error in respect to the error in the next layer)



Backpropagation
Layer l+1

Slide 47:
Compute how the cost function depends on a weight



Backpropagation
Layer 3
Layer 4

Slide 48:
Compute how the cost function depends on a bias



Backpropagation
Layer 2
Layer 3
Layer 4

Slide 49:
Backpropagation

Slide 50:
Backpropagation
Let’s see how this will look in a pseudocode:
Forward pass

Slide 51:
Backpropagation
Backward pass

Slide 52:
How to adjust hyper-parameters

Slide 53:
How to adjust hyper-parameters

Slide 54:
The first, and probably the most difficult is to achieve any non-trivial learning. You must obtain results better than you would obtain by a random selection.

In the case of MNIST digits, this means you should obtain something greater than 10%

Start with a smaller dataset. This increases the speed 
In the case of MNIST digits this could mean to work with only two numbers (0 and 1)
Start with a smaller network.
In the case of MNIST digits, that could mean to start with a network of 784x10 neurons. (No hidden layer)
How to adjust hyper-parameters

Slide 55:
Increase monitoring frequency
Work with just a fraction of the validation set
Monitor the accuracy not only on iterations, but also after you have computed some mini batches (for example, 10 minibatches)

All of the above steps are useful to allow you to receive quick feedback from the network. This allows to test many values for the parameters.


Start by adjusting the learning rate until you see some learning happens. 


How to adjust hyper-parameters

Slide 56:
How to adjust hyper-parameters

Slide 57:
You should start with a value for the learning rate where the training cost decreases in the first iterations.

Increase it by magnitude (10) until the network starts oscillating. This is the threshold

You can then refine it by slowly increasing it until the costs starts oscillating again (gets close to the threshold). In fact, the value should be a factor, or two below the threshold.


How to adjust hyper-parameters

Slide 58:
Choosing the number of iterations is simple: just use early stopping. That means, after each iterations test the network on the validation set. If after x iterations there is no improvement in the classification accuracy, then stop. X can be for example 10 iterations.


At the beginning you should let the network learn for a significant number of iterations in order to avoid the situation where it gets to a plateau only to continue learning again

How to adjust hyper-parameters

Slide 59:
How to adjust hyper-parameters

Slide 60:
Common batch sizes:
Small Batch Sizes (8-32):
Useful when data is limited or memory is constrained
Typically enhances generalization but slows down convergence due to noisier gradients

Medium Batch Sizes (32 - 256):
Popular choice as it balances training stability, speed, and generalization.
Works well on most standard tasks and networks 
Often a good starting point; adjustments can then be made based on validation performance.

Large Batch Sizes (512 or more):
Effective when training time is a high priority, and computational resources are abundant.
Often beneficial for massive datasets,
How to adjust hyper-parameters

Slide 61:
Mini Batch Size:
Start with a batch size that should fit in the GPU, example 32 or 64

Monitor validation accuracy against time (real time, not number of epochs) and .

Double the batch size and monitor validation accuracy. Doubling in batch size should translate in (almost) doubling the training speed.  Time per step should be almost the same if the memory of the GPU hasn’t saturated.

Stop when increasing the batch size is detrimental.
How to adjust hyper-parameters

Slide 62:
Questions & Discussion

Slide 63:
Bibliography
http://neuralnetworksanddeeplearning.com/
Chris Bishop, “Neural Network for Pattern Recognition”
http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html
http://deeplearning.net/
https://www.pinecone.io/learn/weight-initialization/








================================================================================
COURSE 5: WEIGHT INITIALIZATION & OVERFITTING
================================================================================

Slide 1:
Course 5: Improve Generalization
Neural Networks

Slide 2:
Other Activation Functions
Weight Initialization
Vanishing Gradient
Overfitting	
Regularization
Dropout
Max Norm Constraint
Increase Dataset	
Overview

Slide 3:
Activation Functions
Basic Neural Network Elements

Slide 4:
Logistic :
Activation Functions
y=0.5
y=1
-6
-4
-2
2
4
6

Slide 5:
Logistic :
Activation Functions
Advantages:

Since the output is [0,1] it is easy to represent probabilities

It’s form resembles the one of the binary threshold which make it similar to how we would think of a biological neuron
Used in binary classification with Binary Cross Entropy
Gradient is easy to compute y(1-y)

Disadvantages:

The output can be very close to zero. This can make learning very slow for neuron output values lower than -4

Computation during inference is more expensive. (compute exponential)


	

	

Slide 6:
Logistic :
Activation Functions
No gradient in 
this quadrant
No gradient in 
this quadrant

Slide 7:
Tanh (hyperbolic tangent):
Activation Functions
Source: https://medium.com/machine-intelligence-report/how-do-neural-networks-work-57d1ab5337ce

Slide 8:
tanh :
Activation Functions
Advantages:

Has steeper gradients in the interval [-1, 1]. This can make learning faster

It outputs both positives and negatives value. During backpropagation the sign of gradient of each weight can be different.

Used in weighting information

Disadvantages:


Computation during inference and training is more expensive.


	

	

Slide 9:
Weight Initialization

Slide 10:
Weight Initialization
Even if the network learns the weights, the initial value of the weights is important since these dictate how fast and to which local minimum the weights converge


The loss surface of a deep neural network, with many layers and many dimensions is complex and not convex. Choosing a good starting point is important.


The main initialization methods available are:

	* LeCun Initialization

	* Xavier or Glorot

	* He Initialization



	

	

Slide 11:
Weight Initialization

Slide 12:
Weight Initialization
y=0.5

Slide 13:
Weight Initialization
y=0.5

Slide 14:
Weight Initialization

Slide 15:
Weight Initialization

 
So, even though we have multiple neurons in the hidden layer, they will always have the same weights and same activation (at layer level).

Is almost as there is only one neuron in the hidden layer

It’s more efficient to just increase a learning rate and have a single neuron.
   					

Slide 16:
Weight Initialization
What if we would initialize all of them with a constant (different than 0)?

Slide 17:
Weight Initialization
What if we would initialize all of them with a constant (different than 0)?









It is the same scenario as initializing them with 0, it is just that we’ve skipped the first iteration.

The error and activations are different than the ones obtained by initializing with zero, but they are still the same across neurons from the same layer			

Slide 18:
Weight Initialization

 
So , to increase the change of the neurons learning different features we need to initialize the weights using random variables


Each neuron will produce a different output, which might be bigger on certain inputs, which will drive a bigger gradient, thus creating a feature detector.

   					

Slide 19:
Weight Initialization

Slide 20:
We will make an experiment:

We will consider a neural network with 5 layers

Weight Initialization
Input Layer
Hidden Layers
Output Layer

Slide 21:
Weight Initialization

Slide 22:
Observations :

The activations for each layer is almost every time -1 or 1. 
The number of activations that output the maximum value increases from layer to layer
Weight Initialization

Slide 23:
The problem with the activations being on extremes is that the gradient, that is needed in modifying the weights , is very small.

Thus, the learning will be very small. The neuron “saturates” on large activation values
	

Weight Initialization

Slide 24:
Weight Initialization

Slide 25:
Weight Initialization

Slide 26:
Weight Initialization
-14.28
14.28
-7.14
7.14

Slide 27:
Weight Initialization

Slide 28:
Observations:

Using very small values for the weights, leads to decreasing small activations in the upper layers. 

When computing the gradient we take into account the activation value as well 



So, we get into a situation where the network can’t learn since the weight updates are too small


Weight Initialization

Slide 29:
Weight Initialization

Slide 30:
Weight Initialization

Slide 31:
Weight Initialization

Slide 32:
Weight Initialization

Slide 33:
Weight Initialization

Slide 34:
Weight Initialization

Slide 35:
Weight Initialization

Slide 36:
Weight Initialization

Slide 37:
By using small weights, the network accuracy increases

Also, using small weights, the network arrives faster at the best accuracy


Weight Initialization

Slide 38:
Vanishing Gradient

Slide 39:
Vanishing Gradient
input
output
error

Slide 40:
Vanishing Gradient
Graph of tanh’
<1
<1
<1
<1
<1

Slide 41:
Tanh activation

Slide 42:

This makes the error that reaches the lower layers very small, depending on the number of layers

This makes the lower layers adapt very slowly, and with it the entire network, since the upper layers depend on the ones below
Vanishing Gradient
Layer 1
Layer 2
Layer 3
Layer 4

Slide 43:
Relu Activation

Slide 44:
ReLU

Slide 45:
ReLU
Source: https://medium.com/machine-intelligence-report/how-do-neural-networks-work-57d1ab5337ce

Slide 46:
ReLU
Advantages:

Having constant gradients, it does not contribute to the vanishing gradient problem

ReLU encourages sparsity in the network. Since it outputs zero for all negative inputs, many neurons will be inactive (i.e., have an output of zero) n. for any given input.

Very fast to compute both on feedforward and backpropagation side

Relu and its variants is the most widely used activation function

Slide 47:
Sparsity

Sparsity means to have the size of the feature vector larger than the size of input, but only a fraction of the feature vector is activated for an input

So a neuron will activate only in some
Inputs

ReLU encourages sparsity, since with a
weight initialization of mean 0, half of the
neurons will output 0
ReLU
Source: Deep Sparse Rectifier Neural Networks , Xavier Glorot, Antoine Bordes, Antoine Bordes

Slide 48:
Sparsity. Advantages:
Specialized features:
	When a neuron fires only on some certain input it effectively becomes 	specialized in detecting the input
More likely to be linear separable,
The next layer may contain more features than the previous, thus transforming the data into a multidimensional space where it might become separable
Reduced Redundancy:
Neurons might develop complementary features instead of redundant ones leading to a more diverse set of features



ReLU
Source: Deep Sparse Rectifier Neural Networks , Xavier Glorot, Antoine Bordes, Antoine Bordes

Slide 49:
ReLU
Disadvantages:


It is unbounded: can lead to exploding activations. This can be resolved by clipping the activation value. (i.e. ReLU6)

Not zero centered and it doesn’t output negative values

Dying ReLU: which can lead to slow learning after many iterations



Slide 50:
ReLU
Dying ReLU:

A dying ReLU outputs 0 on any input value.

From backpropagation, we know that the weights are updated based on the computed error:



The error at that neuron will always be zero and so the gradient. So its weights will not get modifies. The neuron … dies

Slide 51:
ReLU
Dying ReLU:

Some of the reasons why this happen are:

Large learning rate: if the error is negative, then a large learning rate might make a sufficient number of weights negative.

Imbalanced input distribution: The data might be distributed towards negative values.  Normalization is recommended

Poor weight initialization:  large weight updates might lead to large error, thus performing large updates (which might be negative)

Slide 52:
ReLU
Source: https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7, https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7


Slide 53:
ReLU

Slide 54:
ReLU
He initialization with ReLU activation
Xavier initialization with ReLU activation. 

Slide 55:
Overfitting

Slide 56:
Overfitting
Overfitting describes the process that happens when a model customizes itself too much to the data and does not generalize it

Which of the following models is better?





Slide 57:
Overfitting
Experiment:
We will train a neural network on the MNIST dataset with the same architecture from the last course (784, 36, 10), but this time we will use a training data of 1000 images instead of 50000.

We will use the same learning rate (0.3), the same mini batch size (10) and we will use cross-entropy as our cost function.

But since we use a smaller dataset, we will train it more iterations: 500 instead of 30.

(So nothing changes in the network but the training set size and the number of iterations)




Slide 58:
Overfitting
Experiment:
We will train a neural network on the MNIST dataset with one hidden layer ( (784, 36, 10), but we will use a fraction of the data set: 1000 images instead of 50000.




Input Layer
Hidden Layer
Output Layer
Learning rate: 0.3
Minibatch size: 10Nr Iterations: 500


We use a large nr of iterations to compensate for the small dataset size


Slide 59:
Cost of training data

The cost of the training data decreases as we have expected

At the end of the training cycle, the error is very small (0.005)

Overfitting

Slide 60:
Accuracy on testing data

As it can be seen in the graph, the best accuracy is achieved at about iteration 270. After that it just fluctuates around the same point

Beyond that point, the network is overtraining or overfitting

We should have stopped at iteration 270
Overfitting

Slide 61:
Accuracy on training data

And if we check the accuracy on the training data, from about iteration 42 is 1000 (100%).

That means we should have correctly identify all the numbers
Overfitting

Slide 62:
Cost of testing data

The cost of the testing data decreases until around iteration 20, after which it starts increasing

This shouldn’t have happened if the model correctly approximated the data. 

There should have been a continuous decrease in testing cost

Overfitting

Slide 63:
Overfitting
This clearly shows that the ANN learns particularities of the data and does not generalize. 

The main reason for why this happens is that the training data is to small compared to the size of the network.

Our network has: 784*36 + 36*10 weights and 36+10 biases. That means it has 28584 parameters that it can use in order to model 1000 elements.

How does the same network behave on the larger dataset (50000)







Slide 64:
Accuracy on test data vs training data

Overfitting still happens here, but to lower degree.

Beyond iteration 15, the network isn’t learning anymore. 

However, the difference between the two accuracies is not that much. Only 2.42 % vs 18%
Overfitting

Slide 65:
Solutions for Overfitting	

Slide 66:
Solutions for Overfitting	
A good way to detect that you are not overtraining is to compare the accuracy on training data and accuracy on test data. These should be close with the accuracy on training being slightly higher

So, increasing the training data does help to solve overfitting. With very large datasets, it becomes very difficult for a neural network to overfit.

Another obvious solution would be to reduce the network size

The solutions above are sometimes impractical:
Can not get more training data
It may take much too long to train on a larger data set
Large networks tend to be more powerful than smaller ones








Slide 67:
Solutions for Overfitting: Regularization	
Understanding: bias and variance

Bias:
The bias measures how far are the estimated elements (their average) from the target

Variance:
The variance measures how spread are the estimated elements from their mean











Slide 68:
Let’s suppose there are some darts players who want to hit the center of the target (approximate a point) and each players shoots many darts.

This is how the target would look depending on how biased are the estimated shoots from the target or how spread are they (variance).
Solutions for Overfitting: Regularization	

Slide 69:
Solutions for Overfitting: Regularization	

Slide 70:
Solutions for Overfitting: Regularization	

Slide 71:
Solutions for Overfitting: Regularization	

Slide 72:
Solutions for Overfitting: Regularization	

Slide 73:
Solutions for Overfitting: Regularization	








The model on the left underfits the data. It has high bias (big distance from the targets), but low variance. If we move one point, the line would probably not change very much
The model in the right, overfits the data. It has low bias (there is almost an exact match of the target points) but it has high variance. If we move one point there will probably be a big change in the model
The model in the center is the right one since it has lower bias than the one on the left and a lower variance from the one from the right



Slide 74:
Solutions for Overfitting: Regularization

Slide 75:
Solutions for Overfitting: Regularization	

Slide 76:
Solutions for Overfitting: Regularization	

Slide 77:
Solutions for Overfitting: Regularization	

Slide 78:
Solutions for Overfitting: Regularization	

Slide 79:
Solutions for Overfitting: Regularization	

Slide 80:
Solutions for Overfitting: Regularization	

Slide 81:
Solutions for Overfitting: Regularization	

Slide 82:
The idea of this technique is to make the network use less weights during training

On each minibatch that we train, we randomly select a fraction  from the hidden neurons and make them “invisible” (0)


Solutions for Overfitting: Dropout	
Layer 1
Layer 2
Layer 4

Slide 83:

We update the visible weights as before, using backpropagation
We restore the invisible hidden neurons and their connection
We then select another minibatch where we randomly select another fraction from the hidden neurons that we will make them invisible

After we have finished the training, we restore all the hidden neurons and adjust their activation to accommodate for the increase of variance in the input


Solutions for Overfitting: Dropout	

Slide 84:
Making a neuron “invisible” actually means that their activation will be 0.

Doing so, it’s activation will not matter to the upper layer , since all weight that connect it to the upper layer will be multiplied by 0

The weights that go into the neuron will also not be updated, since the error at that neuron will be 0 (since it is multiplied by the activation)





Solutions for Overfitting: Dropout	

Slide 85:
Solutions for Overfitting: Dropout	

Slide 86:

Implementation:






Solutions for Overfitting: Dropout	
Layer 1
Layer 2
x
x
=
During testing:

 No adjustments are necessary since the neurons were trained using a higher variance of activation values: 

Slide 87:
Advantages:
Makes the network robust to the loss of any individual neuron (Dying Relu)

Removes coadaptation of neurons, thus increases exploring of new features. 

The resulted model is less likely to overfit since is the  average of multiple models








Solutions for Overfitting: Dropout	

Slide 88:
Advantages:

Involves more neurons in learning. 
Neurons that activate on features from the input, produce a higher activation and thus their weight will be increased. This might make the Network rely only a part of the neurons

    

Solutions for Overfitting: Dropout	
3
1
2

Slide 89:
Improvement of the accuracy by using 20% dropout in the hidden layer when used on the entire dataset



Solutions for Overfitting: Dropout	

Slide 90:
Improvement of the accuracy by using 
50% dropout in the hidden layer,
On a dataset of just 1000 elements



Solutions for Overfitting: Dropout	

Slide 91:
Solutions for Overfitting: Maxnorm	

Slide 92:
Improvement of the accuracy, on a dataset of just 1000 elements, by using :

50% dropout in the hidden layer
Maxnorm of 5 for hidden layer + 50% dropout



Solutions for Overfitting: Maxnorm	

Slide 93:
As we’ve seen earlier, when using a bigger dataset, our network didn’t overfit that bad

So, another idea is to increase the data. 
Since, sometimes it is difficult to find training data, a solution is to add small noise to our existing data set using different functions, like slightly rotating the image or flipping it horizontally

This is called data augmentation

Solutions for Overfitting: Increase Dataset

Slide 94:
Data augmentation should be tailored to the dataset and problem it tries to solve.

Ex: Data augmentation in image processing:

Valid operations could be:
Rotation
Scaling
Flipping
Zooming
Skewing	
Applying a patch over a part of an image 

Solutions for Overfitting: Increase Dataset

Slide 95:
Data augmentation should be tailored to the dataset and problem it tries to solve.

Ex: Data augmentation in image processing:

An invalid operation, depending on the problem might me:
Flipping a photograph vertically
Applying operations to the image without modifying the metadata:
Example coordinates for an object In object detection problem

Solutions for Overfitting: Increase Dataset

Slide 96:
In “Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis, by Patrice Simard, Dave Steinkraus, and John Platt (2003) the authors were using a neural network with 800 hidden neurons to classify MNIST digits.

They achieved an increase of accuracy from 98.4% to 98.9% by:
Rotations
Translating
Skewing the image
By using “elastic distortions” (that should emulate random oscillations found in human muscles) they increased the accuracy to 99.3%

Solutions for Overfitting: Increase Dataset

Slide 97:
Questions & Discussion

Slide 98:
Framework created by Andrej Karpathy
http://cs.stanford.edu/
Demo 

Slide 99:
http://neuralnetworksanddeeplearning.com/
http://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html
Nitish Srivastava , Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Dropout: “A Simple Way to Prevent Neural Networks from Overfitting”, Journal of Machine Learning Research 15, (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/

Bibliography



================================================================================
COURSE 6: OPTIMIZERS
================================================================================

Page 1:
Internal Use Only
Neural Networks
Course 6: Optimizers

Page 2:
Internal Use Only
Overview
Space & memory considerations
►
Need of optimizers
►
Momentum and Nesterov Acclerated Gradient
►
RProp
►
Adagrad
►
RmsProp
►
Adam
►
Adadelta
►
Conclusions
►

Page 3:
Internal Use Only
Space & memory considerations

Page 4:
Internal Use Only
Space & memory considerations
Let’s see how a neural network looks like:
n(cid:0)m m (cid:0)m m (cid:0)m m (cid:0)m m (cid:0)k
1 1 2 2 3 p-1 p p
Input Layer Hidden Layers Output Layer

Page 5:
Internal Use Only
Space & memory considerations
Let’s see how a neural network looks like:
Layer
n(cid:0)k m (cid:0)m m (cid:0)m m (cid:0)m m (cid:0)k
1 2 1 2 2 3 … n p-11 p p
1 W 1,1 W 1,2 … W 1,n 1 Bias
1
2 W W … W 2 Bias
2,1 2,2 2,n
2
X 3 W 3,1 W 3,2 … W 2,n + 3 Bias 3
4 W W … W 4 Bias
4,1 4,2 3,n 4
5 W W … W 5 Bias
5,1 5,2 4,n 5
… …
k W W … W k Bias
k,1 k,2 k,n k
Input Layer Hidden Layers Output Layer

Page 6:
Internal Use Only
Space & memory considerations
The number of parameters of a model is the total number of weights and biases
from the model. For example:
784(cid:0)32 32(cid:0)20 20(cid:0)10
This means that when you hear that a LLM model has let's say 7B parameters,
this means that the sum of the number of parameters from each layer is 7
billions.

Page 7:
Internal Use Only
Space & memory considerations
It’s also important to understand that the output of each layer need also to
be stored in order to use it for:
► Feed-forward step (to compute the next layer)
► Backpropagation step (to compute the gradients)
n(cid:0)k k(cid:0)p
The intermediate result must be stored in
order to be used as an input for the next
layer

Page 8:
Internal Use Only
Space & memory considerations
It is important to note that on the feedforward step we don’t need to keep
an intermediate output more than it is required to compute the next layer.
Furthermore, the memory associated with that intermediate output can be
reused.
When it comes to training however, the activation of each layer must be
stored for the backpropagation algorithm.
As we will see, the size can also be affected by the optimizer that we
choose.

Page 9:
Internal Use Only
Space & memory considerations
Using minibatch implies that we first compute (and store) the gradients
and then we update the weight and bias matrixes. This means that the
actual representation of a layer would look like this:
Layer
n
n
k k
k
k
This also means that the training
Weight Biases implies a size twice as large as
Gradients for Gradients for
Matrix Vector the actual size of the model.
Weight Matrix Biases Vector

Page 10:
Internal Use Only
Space & memory considerations
So, a training operation on minibatch requires 2 to 3 times the size of the
model:
Size of the weights (model size)
•
Size of the activations
•
Size of the gradients (model size)
•
All this data must be in the GPU. Besides that we also have the training and labels for
the minibatch data.
The memory space allocated for GPU is very limited. Theoretically a minibatch operation
on a gpu takes approx. the same amount of time, no matter the nr of inputs (if the data is
in GPU memory)
Larger minibatches lead to faster convergence

Page 11:
Internal Use Only
Need of optimizers

Page 12:
Internal Use Only
Need of optimizers
►

Page 13:
Internal Use Only
Need of optimizers
► How gradient descent works?
We’re computing partial derivatives, so we
can always imagine that when computing a
gradient we’re in this case.
The gradient w.r.t to a weight has two parts:
• The sign : gives us direction
• The magnitude: tells us how steep is the
Error function at that given point.
There is no other direction than left or right (
+ or -)

Page 14:
Internal Use Only
Problems of SGD
► How gradient descent works?
If we think about a linear neuron with two weights, then the graph turns into a
quadratic bowl
Source:
https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gra
dient-descent/

Page 15:
Internal Use Only
Need of optimizers
► How gradient descent works?
If we take vertical section then we will get parabolas; if we take horizontal sections we will
get elliptical contours, where the minimal error is at the center
When we apply back-propagation on these weights, we compute how the Error function is
minimized in each of these (two) directions, combine the vectors and move in that direction

Page 16:
Internal Use Only
Need of optimizers
► How gradient descent works?
The resulted vector is perpendicular to the contour line (steepest descent)
It’s obvious that we want to go downhill, right to the center; but the gradient will
point in that direction only if the ellipse is a circle. (which rarely happens)
Since we make a move that is direct proportional to the size of the gradient, it is
possible to move a big step in the wrong direction (across the ellipse) and a little step
towards the good direction (along the ellipse)

Page 17:
Internal Use Only
Need of optimizers
► Surface of the Loss function
A network with many parameters and non linear activations rarely have a convex function.
Usually, the surface of the loss function has multiple local minimums and different shapes
connecting them, many time with multiple saddle points
Source: http://www.offconvex.org/2016/03/22/saddlepoints/

Page 18:
Internal Use Only
Need of optimizers
► Surface of the Loss function
A saddle point is defined as a place on the Loss Function
where the overall gradient is zero, but the point is not a
maximum nor a minimum.
Arriving at a saddle point makes learning very difficult,
since using the first derivative is useless (is zero)
The number of saddle points outnumbers the local
minimum and local maximums.
source: https://en.wikipedia.org/wiki/Saddle_point

Page 19:
Internal Use Only
Need of optimizers
► Surface of the Loss function
A saddle point is often surrounded by plateaus(same value
for loss), meaning the saddle is not steep. This makes
learning difficult.
One observation is that saddle points are “unstable”.
Meaning, if noise is added , that will move the loss
function beyond the saddle point where some gradients are
different than zero.
Using a smaller minibatch could add the necessary noise,
but this also increases training time.
Source: http://www.offconvex.org/2016/03/22/saddlepoints/

Page 20:
Internal Use Only
Need of optimizers
How learning rate affects error
minimization ?
If we use a small learning rate, then
there will not be an important
movement in the correct direction
If we use a large learning rate, then
the error will oscillate (overshoot)
across the parabola

Page 21:
Internal Use Only
Need of optimizers
How learning rate affects error
minimization ?
This happens especially for SGD
(where the updates are performed only
on a part of the data).
This can bring big oscillations in the
Error functions (which sometimes can
be beneficial).
A solution is to gradually reduce the
learning rate
Fluctuation in the total objective function as
gradient steps w.r.t. mini-batches
Source: wikipedia

Page 22:
Internal Use Only
Need of optimizers
► Decreasing learning rate
Decreasing the learning rate reduces fluctuations when using small minibatches.
However, this also means a reduced speed in learning, so it shouldn’t be done too
soon.
A good criteria is to monitor the error on a validation set. If the errors drops
gradually and consistent, then the learning rate can be increased.
If the error doesn’t drop anymore, the learning rate should be decreased

Page 23:
Internal Use Only
Notation
►

Page 24:
Internal Use Only
Momentum

Page 25:
Internal Use Only
Momentum
Intuition:
The next gradient will likely be in the direction as the previous. So make a bigger move in
that direction.
This is because:
• When updating the weights we only use a fraction of the previous gradient (by using a small learning rate)
• There are only two possible directions (positive or negative )
Momentum works by constantly adding previous gradient. Small steps add up. It is like
having acceleration

Page 26:
Internal Use Only
Momentum
Simple SGD SGD with momentum
► ►
Since steps are taken according to With momentum, small but steady steps
the gradient, it may take far too build up and the gradients in opposite
many steps for the SGD to reach directions cancel out
the minimum

Page 27:
Internal Use Only
Momentum
►

Page 28:
Internal Use Only
Momentum
• Momentum is computed across minibatches
• This reduces the zig-zag pattern that might be present in Stochastic gradient
descent because outliers matter less over the long run.
• Example:
Two minibatches with 32 samples from which 4 are outliers.
Mini-Batch 1 Mini-Batch 2

Page 29:
Internal Use Only
Momentum
Momentum implies we store the velocity factor for each weight and bias
Layer
n
n n
k k
k k k k
Weight Biases
Gradients for Gradients for Momentum for Momentum for
Matrix Vector
Weight Matrix Biases Vector Weight Matrix Biases Vector
In other words, we triple the size of
a layer by adding a new matrix and
bias vector.

Page 30:
Internal Use Only
Momentum
Considerations:
• Use a small learning rate. This allows to make small adjustments near the
minimum.
• Momentum helps travel the saddle point of functions, since:
very small steps add up.
•
the added velocity adds noise before arriving at the saddle point, making
•
the learning not stop. (it give it a push)
• The combination of a small learning rate and momentum allows for a
balance between exploration (navigating through the loss landscape to
avoid local minima) and exploitation (fine-tuning the parameters to reach
the lowest point in a minimum).

Page 31:
Internal Use Only
Nesterov Accelerated Gradient

Page 32:
Internal Use Only
Nesterov Accelerated Gradient
NAG is really a modified version of momentum
►
The difference is that at each moment, we can look ahead and approximate where the
►
momentum will take us on the next move.
Based on that approximation (which could be slightly off course) we can better adjust
►
our direction
Momentum Nesterov Accelerated Gradient
L
o
p o
k
u
m
s t e
m
s t
e p a h e a
d g
ra
m e n t al St e p n t u d ie n t
u e
o A ct m
M
o
M
Actual
Step
Gradient

Page 33:
Internal Use Only
Nesterov Accelerated Gradient
►

Page 34:
Internal Use Only
Nesterov Accelerated Gradient
►

Page 35:
Internal Use Only
Nesterov Accelerated Gradient
The 'look-ahead' step is the key to NAG's effectiveness. By calculating the gradient at
the look-ahead position, NAG anticipates the future landscape of the loss function.
If this look-ahead step results in a parameter position that is in a less favorable direction
(e.g., heading towards a higher loss), the gradient computed there will correct this,
forcing the updates towards a more optimal position.
This is particularly useful in scenarios where the loss surface has sharp curves or local
minima, as it allows NAG to adjust its trajectory more intelligently than standard
momentum, which only looks at the past gradients.

Page 36:
Internal Use Only
Nesterov Accelerated Gradient
As a general idea:
NAG often converges faster and more effectively than standard
►
momentum, especially in complex, high-dimensional spaces typical in
deep learning.
It is more stable and less prone to the problem of overshooting the
►
minimum, a common issue with algorithms that rely heavily on
momentum.
Overshooting happens when the update steps during the
optimization process are too large, causing the algorithm to
miss the minimum of the loss function and jump over to the
other side of the curve. This can happen if the learning rate
is set too high or if the momentum term accumulates
excessively without sufficient correction

Page 37:
Internal Use Only
Rprop
Resilient Propagation

Page 38:
Internal Use Only
Resilient Backpropagation (Rprop)
Resilient backpropagation (Rprop) is an optimization algorithm designed
to eliminate some of the issues faced by gradient descent methods.
RProp focuses solely on the direction of the gradient and not its
magnitude. This means that the update is performed based on the sign of
the gradient for each individual weight and bias in the network.

Page 39:
Internal Use Only
Rprop
►

Page 40:
Internal Use Only
Resilient Backpropagation (Rprop)
►

Page 41:
Internal Use Only
Resilient Backpropagation (Rprop)
►

Page 42:
Internal Use Only
Resilient Backpropagation (Rprop)
Just like momentum, a separate set of values (update values) and the
signed of the previous gradient have to be stored for each weight and
bias from the network
Layer
n n n n
+
+ + - + -
+
- + - + -
-
+ - + + -
k k k k k k k + + - + - k -
+
+ - + + +
-
- + + - -
Weight Biases Gradients for Gradients for Update values Update values Prev. Grad. Signs Prev. Grad. Signs
Matrix Vector Weight Matrix Biases Vector for Weight Matrix for Biases Vector for Weight Matrix for Biases Vector
In other words, we at least triple the size of a layer by adding
a two new matrixes and two new bias vector. The sign
matrixes and bias vector can be optimized (by using bit-sets)

Page 43:
Internal Use Only
Rprop
Advantages of Rprop:
Since the update is independent of the size of the gradient, it can escape
•
plateaus very quickly (just like momentum)
It tends to be more stable and less sensitive to the specific topology of the error
•
landscape, as it is not as affected by small, noisy gradients or very steep
gradients.
There is no need to tune a learning rate or a momentum factor
•
One of the fastest algorithms available
•

Page 44:
Internal Use Only
Rprop
Disadvantages of Rprop
:
It doesn’t work with minibatches (not suitable for large datasests)
►
For SGD, if a small (constant) learning rate was used, and we have nine minibatch
gradients of +0.1 and one gradient of -0.9, they will cancel each other (weight stays the
same for the entire dataset)
On Rprop, we will increase the step size 9 times, and decrease it once

Page 45:
Internal Use Only
Adagrad
Adaptive gradient

Page 46:
Internal Use Only
Adaptive Gradient Algorithm (AdaGrad)
Adaptive Gradient Algorithm (AdaGrad) is an optimization method that
provides an adaptive learning rate for each parameter.
The learning rate for each parameter is adapted by scaling it with the
inverse square root of the sum of all past squared gradients for that
parameter.
Parameters with larger past gradients receive a smaller learning rate, and
the ones with smaller past gradients receive a bigger learning rate.

Page 47:
Internal Use Only
Adagrad
►

Page 48:
Internal Use Only
Adaptive Gradient Algorithm (AdaGrad)
In case of AdaGrad, we need to store for each parameter is the sum of
the squares of the gradients. This sum is then used to scale the global
learning rate for each parameter individually during the update step.
Layer
n n n
k k k k k k
Weight Biases Gradients for Gradients for Sum of squared gradients Sum of squared gradients
Matrix Vector Weight Matrix Biases Vector for Weight Matrix for Biases Vector
In other words, we triple the size of a layer
by adding a matrix and vector for each later

Page 49:
Internal Use Only
Adagrad
Observations:
• Big gradients will receive small learning rates, while small gradients will receive
a bigger learning rate
• The starting learning rate on Adagrad is important since this is the largest
learning rate for each weight

Page 50:
Internal Use Only
Adagrad
►

Page 51:
Internal Use Only
Rmsprop
Root mean square propagation

Page 52:
Internal Use Only
Root Mean Square Propagation (RMSprop)
Root Mean Square Propagation (RMSprop) is an adaptive learning rate
optimization algorithm, based on AdaGrad and design to fix some of the
problems AdaGrad has.
RMSprop modifies the gradient accumulation process of AdaGrad by
using an exponentially weighted moving average of the squared
gradients. This means it doesn't accumulate all past squared gradients
but instead gives more weight to the more recent gradients.

Page 53:
Internal Use Only
RMSProp
►

Page 54:
Internal Use Only
Root Mean Square Propagation (RMSprop)
Just like in the case of AdaGrad, we need to store for each parameter is
the sum of the squares of the gradients. This sum is then used to scale
the global learning rate for each parameter individually during the update
step.
Layer
n n n
k k k k k k
Weight Biases Gradients for Gradients for Sum of squared gradients Sum of squared gradients
Matrix Vector Weight Matrix Biases Vector for Weight Matrix for Biases Vector
In other words, we triple the size of a layer
by adding a matrix and vector for each later

Page 55:
Internal Use Only
Root Mean Square Propagation (RMSprop)
►

Page 56:
Internal Use Only
Adam
Adaptive Moment Estimation

Page 57:
Internal Use Only
Adaptive Moment Estimation (ADAM)
Adaptive Moment Estimation (ADAM) is an optimization algorithm that
combines ideas from both Momentum and RMSprop to update network
weights.
Adam calculates an exponential moving average of the gradient and the
squared gradient.

Page 58:
Internal Use Only
Adam
►

Page 59:
Internal Use Only
Adam
►

Page 60:
Internal Use Only
Adaptive Moment Estimation (ADAM)
In case of Adam we need to store two matrixes (one for momentum) and
another one for the RMSProp-like computations.
Layer
n n n n
k k k k k k k k
Weight Biases Gradients for Gradients for Momentum Momentum RMSProp RMSProp
Matrix Vector Weight Matrix Biases Vector for Weight Matrix for Biases Vector for Weight Matrix for Biases Vector
In other words, we have quadrupled the size of a layer by
adding a two new matrixes and two new bias vector.

Page 61:
Internal Use Only
Adaptive Moment Estimation (ADAM)
Advantages:
• Adam adjusts the learning rate for each parameter individually based on estimates
of first (mean) and second (uncentered variance) moments of the gradients.
• Efficient for problems with large datasets and high-dimensional parameter spaces,
which is typical in deep learning.
• Well-suited for problems with sparse gradients (e.g., Natural Language Processing
and Computer Vision tasks)
• Has a bias corrections to the first and second moment estimates, which counteract
the biases toward zero that might occur especially in the initial time steps.
• Requires less tuning of the hyperparameters (ADAM can often be used out of the
box with default settings and it produces good results).

Page 62:
Internal Use Only
Adaptive Moment Estimation (ADAM)
Disadvantages:
► Sensitive to Initial Learning Rate.
► Potential for Overfitting (due to its rapid convergence)

Page 63:
Internal Use Only
Nesterov-accelerated Adaptive Moment
Estimation

Page 64:
Internal Use Only
Nesterov-accelerated Adaptive Moment Estimation
Nesterov-accelerated Adaptive Moment Estimation (NADAM), is an
optimization algorithm that combines the ideas of Nesterov momentum
with Adam. It's essentially Adam with Nesterov momentum integrated into
the moment estimates.
NADAM incorporates the Nesterov momentum by modifying the way the
first moment (the moving average of the gradients) is calculated. Instead
of using the current gradient to update the first moment as in Adam,
NADAM uses the lookahead gradient, as is done in Nesterov accelerated
gradient (NAG).

Page 65:
Internal Use Only
Nesterov-accelerated Adaptive Moment Estimation
►

Page 66:
Internal Use Only
Nesterov-accelerated Adaptive Moment Estimation
►

Page 67:
Internal Use Only
Nesterov-accelerated Adaptive Moment Estimation
Observations:
• NADAM maintains the same advantages and disadvantages as ADAM
• Potentially faster convergence and better performance on certain problems
• It is particularly useful for tasks where the benefits of Nesterov momentum, which
anticipates the future gradient, are relevant
• Like Adam, Nadam can sometimes converge rapidly to suboptimal solutions for
certain kinds of problems, especially those with noisy or sparse gradients.
• It can be more sensitive to the choice of hyperparameters compared to Adam, due
to the additional complexity introduced by the Nesterov term.

Page 68:
Internal Use Only
AdaDelta
Adaptive Delta

Page 69:
Internal Use Only
AdaDelta
AdaDelta is an extension of AdaGrad that instead of accumulating all
past squared gradients, it computes an exponential running average on
past gradients
It was proposed to address the diminishing learning rates of AdaGrad,
which can stop learning altogether too early in training. By maintaining the
moving average of gradient information, AdaDelta continues to learn and
adapt as training progresses.
A key advantage of AdaDelta is that it does not require an external
learning rate. This method derives its own learning rate from the data.

Page 70:
Internal Use Only
AdaDelta
►

Page 71:
Internal Use Only
AdaDelta
In case of AdaDelta we need to store for each parameter the exponential
moving average (EMA) and the exponential moving average for square
parameter update (EMASU).
Layer
n n n n
k k k k k k k k
Weight Biases Gradients for Gradients for EMA EMA EMASU EMASU
Matrix Vector Weight Matrix Biases Vector for Weight Matrix for Biases Vector for Weight Matrix for Biases Vector
In other words, we at quadruple the size of a layer by adding a
two new matrixes and two new bias vector.

Page 72:
Internal Use Only
Adadelta
►
Source:medium.com

Page 73:
Internal Use Only
Adadelta
►

Page 74:
Internal Use Only
Adadelta
►

Page 75:
Internal Use Only
Adadelta
►

Page 76:
Internal Use Only
Optimizers comparison
2 hidden layers:
1st: 500 neurons
2nd: 300 neurons
Last layer, softmax (10 neurons)
Used relu instead of sigmoid
Source:Matthew D. Zeiler

Page 77:
Internal Use Only
Optimizers comparison
Source: Alec Radford

Page 78:
Internal Use Only
Optimizers comparison
Source: Alec Radford

Page 79:
Internal Use Only
Questions & Discussion

Page 80:
Internal Use Only
Bibliograpy
► http://neupy.com/versions/0.1.4/2015/07/04/visualize_backpropagation_algorithms.html
► http://cs231n.github.io/neural-networks-3/
► https://intelligentartificiality.wordpress.com/2016/02/19/adaptive-stochastic-learning-via-the-ad
adelta-method/
► https://www.quora.com/What-is-an-intuitive-explanation-of-the-AdaDelta-Deep-Learning-opti
mizer
► https://arxiv.org/pdf/1609.04747.pdf
► http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html
► https://arxiv.org/pdf/1212.0901v2.pdf
► https://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/
► https://visualstudiomagazine.com/articles/2015/03/01/resilient-back-propagation.aspx
► http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
► http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf
► https://www.khanacademy.org/math/calculus-home/series-calc/taylor-series-calc/v/generalized-
taylor-series-approximation



================================================================================
COURSE 7: PYTORCH
================================================================================

Slide 1:
Course 7: Pytorch
Neural Networks

Slide 2:
PyTorch
What is PyTorch
Data types
Tensors
Layers
Module base class
Datasets & DataLoaders
Checkpoints
Backpropagation in Pytorch

Overview

Slide 3:
What is PyTorch

Slide 4:
What is PyTorch
A Python library that provides tensor operations (with support for GPU) and various deep learning out-of-a-box functionalities.

Current version: 2.9.1 (Nov.2025)
Site: https://pytorch.org/  
GitHub: https://github.com/pytorch/pytorch 
Documentation: https://pytorch.org/docs/stable/index.html  


Slide 5:
What is PyTorch
To install PyTorch, use the following command:


https://pytorch.org
For cpu only: pip3 install torch torchvision torchaudio

Slide 6:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):


Tensor operations (e.g., torch.add, torch.mul)
Mathematical operations (e.g., torch.sin, torch.log)
Reduction operations (e.g., torch.mean, torch.sum)


Slide 7:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):


Layers (e.g., torch.nn.Linear, torch.nn.Conv2d)
Activation functions (e.g., torch.nn.ReLU, torch.nn.Sigmoid)
Loss functions (e.g., torch.nn.CrossEntropyLoss, torch.nn.MSELoss)
Utilities for building neural network models (torch.nn.Module base class)
Predefined models for various tasks (e.g., ResNet, VGG)

Slide 8:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):


Stochastic Gradient Descent (torch.optim.SGD)
Adam (torch.optim.Adam)
RMSprop (torch.optim.RMSprop)

Slide 9:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):


Provides the Variable class, which is now mostly integrated directly with tensors via the requires_grad attribute.
Manages the automatic differentiation of operations on tensors.
Contains the Function class, which forms the basis for the dynamic computation graph.

Slide 10:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):



Slide 11:
What is PyTorch
PyTorch consists in multiple modules (with different functionalities):
Besides this, the following modules are also supported:
torchvision, torchaudio and torchtext (tools, models and utilities for audio, vision and text processing)

jit – a Just In Time compiler to convert PyTorch code into a form that can be optimized and executed in various environments without a dependency on the Python runtime

onnx - exporting PyTorch models in the Open Neural Network Exchange (ONNX) format, which can then be consumed by various deep learning frameworks and tools.


Slide 12:
Data Types

Slide 13:
Data types
PyTorch supports the most of the same types as NumPy supports.
Integers:
int8 : 8-bit signed integer (-128 to 127)
int16 : 16-bit signed integer (-32768 to 32767)
int32 : 32-bit signed integer (-231 to 231 -1)
int64 : 64-bit signed integer (-263 to 263 -1)
Unsigned Integers:
uint8 : 8-bit unsigned integer (0 to 255)

Slide 14:
Data types
PyTorch supports the same types as NumPy supports.
Floating Point:
float16: Half precision float (or half)
float32: Single precision float (or float) (default)
float64: Double precision float (or double)
Complex Numbers:
complex64: Complex number with two 32-bit floats (real and imaginary components)
complex128: Complex number with two 64-bit floats 
Boolean:
bool: Boolean type storing True and False values


Slide 15:
Data types
Most of these types are in fact C/C++ types (or the actual types that are accepted under the current architecture).

Because of this, numpy and pytorch can share some memory zones with continuous data (arrays).

However, PyTorch has to take into consideration that not all CPU types have a similar type on the GPU (with a similar representation).  

Slide 16:
Tensors

Slide 17:
Tensors
A tensor is a multi-dimensional array. It is very similar to a NumPy's array, but has some additional properties, the most important one being the ability to be used on GPUs (graphics processing units) to accelerate computing.

A tensor (if runs on a CPU) can share the same memory with a NumPy array (making working with numpy fairly easy).

Slide 18:
Tensors
A tensor can be created in the following ways:
From a python list: torch.tensor
With zeros: torch.zeros
With ones: torch.ones
With random values: torch.rand, torch.randn, torch.randint
With a specific value: torch.fill
Based on an interval: torch.arange
From a numpy array: torch.from_numpy
Or an empty one: torch.empty
The format is similar to the one from numpy (meaning you have to provide a shape, a type, etc). 

Slide 19:
Tensors
Let’s see some examples on how to build a tensor.
import torch
t1 = torch.tensor([1,2,3])
t2 = torch.ones((2,3))
t3 = torch.zeros(3,2)
t4 = torch.rand(3,3)
t5 = torch.arange(1,9)
print("t1=",t1)
print("t2=",t2)
print("t3=",t3)
print("t4=",t4)
print("t5=",t5)

Slide 20:
Tensors
Or, if we want to specify a data type:
import torch
t1 = torch.tensor([1,2,3],dtype = torch.int16)
t2 = torch.ones((2,3),dtype = torch.int8)
t3 = torch.zeros(3,2,dtype = torch.uint8)
t4 = torch.rand(3,3,dtype = torch.float64)
print("t1=",t1)
print("t2=",t2)
print("t3=",t3)
print("t4=",t4)

Slide 21:
Tensors
As previously said, a numpy array can also be used as a parameter to create a tensor.
import torch, numpy
a = numpy.array([[1,2,3],[4,5,6]])
print("a=",a)
t = torch.from_numpy(a)
print("t=",t)

Slide 22:
Tensors
What is important to understand in this case is that they share the same memory space (if run on CPU)
import torch, numpy
a = numpy.array([[1,2,3],[4,5,6]])
print("a=",a)
t = torch.from_numpy(a)
print("t=",t)
a[1,1] = 100
print(t)
Notice that if we modify variable a, the tensor  t  is changing as well

Slide 23:
Tensors
The indexing and slicing rules apply in a similar manner as with NumPy.
import torch
t = torch.tensor([[1,2,3],[4,5,6]])
print("t[0,0]    = ",t[0,0])
print("t[0]      = ",t[0])
print("t[-1,-1]  = ",t[-1,-1])
print("t[0,:1]   = ",t[0,:2])
print("t[-1,1:]  = ",t[-1,1:])
v = torch.tensor([1,2,3,4,5])
print("v      = ",v)
print("v[v>3] = ",v[v>3])

Slide 24:
Tensors
Similarly, for a tensor there are several information that can be provided (such as shape, element size, etc). Additionally, an extra property called .device can provide information on tensor location (CPU or GPU)
import torch
t = torch.tensor([[1,2,3],[4,5,6]])
print("Shape    = ",t.shape)
print("Type     = ",t.dtype)
print("Dim      = ",t.ndim)
print("Size     = ",t.element_size())
print("Device   = ",t.device)

Slide 25:
Tensors
A reshape method (.view) is also provided and can be used similarly as with NumPy to change the shape of an existing tensor.
import torch
t = torch.arange(1,9)
print("Vector = ",t)
t = t.view(4,2)
print("4x2 matrix = ",t)
t = t.view(2,2,2)
print("2x2x2 array = ",t)

Slide 26:
Tensors
In the previous example we have used a syntax as follows:

	new_tensor = tensor.view(new shape)

It is important to understand that this DOES NOT COPY the memory, it just returns another tensor that shares the same memory with the original one, but with a different shape.
As a result, a change in the values from the memory will affect both tensors (the original one and the resulted one).

Slide 27:
Tensors
Let’s see an example:






As it can be observed, after we change element with index 3 in the tensor t, the change is also observable in matrix m
import torch
t = torch.arange(1,9)
print("Vector = ",t)
m = t.view(4,2)
print("4x2 matrix = ",m)
t[3] = 1000
print("matrix = ",m)

print("t.addr = ",hex(t.data_ptr()))
print("m.addr = ",hex(m.data_ptr()))


Slide 28:
Tensors
It is important to understand that this operation only works when the array is contiguous. 

	new_tensor = tensor.view(new shape)

An array is contiguous if the rows are stored as contiguous blocks of memory. The next memory address stores the next row value.


Slide 29:
Tensors
Contiguous array:

	


t = torch.arange(1,13)
print("Vector = ",t)
print("is contiguous = ",t.is_contiguous())


m = t.view(4,3)
print("4x3 matrix = ",m)
print("is contiguous = ",m.is_contiguous())


Slide 30:
Tensors
Contiguous array:

	


mt = m.t() #transpose matrix
print("3x4 matrix = ",mt)
print("is contiguous = ",mt.is_contiguous())

t2 = mt.view((1,12)) #reshape into vector


Slide 31:
Tensors
Contiguous array:

	


t2 = mt.reshape((1,12)) #reshape into vector
print("Vector = ",t2)
print("is contiguous = ",t2.is_contiguous())

print("t.addr = ",hex(t.data_ptr()))
print("mt.addr = ",hex(mt.data_ptr()))
print("t2.addr = ",hex(t2.data_ptr()))

Reshape works in a similar fashion to view, but in case the array is not contiguous it will perform a copy. If the array is contiguous, it will work as view. 


Slide 32:
Tensors
Scalar and element-wise operation work in a similar manner
import torch
t = torch.tensor(([1,2,3],[4,5,6]))
t = t * 2
print("t = ",t)
t = t + 10
print("t = ",t)
v = torch.ones(2,3, dtype=torch.int)
print("v = ",v)
t += v
print("t = ",t)

Slide 33:
Tensors
There are also several simple statistical functions that can be used (like sum, median, mean, etc). Its important to notice that the result of these function is a tensor (and not necessarily a scalar value) – even if the tensor is a vector with one element (containing just one scalar).To get the scalar value, use .item()
import torch
t = torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32)
print("sum(t)    = ",torch.sum(t))
print("mean(t)   = ",torch.mean(t))
print("median(t) = ",torch.median(t))
print("min(t)    = ",torch.min(t))
print("max(t)    = ",torch.max(t))
print("max(t)    = ",torch.max(t).item())


Slide 34:
Tensors
There is also a method called .dot that performs the dot product between two tensors. 




In this case the dot product is: 1x10 + 2x20 + 3x30 = 10+40+90 = 140
import torch
v1 = torch.tensor([1,2,3])
v2 = torch.tensor([10,20,30])
result = v1.dot(v2)
print("v1 . v2 = ", result)

Slide 35:
Tensors
However, while the method .dot can be used in NumPy to multiply matrixes as well:



   in PyTorch it only works with vectors:
import numpy
v1 = numpy.array([1,2,3])
v2 = numpy.array([[10,20],[30,40],[5,6]])
result = v1.dot(v2)
print("v1 . v2 = ", result)
import torch
v1 = torch.tensor([1,2,3])
v2 = torch.tensor([[10,20],[30,40],[5,6]])
result = v1.dot(v2)
print("v1 . v2 = ", result)

Slide 36:
Tensors
We can multiply two matrixes using torch.mm(M1,M2) method (mm stands for matrix multiplication). Its important to highlight that both parameters M1 and M2 must be matrixes – bi-dimensional arrays). 
import torch, numpy
m1 = torch.tensor([[1,2,3]])
m2 = torch.tensor([[10,20],[30,40],[5,6]])
result = torch.mm(m1,m2)
print("m1 x m2 = ", result)
There is also a similar operation, for 3d tensors, called bmm. (batch matrix multiplicaton)

Slide 37:
Tensors
If M1 and M2 have more than 2 dimensions, we need to use torch.bmm(M1,M2) method (bmm stands for batch matrix multiplication). 
import torch
#create a 4d tensor of shape (2,2,2,3)m1 = torch.arange(1,2*2*2*3+1).view((2,2,2,3)) #create a 4d tensor of shape (2,2,3,4)m2 = torch.arange(1,2*2*3*4+1).view((2,2,3,4)) print("m1 x m2 = ", torch.matmul(m1,m2))

Slide 38:
Tensors
When performing operations that use multiple tensors, the tensors must be on the same device




import torch
tensor = torch.tensor([1, 2, 3, 4])
x = torch.tensor([11,12,13,14])

if torch.cuda.is_available():
   device = torch.device("cuda:0") 
   tensor_gpu = tensor.to(device)
   print(x+tensor_gpu)

Slide 39:
Tensors
When performing operations that use multiple tensors, the tensors must be on the same device




import torch
tensor = torch.tensor([1, 2, 3, 4])
x = torch.tensor([11,12,13,14])

if torch.cuda.is_available():
   device = torch.device("cuda:0") 
   tensor_gpu = tensor.to(device)
   print(x.to(device)+tensor_gpu)

Slide 40:
Tensors
The device of a tensor can be obtained by using .device attribute




def add_random(t):
    r= torch.rand(t.shape)
    return (r.to(t.device)+t)
 
 
tensor = torch.tensor([1, 2, 3, 4])
x = torch.tensor([11,12,13,14])

if torch.cuda.is_available():
    device = torch.device("cuda:0") 
    tensor_gpu = tensor.to(device)
 
    print("tensor_gpu+random=",add_random(tensor_gpu))
 
print("tensor_cpu+random=",add_random(x))
 

Slide 41:
Tensors
Torch.squeeze is used to remove dimensions of size 1 from a tensor.




import torch
x = torch.tensor([[1], [2], [3], [4]])
print(x.shape)
print("After squeeze")
x = torch.squeeze(x,1)
print(x.shape)
print(x)

Slide 42:
Tensors
Similar, you can add another dimension to the tensor using unsqueeze.





import torch
x = torch.tensor([1,2,3,4])
y = torch.tensor([11,12,13,14])z = torch.concat((x,y))
print(z)
x1 = torch.unsqueeze(x,0)
y1 = torch.unsqueeze(y,0)
z = torch.concat((x1,y1))
print(x1, y1)
print(z)

Slide 43:
Layers

Slide 44:
Layers
In a previous course we have defied a simple neuronal network as follows:

Slide 45:
Layers
In practice we can describe the same architecture in more details:
Input Layer
Hidden Layers
Output Layer

Slide 46:
Layers
In practice we can describe the same architecture in more details:
Input Layer
Hidden Layers
Output Layer
If we are to generalize, each one of these bocks is just a function that receives an input (e.g. a vector of scalars) and returns an output that will further be feed into the next function.

Slide 47:
Layers

Slide 48:
Layers
Let’s see some examples of most common activation functions:
softmax (torch.nn.Softmax)
ReLU (torch.nn.RelU)
Sigmoid (torch.nn.Sigmoid)
Tanh (torch.nn.Tanh)
Threshold (torch.nn.Threshold)



Slide 49:
Layers
Let’s see some examples on how activation functions can be used.
import torch
tensor = torch.tensor([-1, 0, 1, 2],dtype=float)
softmax = torch.nn.Softmax(dim=0)
relu = torch.nn.ReLU()
sigmoid = torch.nn.Sigmoid()
tanh = torch.nn.Tanh()
print("Softmax = ",softmax(tensor))
print("ReLU    = ",relu(tensor))
print("Sigmoid = ",sigmoid(tensor))
print("tanh    = ",tanh(tensor))

Slide 50:
Layers
Let’s see some examples on how activation functions can be used.
import torch
tensor = torch.tensor([-1, 0, 1, 2],dtype=float)
softmax = torch.nn.Softmax(dim=0)
relu = torch.nn.ReLU()
sigmoid = torch.nn.Sigmoid()
tanh = torch.nn.Tanh()
print("Softmax = ",softmax(tensor))
print("ReLU    = ",relu(tensor))
print("Sigmoid = ",sigmoid(tensor))
print("tanh    = ",tanh(tensor))
For example, in this case, the ReLU function is called on every element in the vector. ReLU(x) = max(0,x) so, ReLU([-1,0,1,2])  [0,0,1,2]

Slide 51:
Layers
torch.nn.Linear is the component for matrix multiplication, that is organized as follows:


Slide 52:
Layers

Slide 53:
Layers
Let’s see an example:




The actual operation behind this code is as follows
import torch

tensor = torch.tensor([1.0,2.0,3.0])
linear = torch.nn.Linear(3,2,False)
result = linear(tensor)
print("Linear  =",linear)
print("Output  =",result)
print("weights =",linear.weight)

Slide 54:
Layers
In practice, the input tensor does not have to be a vector (it can be an array where every row is a sample. The output will be an array as well with the same number of rows as the input, each row being a vector with the outputted values that correspond to a specific input.



import torch
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [7.0,8.0,9.0],
                      [1.0,3.0,5.0],
                      [2.0,4.0,6.0]])
linear = torch.nn.Linear(3,2,False)
output = linear(input)
print("Linear  =",linear)
print("Output  =",output)
print("weights =",linear.weight)

Slide 55:
Layers
In practice, the input tensor does not have to be a vector (it can be an array where every row is a sample. The output will be an array as well with the same number of rows as the input, each row being a vector with the outputted values that correspond to a specific input.



import torch
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [7.0,8.0,9.0],
                      [1.0,3.0,5.0],
                      [2.0,4.0,6.0]])
linear = torch.nn.Linear(3,2,False)
output = linear(input)
print("Linear  =",linear)
print("Output  =",output)
print("weights =",linear.weight)
In this case, the output for the sample [7,8,9] from the input matrix (the 3rd row) is the 3rd raw from the output matrix [3.7645, 3.7098]

Slide 56:
Layers
You can also use torch.nn.Parameter(…) to set up your own weights (for example if you have some values that are already precomputed).







In this example we set up all weights to 0 (as such the output will be a zeroed matrix).


import torch
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [7.0,8.0,9.0],
                      [1.0,3.0,5.0],
                      [2.0,4.0,6.0]])
linear = torch.nn.Linear(3,2,False)
linear.weight = torch.nn.Parameter(torch.zeros(2,3,dtype=torch.float))
output = linear(input)
print("Linear  =",linear)
print("Output  =",output)
print("weights =",linear.weight)

Slide 57:
Layers
Another important type of Layers are containers. A container is essentially a group of other layers that are connected (meaning that the output of one layer is the input of another layer).
The most commonly used containers are:
Sequential 
ModuleList
ModuleDict




Slide 58:
Layers
The torch.nn.Sequantial module is initialized with a list of layers (or an OrderedDict) and it chains them (meaning that when it has to compute an input, it passes the input to the first from the list, then the output of that layer is passed to the next one and so on until it reaches the final layer). The output of the final layer will be the output of the Sequantial layer.




Slide 59:
Layers
Let’s see an example that uses: torch.nn.Sequantial
import torch
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [2.0,4.0,6.0]])
net = torch.nn.Sequential(
    torch.nn.Linear(3,2,False),
    torch.nn.ReLU(),
    torch.nn.Linear(2,1,False),
    torch.nn.Sigmoid()
)
output = net(input)
print("net     =",net)
print("Output  =",output)

Slide 60:
Layers
In reality, the Sequential layout is in fact a small neuronal network:
net = torch.nn.Sequential
(
    torch.nn.Linear(3,2,False),
    torch.nn.ReLU(),
    torch.nn.Linear(2,1,False),
    torch.nn.Sigmoid()
)
Weight Matrix (2x3)
ReLU
Weight Matrix(1x2)
Sigmoid

Slide 61:
Layers
The output shape of one layer must match the input shape of the next layer
net = torch.nn.Sequential
(
    torch.nn.Linear(3,2,False),
    torch.nn.ReLU(),
    torch.nn.Linear(3,1,False),
    torch.nn.Sigmoid()
)

Slide 62:
Layers
Layers can also be added to a list and the content of the list to be passed to the Sequential layer

layers =[
 torch.nn.Linear(3,2,False),
 torch.nn.ReLU(),
 torch.nn.Linear(2,1,False),
 torch.nn.Sigmoid() 
]

net = torch.nn.Sequential(*layers)


Slide 63:
Module base class

Slide 64:
Module base class
A Module (torch.nn.Module) is the base class that can be used to implement a neuronal network.
Usually, you have to do the following:
Derive your class from torch.nn.Module
Add different layers and activation functions within your class (you can add them directly as parameters, or within a Sequence layer, …)
Implement a constructor __init__ method
Implement a function forward that receives an input and returns the output




Slide 65:
Module base class
Let’s see an example



import torch
class MyNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(3,1)
        self.activation = torch.nn.ReLU()
    def forward(self, input):
        x = self.layer(input)
        return self.activation(x)
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [2.0,4.0,6.0]])
net = MyNN()
output = net(input)
print("net     =",net)
print("Output  =",output)

Slide 66:
Module base class
Let’s see an example



import torch
class MyNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(3,1)
        self.activation = torch.nn.ReLU()
    def forward(self, input):
        x = self.layer(input)
        return self.activation(x)
input = torch.tensor([[1.0,2.0,3.0],
                      [4.0,5.0,6.0],
                      [2.0,4.0,6.0]])
net = MyNN()
output = net(input)
print("net     =",net)
print("Output  =",output)

Slide 67:
Datasets & Dataloaders

Slide 68:
Datasets
Every machine learning algorithm essentially relies on a dataset (for either training or testing).
However, data can be presented in different forms and as such we will need a common interface that can be used to access different types of data.




Slide 69:
Datasets
PyTorch has an interface/class named: torch.utils.data.Dataset that should be used to create a dataset.
The interface implies 3 methods:
An __init__ function (that loads the dataset)
A __get_item__() method to access one element from the dataset
A __len__() method to provide the number of elements from the dataset





Slide 70:
Datasets
PyTorch has an interface/class named: torch.utils.data.Dataset that should be used to create a dataset.





class <Name>(torch.utils.data.Dataset):
    def __init__(self, ...):
        # instantiate the data set based on parameters
    
    def __len__(self):
        return 0 # returns the number of records from the dataset
    
    def __getitem__(self, idx):
        # do some processing if needed
        # return the sample with index 'idx' and its label
        return (sample, label)


Slide 71:
Datasets
Let’s see an example:





import torch
class First100Numbers(torch.utils.data.Dataset):
    def __init__(self):
        self.list = [(i,i%2) for i in range(1,101)]
        
    
    def __len__(self):
        return len(self.list)
    
    def __getitem__(self, idx):
        return (self.list[idx][0],self.list[idx][1])
d = First100Numbers()
print(d[2])
print(d[5]);
print(len(d))

Slide 72:
Datasets
This method allows one to use 3rd party libraries that can read different types of other datasets, such as:
A database (e.g., an SQL database)
A CSV/TSV file (e.g., with NumPy)
An XML file
A JSON file
A stream …
It also allows one to make some transformations on the data before it gets sent to the neuronal network (e.g., convert some strings into number, picture into pixels, etc)





Slide 73:
Datasets
Pytorch also has a large set of predefined datasets that can be used to out of the box (torchvision.datasets.*):
MNIST: Handwritten digit dataset with 60,000 training samples and 10,000 test samples in 10 classes (digits 0-9).
Fashion-MNIST: A dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category.
CIFAR10/1000: A dataset consisting of 60,000 32x32 color images in 10/100 classes, with 6,000 images per class.
ImageNet: A large dataset designed for use in visual object recognition software research, with more than 14 million images and thousands of classes.
COCO (Common Objects in Context): A large-scale object detection, segmentation, and captioning dataset.
VOC (PASCAL Visual Object Classes): A dataset for object detection, image classification, object segmentation, and person layout.
Kinetics: A large-scale, high-quality dataset of YouTube video URLs which include a diverse range of human-focused actions
… and many other …

Slide 74:
Datasets
The following example downloads the MNIST dataset and stores it locally.
import torch
import torchvision
import torchvision.transforms as transforms
image_to_tensor = transforms.Compose([transforms.ToTensor()])
trainset = torchvision.datasets.MNIST(
      root='./training’, 
      download=True, 
      train=True, 
      transform= image_to_tensor )
testset = torchvision.datasets.MNIST(
      root='./tests’, 
      download=True, 
      train=False, 
      transform= image_to_tensor )

Slide 75:
Datasets
The following example downloads the MNIST dataset and stores it locally.
import torch
import torchvision
import torchvision.transforms as transforms
image_to_tensor = transforms.Compose([transforms.ToTensor()])
trainset = torchvision.datasets.MNIST(
      root='./training’, 
      download=True, 
      train=True, 
      transform= image_to_tensor )
testset = torchvision.datasets.MNIST(
      root='./tests’, 
      download=True, 
      train=False, 
      transform= image_to_tensor )
This is required as images have to be transformed into a tensor. In this case image is a sequence of number, but torchvision library provides several mechanisms to convert even images to tensors., via transforms (including converting to gray scale or other image processing functionalities).

Slide 76:
Datasets
The following example downloads the MNIST dataset and stores it locally.
import torch
import torchvision
import torchvision.transforms as transforms
image_to_tensor = transforms.Compose([transforms.ToTensor()])
trainset = torchvision.datasets.MNIST(
      root='./training’, 
      download=True, 
      train=True, 
      transform= image_to_tensor )
testset = torchvision.datasets.MNIST(
      root='./tests’, 
      download=True, 
      train=False, 
      transform= image_to_tensor )
The training dataset is going to be downloaded into the folder ./training after it is converted using the provided transformer.

Slide 77:
Datasets
To load the dataset that was downloaded locally, you can use the following code: 




Notice that trainset is in fact a Dataset object.
import torchvision
import torch
trainset = torchvision.datasets.MNIST(root='./training', download=False, train=True)
print(trainset)
print("Type = ",type(trainset))
print("Is Dataset = ",issubclass(type(trainset), torch.utils.data.Dataset))

Slide 78:
Dataloaders
Having access to a dataset is not enough. In many cases, the training implies building batches, shuffling the data, and other data sampling procedures.

These procedures work independently from the actual data set (they are agnostic to the content).

As such, PyTorch provide another class: torch.utils.data.DataLoader that can facilitate this procedures.

Slide 79:
Dataloaders`
Let’s see an example that uses Dataloaders:
import torch
from torch.utils.data import Dataset, DataLoader
class SimpleDataset(Dataset):
    def __init__(self):
        self.samples = torch.randn(9, 4)
        self.label = torch.randn(9)
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        return self.samples[idx],self.label[idx]
dataset = SimpleDataset()
dataloader = DataLoader(dataset, batch_size=3, shuffle=True)
for inputs,labels in dataloader:
    print("input = ",inputs)
    print("label = ",labels)

Slide 80:
Checkpoints

Slide 81:
Checkpoints
Checkpoints are a form of model persistence that saves the state of your training process at certain intervals so that you can resume or analyze the training from that point. 

In particular for debugging, checkpoints are essential.
Checkpoints are useful especially in scenarios when training takes a long time. If the training stops for different reason, it can be resumed. 

Slide 82:
Checkpoints
A checkpoint typically includes the following information:
Model State Dictionary: The model's parameters or weights.
Optimizer State Dictionary: The state of the optimizer, including the current learning rate, momentum, etc.
Epoch 
Loss: The loss value at the checkpoint. This is useful for monitoring progress over time.
Any other relevant information

Slide 83:
Checkpoints
Let’s see an example:






To load a checkpoint, use torch.load(…). It is important to store in a checkpoint all information needed to resume the training from that point.
import torch
checkpoint = {
    'epoch': epoch,
    'model_state': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'loss': loss,
    # ... any other relevant data ...
}
torch.save(checkpoint, 'my_checkpoint.pth')

Slide 84:
Loss Functions

Slide 85:
Loss Functions
Loss functions in PyTorch contain the implementation of one part from the backpropagation mechanism: computing the gradients.
The following loss functions are implemented:
Mean Square Error (torch.nn.MSELoss)
Cross Entropy (torch.nn.CrossEntropyLoss)
Negative log likelihood loss (torch.nn.NLLLoss)
Binary cross entropy loss (torch.nn.BCELoss)
Mean absolute error loss (torch.nn.L1Loss)
Currently PyTorch has more than 25 predefined loss function.


Slide 86:
Loss Functions
The regular usage of a loss function is as follows:
Create a loss function (based on the existing one or as something derived from the Module)
Call the loss function with two tensors (one that reflects the prediction, and another one that reflects the expected target). The output of this operation will be another tensor that contains the computed loss between those two tensors
Call the method .backward() on the tensor obtained in step 2. This will compute the gradients for each trainable weight. It is important for all trainable parameters to have requires_grad set to True for weights and biases (applicable only to leafs in the computation graph

Slide 87:
Loss Functions
Let’s see an example (with cross entropy function).
import torch
prediction = torch.tensor([1., 2., 3.], requires_grad=True)
expected = torch.tensor([4., 5., 6.])

loss_function = torch.nn.CrossEntropyLoss()
loss = loss_function(prediction, expected)
loss.backward()

print("loss = ",loss)
print("Gradients = ",prediction.grad)

Slide 88:
Computation Graph
PyTorch builds a list of all layers that need to be updated when the feed forward step is being build.

This list is also called a computational graph and allows other components (such as optimizers) to access the entire network.

Slide 89:
Computation Graph
The computation graph is being build dynamically as operation are performed / computed. Internally, the output tensor from the feed forward step has a link (reference) to the dynamic computation graph.
When the feed forward step end, the computational graph is linked to the output tensor.

Slide 90:
Backpropagation in Pytorch

Slide 91:
Backpropagation in Pytorch
Let’s review the backpropagation process:
Compute the output of the current model given an input (an input can be one item or multiple items)
Compute the loss using the output obtained on the previous step and the expected value (if the input is represented by multiple items, the computed loss is the average of the loss from all items)
Compute the delta gradients for each layer
Update weights using the computed delta gradients from the previous layer.


Slide 92:
Backpropagation in Pytorch
Let’s see how this flow looks like in PyTorch:

 all_weights = model.parameters()
 optimizer = optim.<Name>(all_weights, …)


 output = model(input)

 loss = loss_function(output, expected)

 optimizer.zero_grad()

 loss.backward()

 optimizer.step()
Setup Phase
First, we need to create an optimizer
Example of optimizer: gradient descent, adam, rmsprop, adagarad
Regardless of the chosen optimizer, a reference to all weights (from all layers in the model) has to be provided). 
Any object derived from Module has a method called .parameters() that provides a list of all weights (tensors) that support gradients (requires_grad=True) and as a result, can be adjusted.

Slide 93:
Backpropagation in Pytorch
Let’s see how this flow looks like in PyTorch:

 all_weights = model.parameters()
 optimizer = optim.<Name>(all_weights, …)


 output = model(input)

 loss = loss_function(output, expected)

 optimizer.zero_grad()

 loss.backward()

 optimizer.step()
1. Compute the output
Compute the output of the model (given an input). The output variable is a tensor, that has the entire computation graph linked. 

Slide 94:
Backpropagation in Pytorch
Let’s see how this flow looks like in PyTorch:

 all_weights = model.parameters()
 optimizer = optim.<Name>(all_weights, …)


 output = model(input)

 loss = loss_function(output, expected)

 optimizer.zero_grad()
 loss.backward()

 optimizer.step()

2. Compute the loss
Compute the loss using the output from the previous step and the expected value for the input that was used to obtain the output.

Slide 95:
Backpropagation in Pytorch
Let’s see how this flow looks like in PyTorch:

 all_weights = model.parameters()
 optimizer = optim.<Name>(all_weights, …)


 output = model(input)

 loss = loss_function(output, expected)

 optimizer.zero_grad()
 loss.backward()

 optimizer.step()

3. Compute the delta gradients
Since these steps are repeated, we need first to clear all previously computed gradients (the parameter .grad) from each tensor that resides in computational graph (that is linked by the output tensor).
After we do this, we can compute all delta gradients and we can store them into the .grad parameter from each tensor. This is done via the loss.backward() method.

Slide 96:
Backpropagation in Pytorch
Let’s see how this flow looks like in PyTorch:

 all_weights = model.parameters()
 optimizer = optim.<Name>(all_weights, …)


 output = model(input)

 loss = loss_function(output, expected)

 optimizer.zero_grad()
 loss.backward()

 optimizer.step()

4. Update weights
In this step, the optimizer (that has access to all weights)  has to update the weights based on the delta computed in the previous steps.

Slide 97:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

import torch
import torchvision.datasets as datasets # for Mist
import torchvision.transforms as transforms # Transformations we can perform on our dataset for augmentation
from torch import optim # For optimizers like SGD, Adam, etc.
from torch import nn # To inherit our neural network
from torch.utils.data import DataLoader # For management of the dataset (batches)
from tqdm import tqdm # For nice progress bar!

Slide 98:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

 
class NN(nn.Module):
    def __init__(self, input_size, num_classes):
 
        super(NN, self).__init__()
 
        self.fc1 = nn.Linear(input_size, 50)
        self.fc2 = nn.Linear(50, num_classes)
        self.relu = nn.ReLU()
 
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x
 
 
Create a neural network by inheriting from nn.module.
2 arguments are given: *input size (which will be 784)
*Num classes (which will be 10)
In the init we first call the parent constructor.We then initialize the layers:
A fully connected layer from 784 to 50 (hidden)
A fully connected layer form 50 to num_classes (10)
The activation function
In the forward method we just call them in order and return the output

Slide 99:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

 
# Set device cuda for GPU if it's available otherwise run on the CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Hyperparameters
input_size = 784
num_classes = 10
learning_rate = 0.001
batch_size = 64
num_epochs = 10
 
 
Set the device and set the hyper parameters for training the network.


Slide 100:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

 
# Load Data
train_dataset = datasets.MNIST(
root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(
root="dataset/", train=False, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)
 
 
Download the mnist training and testing dataset. 
Transforms.ToTensor just converts the input to tensor.Initialize two DataLoaders, one for training and one for testing, using a batch_size of 64

Slide 101:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

# Initialize network
model = NN(input_size=input_size, num_classes=num_classes).to(device)
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
Initialize the network and transfer the model to the detected device(i.e. gpu)

Initialize a cross function: CrossEntropyIntialize an optimizer with the model parameters : Adam. 

Slide 102:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

for epoch in range(num_epochs):
    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):
        # Get data to cuda if possible
        data = data.to(device=device)
        targets = targets.to(device=device)
 
        # Get to correct shape
        data = data.reshape(data.shape[0], -1)
        . . .
Enumerate through iterations and batches.

Enumerate(tqdm(train_loader)) is for displaying the progressbar and the batch_indexex:78%|█████████████████████████████████████▎          | 730/938 [00:04<00:01, 179.64it/s

Move the data and the labels to gpu.
Reshape the input data to the format expected by the network (batch_size, 784)

Slide 103:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

for epoch in range(num_epochs):
    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):
        . . .  
        # Forward
        scores = model(data)
        loss = criterion(scores, targets)
 
        # Backward
        optimizer.zero_grad()
        loss.backward()
 
        # Gradient descent or adam step
        optimizer.step()

1. Perform the forward pass
2. Compute the loss function given the input and the target
3. Reset the value of the gradients
4. Backproapagate the loss
5. Apply the gradients through the optimizer

Slide 104:
Backpropagation in Pytorch
Measuring results

def check_accuracy(loader, model):
 
    num_correct = 0
    num_samples = 0
    model.eval()
Create a function to test accuracy:
The first argument is a DataLoader. This way we can test the accuracy both on training and testing data set
The second argument is the model which we want to test.

By calling model.eval we tell the model that it will be in evaluation mode. For this case it doesn’t change anything, but there are layers that behave differently on evaluation than on training. 

Slide 105:
Backpropagation in Pytorch
Measuring results

 with torch.no_grad():
        # Loop through the data
        for x, y in loader:

            # Move data to device
            x = x.to(device=device)
            y = y.to(device=device)
 
            # Get to correct shape
            x = x.reshape(x.shape[0], -1)
            . . .
 
With torch.no_grad() is just an optimization. It tells to pytorch that no grads are needed so less memory is needed. Thus a bigger batch size could be used. It doesn’t impact the results

Move the data to the device and reshape it in the way expected by the model (batch_size, 784)

Slide 106:
Backpropagation in Pytorch
Measuring results

 with torch.no_grad():
        # Loop through the data
        for x, y in loader:
            . . .
        # Forward pass
        scores = model(x)
        _, predictions = scores.max(1)
 
        # Check how many we got correct
        num_correct += (predictions == y).sum()
 
        # Keep track of number of samples
        num_samples += predictions.size(0)

Perform the forward pass to get the output of the network . Get the max value (not used) and maxi index of the output. If the index is equal to the target, add to correctly identified samples

predictions.size(0) is the size of the minibatch

Slide 107:
Backpropagation in Pytorch
Measuring results

def check_accuracy(loader, model):
 
    num_correct = 0
    num_samples = 0
    model.eval()

     with torch.no_grad():
        # Loop through the data
        for x, y in loader:
           . . .
        model.train()
    return num_correct / num_samples

Set the model back to training mode. Compute and return the accuracy

Slide 108:
Backpropagation in Pytorch
Let’s put all of these together and created a simple training:

100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 167.78it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 176.65it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 176.36it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 173.92it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 175.19it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 175.22it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 174.93it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 175.81it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 175.96it/s]
100%|██████████████████████████████████████████████| 938/938 [00:05<00:00, 174.38it/s]
Accuracy on training set: 98.56
Accuracy on test set: 97.10


Slide 109:
Questions & Discussion



================================================================================
COURSE 8: Q LEARNING
================================================================================

Slide 1:
Course 8: Q-Learning
Neural Networks

Slide 2:
What is reinforcement learning
Basic Reinforcement Learning
Q - Learning
Reinforcement Learning using Neural Networks
Questions

Overview

Slide 3:
What is reinforcement learning?

Slide 4:
What is reinforcement learning?
Reinforcement learning is learning what to do--how to map situations to actions--so as to maximize a numerical reward signal
The learner is not told which actions to take
He receives rewards or penalties for each actions he takes
Through trial and error he must find a strategy that maximizes the reward
It is not specific to neural nets, but neural nets can be used in this kind of learning

Differences to supervised learning:
No labeled data
Feedback may be delayed
The agent affects the environment





Slide 5:
What is reinforcement learning?
Examples of reinforcement learning:

Make a robot walk

Play Atari Games

Beat world champion at Backgammon game

Invest in the stock market

Make an helicopter perform stunts 






Slide 6:
The agent does the following steps:
Observes the environment
Takes an action
Receive reward
Learn
Repeat 

Basic Reinforcement Learning
World
Observe state
Take action
Receive reward
The purpose is to find a policy that maximizes the total reward over the lifetime of the agent

Slide 7:
Markov Decision Process: Making a single decision:

Agent is in state 0
	Action A, takes him to State 1, reward is 3
	Action B, takes him to State 2, reward is 2
	
If Goal is to maximize reward, than the answer is simple:
		Take action with the highest reward (A)





Basic Reinforcement Learning
0
1
2
A
B
3
2

Slide 8:
Markov Decision Process: Making multiple decisions:
This can be generalized. Every action affects subsequent actions.






Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10

Slide 9:
Basic Reinforcement Learning

Slide 10:
Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10

Slide 11:
Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10

Slide 12:
Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10

Slide 13:
Basic Reinforcement Learning

Slide 14:
Example of computing the optimal policy using the Q-function







Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10

Slide 15:
Basic Reinforcement Learning

Slide 16:
More complex MDPs:

In this case, the number of steps is unlimited
The value of states 1 or 2 can be infinite

All policies with non-zero reward cycle have 
infinite value


	







Basic Reinforcement Learning
0
1
2
A
B
-1000
1000
3
0
0
B
B
A
1
1
A

Slide 17:
Basic Reinforcement Learning
0
1
2
A
B
-1000
1000
3
0
0
B
B
A
1
1
A

Slide 18:
Basic Reinforcement Learning
0
1
2
A
B
-1000
1000
3
0
0
B
B
A
1
1
A

Slide 19:
If the MDP model is complete, we can compute the optimal value function directly, through dynamic programming (recursive)







Basic Reinforcement Learning
0
1
2
A
B
1
2
3
4
5
A
A
A
A
B
1
1
-1000
1
10
0

Slide 20:
What happens when we don’t have a complete MDP:
We know the states and actions
We don’t have the system model (no transition function or reward function)

We are allowed to sample from the MDP.

We must perform actions in order to generate experiences (s,a,r,s’). Based on a (large) number of experiences we can approximate the transition function and the reward function

This is called reinforcement learning








Basic Reinforcement Learning

Slide 21:
Monte Carlo Method:

The Monte Carlo method is a computational technique that uses random sampling to solve mathematical or physical problems that might be difficult or impossible to solve exactly.

In Monte Carlo Reinforcement Learning (MC RL), the algorithm:
Runs complete episodes (like repeated “plays” of a game),
Observes returns,
Uses the average of many random trajectories to estimate value functions.











Basic Reinforcement Learning

Slide 22:
Basic Reinforcement Learning

Slide 23:
Basic Reinforcement Learning

Slide 24:
Basic Reinforcement Learning

Slide 25:
Example: Black Jack Game
	Rules of blackjack:
There is a dealer and a player
Each player is dealt 2 cards. Face-up. The dealer is dealt two cards, one face-up, one face-down
The goal is to get the sum of your cards, as close to 21 as possible but don’t go over 21
Cards value(2-9 their number, J, Q, K -10 points)
An ace can be considered a 1 or a 10
You have two actions possible: stay or hit
The dealer always follows this policy: hit until cards sum 17 or more then stay
The player closer to 21 wins. The one over 21, loses

	










Basic Reinforcement Learning

Slide 26:
	What is the state:
Sum of the player’s cards
If the players has an usable ace
Sum of the dealer’s cards
If the dealer has an usable ace

What are the actions:
Deal
Stay

What are the rewards:
Win: 1
Lose: -1
Draw: 0
	










Basic Reinforcement Learning

Slide 27:
Example: Black Jack Game
	
	We will learn the best strategy (policy), by running the blackjack many times and averaging the results for each state/action combination

	The states/actions (Q(s,a)) will be stored in a table (python dictionary)
	
		










Basic Reinforcement Learning

Slide 28:
Example: Black Jack Game
	
	Let’s say that the game is in the following state:
		The sum of the player’s card is 18
		The card that the dealer shows is 9
	
	This is a difficult position, since:
		There is a high probability that the other card of the 			dealer is 10 points
		
		But if the player chooses to hit, it is a high probability that he 		will go over 21
	
		










Basic Reinforcement Learning

Slide 29:
Observe how the rewards stabilize over time

So, the best actions to take is to stay, since it is less likely to lose (-0.2) than if he would hit (-0.5)
		










Basic Reinforcement Learning

Slide 30:
In the same way, all Q-values can be computed for every possible combination and a proper decision can be taken



		










Basic Reinforcement Learning

Slide 31:
Disadvantages for Monte Carlo:
The algorithm needs to reach the final reward in order to update all the rewards)

If the rewards have high variance, the convergence is slow

The number of states and actions must be small so they can fit in a table

The algorithm needs to evaluate a long trajectory just to update the starting state-action pair


	










Basic Reinforcement Learning

Slide 32:
Q-Learning

Slide 33:
Q-Learning

Slide 34:
Q-Learning

Slide 35:
Q-Learning

Slide 36:
Q-Learning

Slide 37:
Reinforcement Learning using Neural Networks

Slide 38:
Reinforcement Learning using Neural Networks

Slide 39:
This is where Neural Networks come in.

Neural Networks are good at approximating values. 
So, we’ll use neural networks to approximate Q(s,a).

So, like in the case of Atari games, there won’t be much of a problem if a state is approximated (i.e. states that are different only by a bunch of pixels could have the same value)

But how do we train the network? What is our target value?

Reinforcement Learning using Neural Networks

Slide 40:
We will use a neural network that will simulate the Q(s,a) function. 

The network will receive as input:
s: state
a: action
Will output the Q(s,a) function.Since we want to take the path where Q(s,a) has the greatest value, we need to compute Q(s,a) for all the actions(a) available in the state s.












Reinforcement Learning using Neural Networks
state
action
Network – hidden layers
Q-value

Slide 41:
We can improve the architecture of our neural network, by modifying the output shape
Instead of doing a forward pass for each state/action combination, we modify our network to have an output for each action.  
This way, we only need a forward pass for each action













Reinforcement Learning using Neural Networks
state
action
Network – hidden layers
Q-value
state
Network – hidden layers

Slide 42:
Reinforcement Learning using Neural Networks

Slide 43:
Reinforcement Learning using Neural Networks
state
Network – hidden layers

Slide 44:
Algorithm:
At each state we will :
Find the action that has the greatest Q(s,a)

	a. Feed as input for the neural network the current state
	b. Find the action (index) that has the greatest Q(s,a)
	

	












Reinforcement Learning using Neural Networks

Slide 45:
Algorithm:
At each state we will :
2. Take action that has the greatest Q(s,a) :
	a. Get the immediate reward R
	b. Move to the state that the action takes you (s’)	


	












Reinforcement Learning using Neural Networks
Receive immediate reward R

Slide 46:
Algorithm:
At each state we will :
3. Find the greatest value for the Q(s’,a):
	a. Feed as input to the neural network the next state s’
	b. Find maximum value from all the outputs of the network (actions)


	












Reinforcement Learning using Neural Networks

Slide 47:
Reinforcement Learning using Neural Networks
Actions that were not taken. Target values are the ones obtained in step1
Action taken: target is computed based on new data (reward)

Slide 48:
Example:

You are a car and need to reach a destination (ex. the gas station)
You can move into 4 directions (← ↑ → ↓)
The car always moves (so there is no break)
There are also obstacles that the car must avoid
The car only knows what is in front and on its side (consider it is night and it can only see a limited distance that is highlighted by the headlights)


Q-Learning

Slide 49:
State:
Since everything the car sees is through the headlights we can consider that the state of the car is given by the headlights.
Similar to the image on the right, we will consider the headlights to be a matrix of (20,10), centered on the car. 
Each point on the matrix can be:
0 : it doesn’t intersect anything
1: it intersects the gas station
-1: it intersect an object 

Q-Learning

Slide 50:
State:
For the image on the right, the state will be:


Q-Learning

Slide 51:
State:
To simplify things, we can just make a sum on the columns and consider that the current state


Q-Learning

Slide 52:
Example:

Based on this example, the neural network receives as input a vector of 20 elements with values between -10 and 10 and should output 3 value, each one being the Q(s,a) for each action













Reinforcement Learning using Neural Networks
Network – hidden layers

Slide 53:
Example:

Let’s say that our current neural network, based on the current state has outputted the following values for each action













Reinforcement Learning using Neural Networks
Network – hidden layers
330
40
-430

Slide 54:
Example:

We choose the action for which the network has given us the largest output: Left.So , the car should rotate towards left. 













Reinforcement Learning using Neural Networks
Network – hidden layers
330
40
-430

Slide 55:
Example:

We end up in another state, since the car has moved left.
And we also get a reward for taking the action left. The reward is dependant on the environment, but in this case could be a reward that takes into account that we almost collided with an object . So, let’s say the reward is R = -8













Reinforcement Learning using Neural Networks
R=-8

Slide 56:
Example:

We end up in another state, since the car has moved left. Here we feed the new state (s’) to the neural network and find the greatest Q(s’a) which is 350. 













Reinforcement Learning using Neural Networks
Network – hidden layers
320
300
350

Slide 57:
Example:
We get the values output by our network when we were on state (s) and the maximum value computed by the network when we were on state s’ and create the target vector:

 













Reinforcement Learning using Neural Networks
40
-430
R=-8

Slide 58:
Example:
Using the prediction of the neural network at state s and using the target vector that we’ve just computed we compute the loss, apply backpropagation algorithm and update the neural network weights

 













Reinforcement Learning using Neural Networks
Network – hidden layers
330
40
-430
40
-430
307
prediction =
target       =
loss
Backpropagate &
Update weights

Slide 59:
Some tips to make q-learning work better with neural networks:
1.Replay Buffer or experience replay:
 Why?:
Consecutive states are highly correlated. Usually, there is just a slight 
modification between two states.

The network assumes the data is independent and identically distributed.
 
Without replay buffer the network will overfit on recent transitions and learning will be slow	












Reinforcement Learning using Neural Networks

Slide 60:
Reinforcement Learning using Neural Networks

Slide 61:
Some tips to make q-learning work better with neural networks:
1.Replay Buffer or experience replay:
Advantages	
Breaks correlations that are present in recent transitions

Each transition can be used multiple times

Data used in training might come from states that the current model won’t reach anymore since it has changed based on updates


(obs: one could prioritize tuples that have higher error). 
Reinforcement Learning using Neural Networks

Slide 62:
Some tips to make q-learning work better with neural networks:
2.Target Network:
 Why?:
If the same network is used both to compute the prediction and to update itself, then each update changes the target as well. (chasing a moving target)

Errors get amplified when using the same network for target and prediction. 
Ex: Estimate a greater Q’(s,a), then we update the weights which in turn estimates an even greater value for. Q’(s,a)

This causes oscillations and divergence. This leads to slow or no convergence. 	












Reinforcement Learning using Neural Networks

Slide 63:
Reinforcement Learning using Neural Networks

Slide 64:
Some tips to make q-learning work better with neural networks:
2.Target Network:
Advantages:
Training becomes more stable

Training is similar to supervised learning where targets are fixed

Fits good with replay buffer. Without target network, tuples saved in replay buffer become inconsistent (you will get to a different s’ state or get a different reward)



Reinforcement Learning using Neural Networks

Slide 65:
Some tips to make q-learning work better with neural networks:
3. ε-greedy exploration
 Why?:
We need the agent to explore as much as possible, especially in the beginning. Failing to do so will mostly lead to suboptimal solutions

As the agent trains we need to decrease exploration and increase exploitation. 













Reinforcement Learning using Neural Networks

Slide 66:
Reinforcement Learning using Neural Networks

Slide 67:
Reinforcement Learning using Neural Networks

Slide 68:
Reinforcement Learning using Neural Networks

Slide 69:
Reinforcement Learning using Neural Networks

Slide 70:
Reinforcement Learning using Neural Networks

Slide 71:
Example:
We will see an example of a q-learning algorithm applied on the previous example

You are a car and need to reach a destination (ex. the gas station)
You can move into 4 directions (← ↑ → ↓)
The car always moves (so there is no break)
There are also obstacles that the car must avoid
The car only knows what is in front and on its side (consider it is night and it can only see a limited distance that is highlighted by the headlights)

Obs: the car can’t see the map. Only we can
Q-Learning

Slide 72:
Example	

Slide 73:
Example	

Slide 74:
Example	

Slide 75:
Example:

Neural Network:
	The Neural network has 3 layers:
		input: 20 
		hidden: 150
		output: 3
	The activation of the first 2 layers is rectifier linear, while the output is linear
	
	The learning is performed using RMSprop algorithm

	The batch size is 100.




Example	

Slide 76:
Questions & Discussion

Slide 77:
“Reinforcement Learning: An Introduction”,  Richard S. Sutton , Andrew G. Barto, MIT Press, Cambridge, MA, 2017 
https://www.nervanasys.com/demystifying-deep-reinforcement-learning/
https://gym.openai.com
https://en.wikipedia.org/wiki/Q-learning
https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.kp2idsahl
http://outlace.com/Reinforcement-Learning-Part-3/
https://www.youtube.com/watch?v=yNeSFbE1jdY&spfreload=10
http://www.mcgovern-fagg.org/amy/courses/cs5033_fall2007/Lundgaard_McKee.pdf
http://cs231n.stanford.edu/reports2016/121_Report.pdf

References



================================================================================
COURSE 9: CONVOLUTIONAL NETWORKS
================================================================================

Slide 1:
Course 9: Convolutional Neural Networks
Deep Neural Networks

Slide 2:
How mammal brain sees the world
Correlation and Convolution
Fourier Transform
Convolutional Neural Networks
Implementation of a Conv Net in Pytorch


Overview

Slide 3:
How mammal brain sees the world

Slide 4:
How mammal brain sees the world
In the early 1950s, David Hubel and Torsten Wiesel performed some experiments on a cat in order to understand how the brain works

Several images with simple shape were slided in front of the cat eyes and activities of a neuron were recorded

You can watch the video here: https://www.youtube.com/watch?v=Cw5PKV9Rj3o



Slide 5:
How mammal brain sees the world
They have observed the following:
The visual cortex contains a complex arrangement of cells. These cells are sensitive to small regions of the visual field, called receptive field
The sub regions are tiled to cover the entire visual field
Two basic cells were identified: 
Simple: respond to specific edge (orientation and size)
Complex:are invariant to exact position of a pattern



Slide 6:
Correlation and Convolution

Slide 7:
Correlation and Convolution
Correlation and Convolution are one of the simplest, yet very useful operations that can be performed on images
They are:
Shift-invariant: the output of the convolution is affected by the same amount as each of the inputs
Linear: we replace each pixel of a linear combination of the neighbors


We will look at correlation and convolution as image operators. The images we will test are either 1 dimension (vector) or 2 dimensions (matrix)

Slide 8:
Correlation and Convolution
Example: local averaging as correlation.


Take an image as input and produce another image
We will average every 3 consecutive numbers:



What about the boundaries?

Slide 9:
Correlation and Convolution
Five way to treat the boundaries:
1. Pad with zeros (zeros)

2. Pad with the first and the last values (replicate)

3. Make input circular (circular)

3. Pad input in reflection (reflective)

4. Valid:. Do not apply the operation where there the are missing values. This reduces the output

Slide 10:
Correlation and Convolution
Padding is in important since:
It retains the same image size (improves performance)
The information on the borders would be washed away after a few layers.

Using zero padding might produce artifacts in the output image (which some times are desired). 
On the other hand, if one needs to upscale an image, other variants of padding should be used.

Slide 11:
Correlation and Convolution
Example: Same averaging operation, but this time by multiplying with 1/3 and adding the numbers.





box filter

Slide 12:
Correlation and Convolution

Slide 13:
Correlation and Convolution

Slide 14:
Correlation and Convolution

Slide 15:
Correlation and Convolution

Slide 16:
Correlation and Convolution
Cross-Correlation Operation:



Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/

Slide 17:
Correlation and Convolution
Cross-Correlation Operation:



Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/

Slide 18:
Correlation and Convolution
Cross-Correlation Operation:
Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/

Slide 19:
Correlation and Convolution

Slide 20:
Convolution operation in NN
Convolutional Operation:
Flipping the kernel:





With the resulted kernel, perform the cross-correlation operation
Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/
Flip rows
Flip columns

Slide 21:
Convolution operation in NN
Convolution Operation:



Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/
Flipped Filter

Slide 22:
Convolution operation in NN
Convolution Operation:



Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/
Flipped Filter
Result
=

Slide 23:
Correlation and Convolution
Convolution
The advantage of convolution is that it is associative. Correlation is not.
For example, we want to smooth an image and find it’s average. 
With correlation we have to do two correlation operations. 
With convolution a single operation is needed using a convolved Gaussian filter by an average one.

Convolution Is usually for image processing (smoothing) while correlation is used to match a template

Slide 24:
Correlation and Convolution
Convolution

Until now we’ve used convolution/correlation in order to modify an image. 

In order to understand why convolution/correlation is very powerfull when used in neural network, we must look at the Convolution Theorem


Slide 25:
Convolution Theorem

Slide 26:
Convolution Theorem

Fast Fourier Transform:

The Fast Fourier transform algorithm transforms an original function in a sum of wave-like cosine and sine terms.



Images from https://en.wikipedia.org/wiki/Frequency_domain

Slide 27:
Convolution Theorem
Fast Fourier Transform:
This makes it easy to understand for sounds. It transforms the function from time-domain to frequency domain.
Example: a song played by an orchestra

An equalizer is a filter that affects only some frequencies.



Slide 28:
Convolution Theorem

Slide 29:
Convolution Theorem
Fourier Transforms.

Images in Fourier space:
There is an axis that runs horizontally and
	One vertically. These tell how frequencies 
    vary horizontally or vertically
 The center of the image is (0,0) frequency

Images from https://www.cs.unm.edu/~brayer/vision/fourier.html

Slide 30:
Convolution Theorem
Fourier Transforms.
When an image is rotated, its Fourier Transform is also rotated (image a)
The Fourier Transform treats an image as it is part of a periodically replicated array of identical images extended to infinity (image b)

Images from https://www.cs.unm.edu/~brayer/vision/fourier.html

Slide 31:
Convolution Theorem
Fourier Transforms.
Most images will have a white blob at the center. This is because in an image, most of the transitions are slow (low frequencies)

The image on the right has been smoothed only on the horizontal. Notice how high frequencieshave been attenuated

Images from https://www.cs.unm.edu/~brayer/vision/fourier.html

Slide 32:
Convolution Theorem
A line must be represented by a high frequency pattern.
In the Fourier Transform a strong edge results into a line going to a high frequency perpendicular to the edge. Circular segments are also circular in FT


Images from https://www.cs.unm.edu/~brayer/vision/fourier.html

Slide 33:
Convolution Theorem

Slide 34:
Convolution Theorem
This makes filtering in Fourier space very easy.
As seen earlier the Fourier transform maintains important information from the original image. I.e. it retains the orientation of an edge.
In order to extract only certain information from an image (i.e. edge), we can create a filter that when transformed in Fourier space contains the Fourier transform of the edge and zeros


Line in spatial domain
Line in Fourier domain
Images from https://www.cs.unm.edu/~brayer/vision/fourier.html

Slide 35:
Correlation and Convolution: kernels
Images from https://en.wikipedia.org/wiki/Kernel_(image_processing)
Edge detection

Slide 36:
Convolution operation in NN
Kernels can be used to transform the image in a format that is easier to process. 
 


https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9

Slide 37:
Correlation and Convolution: kernels
Convolution or Correlation can be viewed as a way of feature engineering.

Feature engineering is one of the most important things in machine learning, but is also difficult.

We would like to use neural networks to create features

Convolutional Neural Networks are based on correlation (even though this can be seen as convolution with a flipped filter)

Slide 38:
Convolution Neural Network

Slide 39:
Convolutional Neural Network: architecture
A convolutional neural network is made of several layers of the following type:
Convolutional layer + nonlinearity (relu)
Maxpooling
Fully connected layers

Image source: https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/

Slide 40:
Convolutional Neural Network: architecture
The first layer is a convolutional layer. It is obtained by convolving a kernel (represented by a set of shared weights) over the input

Each neuron from the first convolutional layer is connected to only a subset of the input (called receptive field)
All the neurons in the hidden layer are connected to their receptive field using the same weights (shared weights)



Slide 41:
Convolution operation in NN
Output size:






Input
Filter
Result

Slide 42:
Convolutional Neural Network: architecture
Convolutional operations have several advantages:
Translation Invariance: they are able to detect patterns like edges, corners, textures regardless where they appear in the input

It is not dependent on the image size.

The number of parameters are reduced drastically
Example: 
an input of 28x28 with a kernel of 5x5 requires 25 weights . This results in an output of 24x24 pixels
A fully connected layer with the same output would require:  24x24x28x28 = 451584 weights

This allows as to have deeper networks with multiple kernels

Slide 43:
Convolutional Neural Network: architecture
Convolutional operation is differentiable
The kernels, which represent the weights of  the convolutional layer are learnt automatically by the network
They are randomly initialized 
They are adjusted, like standard weights, due to the backpropagation algorithm.
The convolution layers work because of the Convolution Theorem.  Even if we implement them as Correlations (so no flipping is happening), since they are learnt, we can consider the network learns the flipped kernel.


	

Slide 44:
Correlation and Convolution
Edge detection using correlation. Observe that the highest values in the output are where the filter matches the input.



Slide 45:
Correlation and Convolution
Edge detection using correlation. Observe that the highest values in the output are where the filter matches the input.



Slide 46:
Correlation and Convolution
Edge detection using correlation. Observe that the highest values in the output are where the filter matches the input.



Slide 47:
Convolution operation in NN
Output size:
Many times, we want to preserve spatial dimensions. Especially on deep neural network. For this we add padding:
Pytorch supports all padding previously discussed:
Zero
Replicate
Circular
Reflective




Images from: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning

Slide 48:
Convolution operation in NN
Images from: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning

Slide 49:
Convolution operation in NN
Strided convolutions:
Many times, we want to down sample to be able to reduce computation cost. 

Strided convolutions:
The filter moves across the input with steps larger than 1. 
In a traditional convolution, stride  S=1



Images from: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning

Slide 50:
Convolution operation in NN
Images from: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning

Slide 51:
Convolution operation in NN
Images from: Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning

Slide 52:
Convolutional Neural Network: architecture
If the image has more than two dimensions (i.e. RGB) the convolution must be performed on each channel separately and the results added.


Image from http://corochann.com

Slide 53:
Convolution operation in NN
Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/
The filter must have the same number of channels as the input

Slide 54:
Convolution operation in NN
Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/

Slide 55:
Convolution operation in NN
Images from https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/
Finally to make up a convolution layer, a bias (ϵ R) is added and an activation function such as ReLU

Slide 56:
Correlation and Convolution
Visualization of first and second layer of features from AlexNet



Slide 57:
Convolutional Neural Network: architecture
Followed by the convolutional layer is usually a pooling layer

The purpose of pooling is to reduce the size of the output (less parameters) reduce overfitting 

The pooling works independently on every Map. Thus there will be as many pooling maps as there are convolutional maps.

Pooling is also called downsampling

Image from http://cs231n.github.io/convolutional-networks/

Slide 58:
Convolutional Neural Network: architecture

Slide 59:
Convolutional Neural Network: architecture
Pooling
Although there are many pooling functions (L2 pooling, Average Pooling) the most used one is max-pooling.
In practice there are two variations of max pooling used:
F=2 and S=2
F=3 and S=2, also called overlapping pooling




Image from http://cs231n.github.io/convolutional-networks/

Slide 60:
Convolutional Neural Network: architecture
Pooling
Although there are many pooling functions (L2 pooling, Average Pooling) the most used one is max-pooling.
In practice there are two variations of max pooling used:
F=2 and S=2
F=3 and S=2, also called overlapping pooling




Image source: https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/


Slide 61:
Convolutional Neural Network: architecture
Pooling
After the pooling layer, the number of channels remains unchanged, but the width and height will be changed.



Image source: https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/


Slide 62:
Convolutional Neural Network: architecture
The fully connected layer are usually used last and their purpose is to exploit the features created by the convolutional + pooling layers before

The fully connected layers are standard layers of neurons that usually use RELU as activation function

Slide 63:
Convolutional Neural Network: architecture
Common architectures:
   The most common convolutional neural networks connect the input to a stack of convolutional layers with relu activations followed by maxpooling. 
After the input has been sufficiently reduced, the classification part is carried by the fully connected layers.
To denote architectures, the following notation will be used:
Conv = convolutional layer
Relu = relu activation
FC = Fully connected
Pool = pooling (usually max-pooling)
Input = input image

Slide 64:
Convolutional Neural Network: architecture

Slide 65:
Convolutional Neural Network: architecture
Common architectures:
InputFCReluFC  (MLP with relu activation)
InputConvReluFC
Input[ConvReluPool]*2FCReluFC (each convolutional layer is followed by a pooling layer)
Input [ConvReluConvReluPool]*3[FCRelu]*2FC (here, pooling is used after a set of 2 convolutional layers. This is done in order to obtain better features, before applying downsampling 

Slide 66:
Convolutional Neural Network: architecture
Recommendations:
Use small filters and many layers rather than large filters
Suppose we have the following architecture in the convolutional part of the network ConvReluConvReluConvRelu
The filter size is 3 and the stride is 1.
In this case, The first convolutional layer looks at 3x3 of the input, the second at 5x5 while the third 7x7
This is better than a single convolutional layer of 7x7

Slide 67:
Convolutional Neural Network: architecture
Recommendations:
In this case, The first convolutional layer looks at 3x3 of the input, the second at 5x5 while the third 7x7
Source: Dong, Zhiang & Xie, Miao & Li, Xiaoqiang. (2023). Multi-Scale Receptive Fields Convolutional Network for Action Recognition. Applied Sciences. 13. 3403. 10.3390/app13063403.

Slide 68:
Convolutional Neural Network: architecture

Slide 69:
Convolutional Neural Network: architecture
Recommendations:
Prefer small filters over large ones (i.e. 3x3, 5x5)
Retain the input size in the convolutional layers and let pooling do the down sampling. (use stride of 1 and padding)
Keep pooling to 2x2 or 3x3 and stride 2. Larger pooling are too destructive
Use Padding (in order not to lose information from the edges)
Compromise only on the first layer(i.e. 7x7 and stride 2)

Slide 70:
Convolutional Neural Networks
Parameters
in_channels (int) – Number of channels in the input image
out_channels (int) – Number of channels produced by the convolution
kernel_size (int or tuple) – Size of the convolving kernel
stride (int or tuple, optional) – Stride of the convolution. Default: 1
padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0
dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1
groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1
bias (bool, optional) – If True, adds a learnable bias to the output. Default: True
padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'

Slide 71:
Convolutional Neural Networks
Parameters
kernel_size (Union[int, Tuple[int, int]]) – the size of the window to take a max over
stride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is kernel_size
padding (Union[int, Tuple[int, int]]) – Implicit negative infinity padding to be added on both sides
dilation (Union[int, Tuple[int, int]]) – a parameter that controls the stride of elements in the window
return_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later
ceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape


Slide 72:
Convolutional Neural Network: implementation
Implement MNIST classification using using convolutional neural network
Conv 3x3.     MaxPooling 2x2     Conv 3x3    MaxPooling 2x2               Reshape. 

Slide 73:
Convolutional Neural Networks
class CNN(nn.Module):
    def __init__(self, in_channels=1, num_classes=10):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=8,
            kernel_size=3,
            stride=1,
            padding=1,
        )

Slide 74:
Convolutional Neural Networks
class CNN(nn.Module):
    def __init__(self, in_channels=1, num_classes=10):
        . . .
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(
            in_channels=8,
            out_channels=16,
            kernel_size=3,
            stride=1,
            padding=1,
        )
        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)
 
Create the rest of the layers neededMaxPooling: 2x2 with stride 2. Since maxpooling doesn’t have parameters we can initialize just one

Another Conv2 layer, similar to previous, but with 16 nr of output channels.

A fully connected layer. The input size is computed as such: 28 /2(1st  maxpooling)=14,  14/2 (2nd maxpooling)=7.So 7x7 x 16 output channels 

Slide 75:
Convolutional Neural Networks
class CNN(nn.Module):
   . . .
   def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.reshape(x.shape[0], -1)
        x = self.fc1(x)
        return x
 
Implement the forward method:

Call the layers in order:
1st convolutional layer followed by relu activation
Use maxpooling
2nd convolutional layer followed by relu activation
Reshape (c,w,h) to 1d . Keep batch size
Apply fully-connected layer that outputs the classes

Slide 76:
Convolutional Neural Network: implementation
Training a simple Convolutional Neural Network on mnist dataset. The rest of the code is the same as in a classical neural network. Just need to remove the reshape(x.shape[0],-1)


100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 113.48it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:09<00:00, 102.39it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 107.37it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 115.21it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 111.51it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 116.38it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 115.38it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 117.01it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 111.25it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 111.64it/s]
Accuracy on training set: 98.17
Accuracy on test set: 98.25


Slide 77:
https://www.youtube.com/watch?v=FwFduRA_L6Q (demo of LeNet1 , first Convolutional Neural Network )
Questions & Discussion

Slide 78:
http://cs231n.github.io/convolutional-networks/(very good explanation of ConvNet)

https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/

http://timdettmers.com/2015/03/26/convolution-deep-learning/

https://www.cs.unm.edu/~brayer/vision/fourier.html
https://grzegorzgwardys.wordpress.com/2016/04/22/8/ (if you are interested in the derivatives of the convolution)




Bibliography



================================================================================
COURSE 10: ACTOR CRITIC
================================================================================

Slide 1:
Course 10: Reinforcement Learning. On-policy 
Neural Networks

Slide 2:
Reinforcement Learning
Off Policy vs On Policy
Gradient Policy Algorithm
Actor Critic
Questions

Overview

Slide 3:
Reinforcement Learning

Slide 4:
Reinforcement Learning: concepts
Reinforcement learning is learning what to do--how to map situations to actions--so as to maximize a numerical reward signal

Reinforcement learning has 3 basic components:
Agent 
Environment
Actions





Slide 5:
Reinforcement Learning: concepts
An agent has 4 basic components:
A policy
	The most important. Is the decision making function. Defines what should the agent do in 	any of the situations it encounters
A reward function
	Defines the goal of the RL agent. It maps the state of the environment to a single number, a 	reward, indicating the intrinsic desirability of the state
A value function
	Defines what is good in the long run. More exactly, the total amount of reward 	the agent can expect starting from that state
A model of the environment
	Defines the states as well as the way to transition from one state to another (through actions)





Slide 6:
Reinforcement Learning: concepts

Slide 7:
Off Policy vs On Policy

Slide 8:
Off Policy vs On Policy

Slide 9:
Off Policy vs On Policy

Slide 10:
Off Policy vs On Policy

Slide 11:
On policy vs Off policy
Off policy algorithms can update the estimated value functions using hypothetical actions, those which have not actually been tried
On policy algorithms update the value function strictly based on experience


The on-policy methods can take into account actions that might bring a very high penalty and adapt accordingly
The on-policy methods can get stuck in local minimum 



Off Policy vs On Policy

Slide 12:
A classic example:
A mouse has to find the path to its house. It can move N, W, S, E
Between the mouse and its house there is a cliff.

If the mouse gets to the cliff, a big reward(penalty) will be received: -100
For every move he makes he gets a reward (penalty) of -1

With a probability of 0.1, the mouse makes a random move  (in order to explore the environment)





Off Policy vs On Policy

Slide 13:
A classic example:
The agent using Q-learning finds the optimal policy: it moves along the cliff and gets to the reward.
	
	    However, from time to time (due to exploration ) the mouse falls into the cliff

The agent using SARSA learning understands that from time to time it performs some random actions and tries to adapt accordingly.
	
	    For this reason, the agent goes several steps away from the cliff and then starts 	orienting to the target






Off Policy vs On Policy

Slide 14:
Gradient Policy Algorithm

Slide 15:
Until now, we’ve been using an algorithm (Q-learning or SARSA) to decide what are actions will be

We’ve used neuronal networks to approximate the value function

However, we can use neural networks to directly learn the right actions the agent needs to take in order to increase the total reward

We’ll use the car example from the previous class






Gradient Policy Algorithm

Slide 16:
Example
Example: Train a car to avoid obstacles and reach to the gas station

States: the car only knows what is in front and on its side. 
It does that through 20 sensors that each can detect an object that is at most 10 units away
(it does not know where the gas station is) or how the map looks like

(This is more realistic)


Slide 17:
Example
Actions:
The car always moves. It can, however, decide to move left , right or do nothing (move forward)


Slide 18:
Until now, we’ve been using an algorithm (Q-learning or SARSA) to decide what are actions will be

We’ve used neuronal networks to approximate the value function

However, we can use neural networks to directly learn the right actions the agent needs to take in order to increase the total reward

For this, we’ll use the Policy Gradient Algorithm






Gradient Policy Algorithm

Slide 19:
Gradient Policy Algorithm
Gradient Policy Advantages:
Policy Based Methods have better convergence. We’re just computing the gradient to say how we can can increase the score. We don’t have to explore that much

Works better in high dimensional action spaces. For example when you don’t have a fixed set of actions, but a value that you have to output
Can learn Stochastic policies:
We don’t need to implement an exploration vs exploitation strategy (i.e. 𝜺- greedy strategy)
Disadvantages:
It finds the local maximum instead of the global one









Slide 20:
We’ll define a policy network that defines the agent (i.e. car)

The network will take the state of the agent (i.e. 10 sensors) and will output which state is the most probable one.

We will train the network using gradient descent in order to maximize the reward

So the reward function will act similar to our cost function








Gradient Policy Algorithm
state
Network – hidden layers

Slide 21:
Gradient Policy Algorithm
We need to define our Reward function based on the policy. Such that an update on the neural network (policy) will result in an increase of the reward.
Our policy (based on the neural network(𝜃) is defined as below
	
	
	
Based on the policy, we can define our value function, that we want to maximize


	









Slide 22:
Gradient Policy Algorithm
Having a full episode starting from the first state, and following our policy 
We compute the objective function
We compute the gradient of the Objective function w.r.t. 𝜃
We update the network weights, to increase the value of the Objective function:








Slide 23:
Gradient Policy Algorithm
The hardest part is to understand how we obtain the gradient: We first expand the expectation:

	
We differentiate w.r.t 𝜃


	We use the following observation: 
		





Slide 24:
Gradient Policy Algorithm
we multiply and divide by 

	
We replace with the logarithm based on the previous observation


	We transform it to expectation
		

	During training, we’ll take samples of episodes, so we’ll just approximate the gradient by the sum.
	

Slide 25:
Gradient Policy Algorithm
We’ll analyze                              in more detail  

Transform it into a sum  
Differentiate wrt to 𝜃
This first sum is dependent on the environment, not on the agent. So we get:

	

Slide 26:
Gradient Policy Algorithm
Replacing this in our Reward function

This can also be noted as follows: 
In case future rewards are discounted, we can arrive at a similar formula, 




Slide 27:
Gradient Policy Algorithm
So, the formula for the gradient of the reward function wrt to the policy is:

		   where



Slide 28:
Gradient Policy Algorithm
This tells us that, in order to train the network to choose an action that will maximize the reward we have to, at each time step
Choose an action  according to our policy:
	
Compute the log of the probability of the action that was chosen

Compute the discounted future return and multiply it by the log value.      The problem here is that we only know this when the episode ends

Compute the gradient of this value w.r.t to each weight in the network:


Slide 29:
Initialize the network

Our network we’ll receive as input the the current environment state

Since we want to be stochastic, we’ll use a SoftMax layer as the final layer

The weights are initialized randomly, depending on the activation function 








Gradient Policy Algorithm: detailed steps
state
Network – hidden layers
Softmax

Slide 30:
Gradient Policy Algorithm:detailed steps
Generate episodes

For each episode:
We will feed the state of the agent to the neural network at each time - step (just like in q-learning)
After the feed-forward step we will choose an action in a stochastic way, based on the softmax layer:
I.e. if we have 3 actions with the probabilities: 0.1, 0.3, 0.6, we’ll generate a random number between 0 and 1 and choose the action based on the interval it is in
Compute the action, move into a new state and get the reward. 


Slide 31:
Gradient Policy Algorithm:detailed steps
Compute the reward:

	In most of the cases, the reward can only be computed at the end of the episode.

	When an episode finishes, we’ll compute the discounted rewards for every state (from the final state to the initial one):





		

Slide 32:
Gradient Policy Algorithm:detailed steps
Compute the reward:

	Since there might be very long episodes and the reward is computed at the end, it is possible to have a big variance in the rewards 

	One way to solve this is to standardize the results (i.e. z-score). This way we’ll increase the probability of better than average results and decrease the probability of the others.





  

		

Slide 33:
Gradient Policy Algorithm:detailed steps
Compute the gradient

For every time-step from the completed episode:
We compute the log of the probability of the action we have chosen
Multiply the value by the previously computed future discounted  reward
Perform backpropagation to obtain the gradient in respect to each weight



  

		

Slide 34:
Gradient Policy Algorithm:detailed steps
Compute the gradient

Some neural network frameworks allow to obtain the gradient automatically, while others expect a loss function.Transforming the computation of the gradient into a loss function:
If you remember from the 4th course, the formula for cross-entropy, for a multilabel network:


	



  

		

Slide 35:
Gradient Policy Algorithm:detailed steps
Compute the gradient

Written in term of probabilities, as our gradient, the cost gets:

So, in order to compute the gradients required by the policy gradient algorithm,                               
use the categorical cross entropy function, as the loss function

Create a target vector of the type one-hot, and set the  tj=Gt




	



  

		

Slide 36:
Gradient Policy Algorithm:detailed steps
Adjust the weights

Using a neural network framework (such as pytorch), this is done automatically. We just have to provide the input (st )the target (Gt ).

The update must be performed at the end of one or multiple episodes. Not during the episode. We’re performing an off-policy update of an on-policy algorithmIf the gradients were computed separate, not part of an automated training process, the update is:




	



  

		

Slide 37:
Actor Critic

Slide 38:
Current reinforcement algorithms presented:
Q-learning:
learns to predict a score for each action, value pair. Based on it we can choose the optimal policy
each small improvement in the Q function translates into an improvement of the policy → training is faster

Policy Gradient:
it’s stochastic and learns what it’s the action that will most probably improve the overall reward. 
learning the policy is usually simpler than the Q functions.
 


Actor Critic

Slide 39:
Some disadvantages of policy gradient:

We need a full episode in order to get the discounted result. This slows down the learning
There are cases when the reward at the end of the episode is very big. This final big reward, discounted at each previous step, will make all the previous decisions as either bad or good
 


Actor Critic
s1
R
actian a1 taken at state s3 was a bad action but was labeled as good due to a final good reward

Slide 40:
Actor Critic
Actor Critic tries to bring advantages from both worlds

It is made of two parts: actor + critic
Actor:
A policy gradient algorithm: improves its policy in order to maximize the predicted rewards
Critic:
Evaluates each state and action and predicts the final reward
Both the actor and the critic are implemented using neural networks
 



Slide 41:
Actor Critic
Q-Actor Critic
 
Formula used for policy gradient algorithm 
Gt is actually de Q(s,a) function.                                                

 
discounted future rewards
future rewards
or

Slide 42:
Actor Critic
Q-Actor Critic
 
We can introduce the Q(s,a) into the policy gradient algorithm
We know we can have a neural network that can learn to approximate  the  Q(s,a) function


Slide 43:
Actor Critic
Training pseudocode:
Initialize neural network for Actor (similar to policy gradient algorithm): 𝜃
Initialize neural network for Critic (similar to Q-learning algorithm): w

Generate Episodes:
	      Use actor to sample an action for the initial state

For t = 1 .. T do:
Perform the action, get the reward rt and the next state s’
Use Actor to sample an action for the next state
Update the actor network 
Update the critic network
 a=a’ and   s=s’

Slide 44:
Actor Critic
Advantage-Actor Critic
 
Similar to Q-critic, but instead of multiplying by Qw(s,a) we multiply the gradient by an advantage function:
So, the gradient becomes:

 

Slide 45:
Actor Critic
Advantage-Actor Critic
 


The idea of the advantage is to compute how much better is to take a specific action at a given state compared to the what is believed to be the best so far. 
The problem with this approach is that we don’t know how to approximate V(st) . We could do it with a neural network, but that brings unnecessary overhead
 

Slide 46:
Actor Critic
Advantage-Actor Critic
 

However, we can write the Q function using the V function:
So, the advantage function becomes:
So, we can use only one neural network, that will approximate V in order to compute the advantage.

Slide 47:
Actor Critic
Depending on how we perform the updates, there can be two variations of the advantage actor critic network:
on-policy update: we perform the update after each step:This is similar to the Q-Actor Critic. This has the advantage of a low variance of the rewards, but is biased on the current policy:
off-policy update: we wait until the episode finishes and perform the update for each visited state.This is similar to the policy gradient algorithm. In this case we can compute the correct values of the Q(s,a) to compute the advantage.
 


Slide 48:
Advantage Actor Critic:

Training the Network that will output the Value state
Input layer is the state of the agent
Output is a scalar, the V(s) function
Since this is a regression problem, MSE function will be used for the cost function




Gradient Policy Algorithm: detailed steps
state
Network – hidden layers
linear
V(s)

Slide 49:
Gradient Policy Algorithm: detailed steps
Network – hidden layers
linear
st
V(st +1) +  rt 

Slide 50:
Advantage Actor Critic: off-policy update

In this scenario, we will update the network at the end of the episode
For each time step , store the st , rt and the predicted V(st )  
After the episode is finished, go backward and compute the discounted reward, at each timestepUpdate the network using input: st and the target Q(st,at) 





Gradient Policy Algorithm: detailed steps
Network – hidden layers
linear
st
Q(st )  real discounted reward

Slide 51:
Implementation

Slide 52:
Implementation
Lunar Lander environment:

Action Space

There are four discrete actions available:
0: do nothing
1: fire left orientation engine
2: fire main engine
3: fire right orientation engine

Observation Space

The state is an 8-dimensional vector: 
the coordinates of the lander in x & y,
its linear velocities in x& y, 
its angle, 
its angular velocity, 
two booleans that represent whether each leg is in contact with the ground or not.

image & text  from: https://gymnasium.farama.org/environments/box2d/lunar_lander/

Slide 53:
Implementation
Lunar Lander environment:

Rewards
After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.
For each step, the reward:
is increased/decreased the closer/further the lander is to the landing pad.
is increased/decreased the slower/faster the lander is moving.
is decreased the more the lander is tilted (angle not horizontal).
is increased by 10 points for each leg that is in contact with the ground.
is decreased by 0.03 points each frame a side engine is firing.
is decreased by 0.3 points each frame the main engine is firing.
The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.

An episode is considered a solution if it scores at least 200 points
The episode finishes if:
the lander crashes (the lander body gets in contact with the moon);
the lander gets outside of the viewport (x coordinate is greater than 1);

text from: https://gymnasium.farama.org/environments/box2d/lunar_lander/

Slide 54:
Implementation
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import gymnasium as gym
from torch.distributions import Categorical
import math
Define the necessary imports
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Create imports for training the lunar lander.

Gymanasium package provides the environment for training the lunar lander

torch.distributions.Categorical is used to choose an action based on a set of probabilities

Ex.







probs = torch.tensor([0.2, 0.7, 0.05, 0.05])
m = Categorical(probs)
m.sample()
m.sample()
m.sample()
m.sample()
m.sample()
m.sample()


Slide 55:
Implementation
# Actor Network
class Actor(nn.Module):
    def __init__(self, input_size, num_actions):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, num_actions)
 
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.softmax(self.fc2(x), dim=-1)
        return x
 
 
Define the actor network
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Define a simple network for Actor Network:

1 layer that receives the state ( 8 units ) and sends them to a 64 units hidden layer with ReLU activation
1 output layer of num_actions (4) with softmax activation

Slide 56:
Implementation
# Critic Network
class Critic(nn.Module):
    def __init__(self, input_size):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 1)
 
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
 
Define the critic network
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Define a simple network for Critic Network:

1 layer that receives the state ( 8 units ) and sends them to a 64 units hidden layer with ReLU activation
1 output layer responsible of outputing a scalar value

Slide 57:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
    optimizer_actor = optim.Adam(actor.parameters(), lr=lr_actor)
    optimizer_critic = optim.Adam(critic.parameters(), lr=lr_critic)
 
    env = gym.make('LunarLander-v3',)
    
    last_20_scores = torch.zeros(20)
    episode = 0
 
Define the actor critic method that will train the actor and the critic networks
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Define the actor_critic method that receives: the actor and critic model as well as some parameters for the optimizer (gamma and lr ).

Initialize the environment. This will not show the window, thus will make the training faster. 

last_20_scores is used for the stopping condition. Other choice for stopping could have been used, like a nr of iterations.

Slide 58:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
        episode+=1
 
        state = env.reset()[0]
        ep_return = 0
        done = False
        step_count = 0        

Define the necessary imports
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Define the stopping condition: no episode less than 200 in the last 20 episodes

Initialize some variables that we will update during training:
ep_return: the entire reward value across an episode
done : if the episode is finished
step_count : nr of steps
episode : current episode count

Slide 59:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            state_tensor = torch.tensor(state, dtype=torch.float32)
            
            # Actor selects action
            action_probs = actor(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample()
            
            # Take action and observe next state and reward
            next_state, reward, done, _,_ = env.step(action.item())
 
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Convert the state to a tensor.
Send the state to the actor model and the probabilities of each action given the state.
Sample an action based on the probabilities.

Take the action and observe the next_state, the immediate reward, if the episode if finished (done). Step also returns other info like if the episode was truncated or other debug information , but these are skipped: (https://gymnasium.farama.org/api/env/#gymnasium.Env.step)

Slide 60:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            . . .
            #add distance as penalty: 	   
            x= next_state[0]
            y= next_state[1]
            #bmr
            train_penalty = y*y
            train_penalty += x*x
            train_penalty += step_count*0.1 
 
            reward -= train_penalty
  
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Add a penalty to the reward in order to improve training. The reward will pe penalized by how far the ship is from the landing zone (0,0). A penalty is also added that is directly proportional to the number of steps.

Slide 61:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            . . .	   
            # Critic estimates value function
            value = critic(state_tensor)
            next_value = critic(torch.FloatTensor(next_state))
            
            # Calculate TD target and Advantage
            td_target = reward + gamma * next_value * (1 - done)
            advantage = td_target - value
   
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Use the critic to evaluate the current state and the next state.
Compute the value based on the current reward and the value of the next state, discounted by gamma.
Compute the advantage between the future state and the current state.

Slide 62:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            . . .	   
	   # Critic update with MSE loss
            critic_loss = F.mse_loss(value, td_target.detach())
            optimizer_critic.zero_grad()
            critic_loss.backward()
            optimizer_critic.step()
    
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Update the Critic network that evaluates the value of the current state. Compute the loss between the output (value) at the current state and the value based on the next state and the immediate received reward.

Update the weights of the networks.

td_target is computed based on next_value which is the output of the critic network. We just want a value for the loss, we do not want gradients to flow through the network that computed the next_value. .detach returns the tensor , but detached from computation graph. Doesn’t require a gradient.

Slide 63:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            . . .	   
            # Actor update
            log_prob = dist.log_prob(action)
            actor_loss = -log_prob * advantage.detach()
            optimizer_actor.zero_grad()
            actor_loss.backward()
            optimizer_actor.step()
     
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Update the Actor network. This is being performed using the derived formulas for the advantage actor critic.

Compute the log of the prob. of the action that was taken. This is equivalent to calling torch.log(action_probs[action]
Multiply by the advantage computed. This is equiavalent to computing rewards in the future
Use this product as a loss function (use a minus to minimize)
Perform backpropagation and update weights

Slide 64:
Implementation
def actor_critic(actor, critic, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):
   . . .
   while torch.min(last_20_scores) <200:
      . . .
      while not done and step_count < max_steps:
            . . .	   
           # Update state, episode return, and step count
           state = next_state
           ep_return += (reward +train_penalty)
           step_count += 1
      # Add the score to the last 20 scores
      last_20_scores = torch.cat((last_20_scores[1:],torch.tensor(ep_return).unsqueeze(dim=0)))
   # training is finished. Close the environment
   env.close()
    
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Move to the next state.Add the reward to the total episode return. The train_penalty is added here just so we can compare to the the ep_return required by the framework. We’ve substracted the penalty before computing the loss, so we’re adding here to cancel that.

Update the total step_count. (This is used in the loss).Add the episode reward to the queue of last 20 episode rewards and remove the oldest one.

Slide 65:
Implementation
def test_trained_agent(actor):
  # Test the trained agent in human mode
    env = gym.make('LunarLander-v3',render_mode='human')
    state = env.reset()[0]
    done = False
    total_reward = 0
    max_steps = 2000  # Maximum steps per episode for testing
 
    
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
Define the method to test the agent.Initialize the environment in human mode. This will show the interface.
Get an initial stateInitialize variables that will be used during episode generation:* done : to decide if the episode has finished* total_reward : to output the total reward during episode* max_stepts: the maximum number of steps after which to termine an episode.

Slide 66:
Implementation
def test_trained_agent(actor):
. . .
    while not done:
        env.render()
        state_tensor = torch.FloatTensor(state)
        action_probs = actor(state_tensor)
        action = torch.argmax(action_probs).item()
        state, reward, done, truncated,info = env.step(action)
        total_reward += reward
 
        if total_reward >= max_steps or done:
             break
    
     print(f"Total reward in human mode: {total_reward}")
 
 
    
code from: https://medium.com/@dixitaniket76/advantage-actor-critic-a2c-algorithm-explained-and-implemented-in-pytorch-dc3354b60b50
While the episode has not finished:

Render the environment
Send the state to the actor and get the probabilities for each action, given the state
Get the action with the highest probability
Move to next state and get reward
Compute total reward for this episode
If exceeded max nr of steps, break the loop

Slide 67:
Questions & Discussion

Slide 68:
http://www2.econ.iastate.edu/tesfatsi/RLUsersGuide.ICAC2005.pdf
https://www.nervanasys.com/demystifying-deep-reinforcement-learning/
https://gym.openai.com
http://karpathy.github.io/2016/05/31/rl/
https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f
https://towardsdatascience.com/policy-gradient-methods-104c783251e0
https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/

References



================================================================================
COURSE 11: LSTM
================================================================================

Slide 1:
Course 11: Recursive Neural Networks
Deep Neural Networks

Slide 2:
How Recurrent Neural Networks Work?
Backpropagation Through Time
Long Short Term Memory
Variations of LSTM





Overview

Slide 3:
Recurrent Neural networks are used in many fields, such as:
Image generation
Sound generation
Text generation
Image Captioning
Neural Machine Translation








Recurrent Neural Networks

Slide 4:
Image Captioning








Recurrent Neural Networks
Image from http://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/

Slide 5:
Music Generation
https://youtu.be/A2gyidoFsoI?t=105








Recurrent Neural Networks

Slide 6:
Recursive Neural Networks
How Recurrent Neural Networks Work?

Slide 7:
How Recurrent Neural Networks Work?
In a traditional neural network we assume all inputs are independent from one another
The idea of recurrent neural networks is to make use of data that comes in sequences.
They are called recurrent because they perform the same operations on every input, which is part of a sequence

Another way to think of RNN is that they have memory that summarize what has been computed so far


Slide 8:
How Recurrent Neural Networks Work?
There is many data that comes into sequences:
Sound (voice, music)


DNA Sequences : ATGAATGATGATGCATCATGT…

Sentences : (sequence of words, or sequence of characters):

Videos (i.e. detecting violence)





Slide 9:
How Recurrent Neural Networks Work?
While a single element in a traditional neural network will look as a set of features (i.e. a vector x), when working with sequences we need to see them as two dimensional: 
	(nr_seq, n ).

Example:
an image  has the shape:  (3, W, H). A batch of images has the shape: (B, 3, W, H)

A video has the shape (S, 3, W, H).  A batch of videos has the shape (B, S, 3, W, H).

 A video element has the same shape as a batch of images. The difference is when training using sequences is that we want to find information from the sequence as well. When training a batch of images we see each image separately 



Slide 10:
How Recurrent Neural Networks Work?
As with ANN or ML, the recurrent neural network can be viewed as having 3 layers:
Input
Hidden: in RNN this is called the state.
Output
The main difference is that the hidden layer (the state) changes w.r.t the input and the previous hidden state 
Images from https://ayearofai.com

Slide 11:
How Recurrent Neural Networks Work?
Images from https://ayearofai.com

Slide 12:
How Recurrent Neural Networks Work?
Depending on the time of when a new input is fed into the network and when an output is analyzed there can be many types of RNN architectures:
One to Many
    I.e.: Captioning an image.
Input = processed image (using conv-nets)

Output = A sentence in which each word is outputted by the network at a time step
Images from https://ayearofai.com

Slide 13:
How Recurrent Neural Networks Work?
Many To one
    I.e.: Sentiment classification.
Input = a sentence(sequence of words), one word per time step
Output = A sentiment (classification) after all of the words have been processed

Many To Many
    I.e.: Machine translation
Input = sequence of words (sentence), one word at some time step
Output = sequence of words (sentence), one word at some time step

Images from https://ayearofai.com

Slide 14:
How Recurrent Neural Networks Work?

Slide 15:
Recursive Neural Networks
Backpropagation Through Time

Slide 16:
Backpropagation Through Time
Training an RNN is performed using Backpropagation.
However, in order to use it, we must unfold the neural network (in time). This is why the algorithms is called Backpropagation through time (BTT). 




Slide 17:
Backpropagation Through Time

Slide 18:
Backpropagation Through Time

Slide 19:
Backpropagation Through Time

Slide 20:
Backpropagation Through Time

Slide 21:
Backpropagation Through Time

Slide 22:
Backpropagation Through Time
Loss to hidden 
Temporal Propagation
Hidden to weight

Slide 23:
Backpropagation Through Time
If we have multiple – outputs, then we need to add the losses for each output

Slide 24:
Backpropagation Through Time
Loss to hidden 
Temporal Propagation
Hidden to weight
Every loss 

Slide 25:
Backpropagation Through Time
Loss to hidden 
Temporal Propagation
Hidden to weight
Every loss 

Slide 26:
Backpropagation Through Time

Slide 27:
Backpropagation Through Time

Slide 28:
Backpropagation Through Time
The network can be unfolded, just like a neural network with one hidden RNN. 








Slide 29:
RNN in pytorch

Slide 30:
RNN in pytorch
Pytorch has support for RNN modules: torch.nn.RNN 
The data can be provided in the following formats:
    (SEQ_NR,  BatchSize, data shape) or (BatchSize,  SEQ_NR, data shape) 

    All data from the same batch have the same number of sequences. This is because of how the GPU works
   If this is not the case (as for most of sequence data), you can use the method:  torch.nn.utils.rnn.pack_sequence







Slide 31:
RNN in pytorch
Parameters
sequences (list[Tensor]) – A list of sequences of decreasing length.

enforce_sorted (bool, optional) – if True, checks that the input contains sequences sorted by length in a decreasing order. If False, this condition is not checked. Default: True.
import torch
from torch.nn.utils.rnn import pack_sequence
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5])
c = torch.tensor([6])
x = pack_sequence([a, b, c])
print(x)

Slide 32:
Backpropagation Through Time
Parameters
input_size – The number of expected features in the input x
hidden_size – The number of features in the hidden state h
num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1
nonlinearity – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'
bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True
batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False
dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0
bidirectional – If True, becomes a bidirectional RNN. Default: False

Slide 33:
RNN in pytorch

Slide 34:
RNN in pytorch

Slide 35:
RNN in pytorch
Train a RNN on MNIST digits:
Normally, mnist digits are not necessarily sequence data, but they can be transformed into one:




We can view a digit as a sequence of length 28, while each element from the sequence has the length 28

Slide 36:
RNN in pytorch
# Imports
import torch
import torch.nn.functional as F # for activation functions
import torchvision.datasets as datasets # for MNIST
import torchvision.transforms as transforms # convert into tensor
from torch import optim # For optimizers like Adam
from torch import nn # For RNN
from torch.utils.data import DataLoader # for batching
from tqdm import tqdm # For a rogress bar!

device = "cuda" if torch.cuda.is_available() else "cpu"

Define the necessary imports

Slide 37:
RNN in pytorch
# Hyperparameters
input_size = 28
hidden_size = 256
num_layers = 2
num_classes = 10
sequence_length = 28
learning_rate = 0.001
batch_size = 64
num_epochs = 10

Our RNN will have two layers, each  with a hidden vector of 256 elements
Set the hyperparameters

Slide 38:
RNN in pytorch
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)
Store the configuration.
Define the RNN layers: 
the input size: the size of each element in the sequence 28* hidden_size : the number of neurons in each hidden layer:256* num_layers: the number of hidden layer: 2* batch_first : the data will be in the following format (N, S, 28 )  = ( 64, 28, 28)

Add a hidden layer that connects the output at each time step to the an output layer . (many to one)

Slide 39:
RNN in pytorch
class RNN(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
 
        # Forward propagate LSTM
        out, _ = self.rnn(x, h0)
        out = out.reshape(out.shape[0], -1)
 
        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out
 
Initialize the hidden vector of each layer.
There must be one hidden vector per layer, per input in the minibatch. The size of each vector we’ve defined in the configuration 

Slide 40:
RNN in pytorch
class RNN(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
 
        # Forward propagate LSTM
        out, _ = self.rnn(x, h0)
        out = out.reshape(out.shape[0], -1)
 
        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out
 

Slide 41:
RNN in pytorch
class RNN(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
 
        # Forward propagate LSTM
        out, _ = self.rnn(x, h0)
        out = out.reshape(out.shape[0], -1)
 
        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out
 
Connect all the outputs to a fully connected layer. The output of the fully connected layers is the classification of the input


Slide 42:
RNN in pytorch
 
# Load Data
train_dataset = datasets.MNIST(
root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(
root="dataset/", train=False, transform=transforms.ToTensor(), download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)
 
 
Download the mnist training and testing dataset. 
Transforms.ToTensor just converts the input to tensor.Initialize two DataLoaders, one for training and one for testing, using a batch_size of 64
Download mnist data


Slide 43:
RNN in pytorch
Initialize the model

# Initialize network
model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
Initialize the network and transfer the model to the detected device(i.e. gpu)

Initialize a cross function: CrossEntropyIntialize an optimizer with the model parameters : Adam. 

Slide 44:
RNN in pytorch
for epoch in range(num_epochs):
    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):
        # Get data to cuda if possible
        data = data.to(device=device).squeeze(1)
        targets = targets.to(device=device)
 
        # forward
        scores = model(data)
        loss = criterion(scores, targets)
 
        # backward
        optimizer.zero_grad()
        loss.backward()
 
        # gradient descent update step/adam step
        optimizer.step()
 
The shape of a tensor from MNIST is: (batchsize, 1, 28, 28). By calling squeeze (2) we remove the redundant 1.

Slide 45:
RNN in pytorch
Results from training using a RNN:

100%|█████████████████████████████████████████████████| 938/938 [00:09<00:00, 103.25it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 105.08it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:09<00:00, 103.99it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 104.78it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 104.75it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 105.23it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 104.77it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:09<00:00, 103.04it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 105.49it/s]
100%|█████████████████████████████████████████████████| 938/938 [00:08<00:00, 104.97it/s]
Accuracy on training set: 98.411667
Accuracy on test set: 97.83


Slide 46:
Backpropagation Through Time

Slide 47:
Backpropagation Through Time
Why no use relu instead of tanh ?
People have tried and didn’t work well. The reasons lies in the architecture of RNN. Hidden states are multiplicative
On very long sequences, weights could also lead to vanishing/ exploding the gradient. Controlling this is difficult since we’re using only 1set of weights for all the sequences (unfolded layers).
When using relu the problem is that relu is not bounded (i.e sigmoid or tanh). When used inside a RNN, causes a unit to have very large value for the activation function.






Slide 48:
Recursive Neural Networks
Long Short Term Memory (LSTM)

Slide 49:
Long Short Term Memory
Long Short Term Memory (LSTM) was invented in 1997 (Hochreiter & Schmidhuber ) in order to overcome the vanishing gradient problem in Vanilla RNN
Since a RNN can be viewed as a Neural Network with memory, the authors took the idea further and transformed the hidden layer of a RNN into a memory cell
As in memory, they consider the LSTM unit to support the following operations:
Reading
Writing (saving)
Deleting 






Slide 50:
Long Short Term Memory

Slide 51:
Long Short Term Memory
Vanilla RNN
LSTM
+

Slide 52:
Long Short Term Memory
As stated earlier, the LSTM unit is viewed as a memory cell that supports reading, writing and deletion. 
Reading doesn’t imply outputting the whole memory cell, but just parts of it. So the LSTM unit can be viewed as having an additional input/output vs the Vanilla RNN 
	

Vanilla RNN
LSTM

Slide 53:
Long Short Term Memory
Images from http://www.wildml.com

Slide 54:
Long Short Term Memory
Taking these decisions is hard, so we’ll use a separate neural network for each of these decisions.
For each decision, there can be two types of neural networks :
Decides how much information to use (gate)
Decide how to weight that information (negative, positive)

(the difference is in the activation function)
x
context
[0.8, 0.9, 0.1, 0.3]

Slide 55:
Long Short Term Memory

1. First type (gate):Decides how much information to use: The first type of neural network is computed using a sigmoid neuron, based on the context. The result is multiplied by the information.



x
context
Ex: [1.0, 0.9, 0.0, 0.7]
[0.8, 0.9, 0.1, 0.3]
[0.8, 0.8, 0.0, 0.2]

Slide 56:
Long Short Term Memory

Slide 57:
Long Short Term Memory
Decides the sign of information: The second neural network is computed using a tanh neuron, based on the context. The result is multiplied by the information.

     Since tanh outputs information in the interval -1,1 we can view it as a counter for the   information. 





A negative input might be as important as a positive one so we need this kind of information.	

x
tanh
context
Ex: [1.0, 0.9, 0.0, 0.7]
[0.9, -0.9, 0.2, -0.3]
[0.9,- 0.8, 0.0, -0.2]

Slide 58:
Long Short Term Memory

Slide 59:
Long Short Term Memory
x
tanh
+
x
tanh
x

Slide 60:
Long Short Term Memory

Slide 61:
Long Short Term Memory

Slide 62:
Long Short Term Memory

Slide 63:
Long Short Term Memory

Slide 64:
Long Short Term Memory

Slide 65:
Long Short Term Memory

Slide 66:
Long Short Term Memory

Slide 67:
Variations of LSTM

Slide 68:
Variations of LSTM
LSTM with PeepHole: context depends on the previous/current state














x
tanh
+
x
tanh
x

Slide 69:
Variations of LSTM
One disadvantage of LSTM is that it is not able to observe the state which it tries to control through the gates

Standard LSTM learns to detect markers and act. However, if these markers don’t exist the network behaves poorly.

If the output gate is closed, the value of the hidden unit (which becomes input at the next time step) becomes 0

The solution is to make the most recent cell state available to the gates






Slide 70:
Long Short Term Memory: GRU
x
+
tanh
1-
x
x
GRU: Gated Recurrent Unit (no memory cell)















Slide 71:
Variations of LSTM

Slide 72:
LSTM in pytorch
Parameters
input_size – The number of expected features in the input x
hidden_size – The number of features in the hidden state h
num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1
bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True
batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False
dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0
bidirectional – If True, becomes a bidirectional LSTM. Default: False
proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0

Slide 73:
LSTM in pytorch

Slide 74:
LSTM in pytorch

Slide 75:
LSTM in pytorch
class RNN_LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN_LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM (input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)
Store the configuration.
Define the RNN layers: 
the input size: the size of each element in the sequence 28* hidden_size : the number of neurons in each hidden layer:256* num_layers: the number of hidden layer: 2* batch_first : the data will be in the following format (N, S, 28 )  = ( 64, 28, 28)

Add a hidden layer that connects the output at each time step to the an output layer . 
Same as RNN, but use nn.LSTM instead of nn.RNN

Slide 76:
LSTM in pytorch
class RNN_LSTM(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0,c0))
        out = out.reshape(out.shape[0], -1)
 
        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out
 
Initialize the hidden vector and the hidden state of each layer.
There must be one hidden vector per layer, per input in the minibatch. The size of each vector we’ve defined in the configuration 

Slide 77:
LSTM in pytorch
class RNN_LSTM(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0,c0))
        out = out.reshape(out.shape[0], -1)
 
        # Decode the hidden state of the last time step
        out = self.fc(out)
        return out
 

Slide 78:
LSTM in pytorch
class RNN_LSTM(nn.Module):
 ...
 def forward(self, x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0,c0))
        out = out.reshape(out.shape[0], -1)
 
 	# Decode the hidden state of the last time step
        out = self.fc(out)
        return out         
Connect all the outputs to a fully connected layer. The output of the fully connected layers is the classification of the input


Slide 79:
LSTM in pytorch
Results from training using a LSM:

100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.76it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.47it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 52.13it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 52.18it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 52.11it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 52.14it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 52.00it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.97it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.90it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 53.87it/s]
Accuracy on training set: 99.459999
Accuracy on test set: 98.76


Slide 80:
GRU in pytorch
Parametersinput_size – The number of expected features in the input x
hidden_size – The number of features in the hidden state h
num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1
bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True
batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False
dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0
bidirectional – If True, becomes a bidirectional GRU. Default: False

Slide 81:
GRU in pytorch
Training a GRU is straight forward. Just replace nn.RNN with nn.GRU

100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 54.74it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 54.74it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 54.51it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 53.36it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:14<00:00, 62.68it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:06<00:00, 140.77it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:16<00:00, 55.26it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 54.89it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 55.07it/s]
100%|██████████████████████████████████████████████████| 938/938 [00:17<00:00, 54.59it/s]
Accuracy on training set: 98.199997
Accuracy on test set: 97.84


Slide 82:
Questions & Discussion

Slide 83:
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
(lstm overview. Good to read it first)

http://harinisuresh.com/2016/10/09/lstms/
http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
http://arunmallya.github.io/writeups/nn/lstm/index.html#/12
(derivatives for lstm)
http://willwolf.io/2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/ (derivatives for lstm)
https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b (lstm very well explained. Most of this class is based on information found here)

https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714 (well designed graphics)

https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms

http://karpathy.github.io/2015/05/21/rnn-effectiveness/
(nice and easy to follow examples of lstm on text generation)






Bibliography


